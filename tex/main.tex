%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%% 
%% 
%% SMH, May 2010. 

\documentclass[a4paper,8pt]{report}
% \documentclass[a4paper,12pt,twoside,openright]{report}


%% 
%% EDIT THE BELOW TO CUSTOMIZE
%% 

\def\authorname{Khalid Aleem\xspace}
\def\authorcollege{Trinity College\xspace}
\def\authoremail{ka476@cam.ac.uk}
\def\dissertationtitle{A Risc-V based co-processor peripheral to accelerate
  linear algebra operations.}
%\def\dissertationtitle{A Risc-V based co-processor to accelerate linear algebra operations.}
% TODO: LOOK INT LACore...
% Maximising throughput for a Risc-V linear algebra co-processor.
% XXX: Project title changes: no later than 21 May 2021 16:00 
\def\wordcount{3,860}


% \usepackage[dvips]{epsfig,graphics} 
\usepackage{epsfig,graphicx,verbatim,parskip,tabularx,setspace,xspace}

\usepackage[english]{babel}
\usepackage{biblatex}
\addbibresource{bibliography.bib}

\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{fancyvrb,listings}

\usepackage[a4paper, margin=2cm]{geometry}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily\color{black},
  keywordstyle=\bfseries\color{blue!40!black},
  commentstyle=\itshape\color{purple!40!black},
  stringstyle=\color{green!40!black},
  morekeywords={uint32_t,uint8_t,uintptr_t,intptr_t}
}

\lstdefinestyle{customcpp}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  xleftmargin=\parindent,
  language=C++,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily\color{black},
  keywordstyle=\bfseries\color{blue!40!black},
  commentstyle=\itshape\color{purple!40!black},
  stringstyle=\color{green!40!black},
  morekeywords={uint64_t,uint32_t,uint8_t}
}

\input{lst-riscv.tex}

%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 
\section{Motivations}
% NOTE: Clear motivation - justify the benefits of success
% NOTE: Contextualise what i'm building
% NOTE: Analysis of requirements and justified & documented use of tools!

With the end of Moore's law\cite{moore} approaching, transistors can no longer be further
miniaturised and single-digit nanometre lithography techniques remain incredibly
expensive. In this environment, it is increasingly common to have accelerators
within system-on-chips (SoCs), where hardware-level specialisation is vital for
good performance in compute tasks. An example of this would be the Qualcomm
Hexagon DSP co-processor\cite{QC-Hexagon}, implemented in many smartphones. Given the recent
acquisition of Xilinx by AMD, and Altera by Intel in 2015\cite{aa-intel-acq}, the dominant
microprocessor manufacturers have both entered the realm of reprogrammable
hardware. It therefore seems increasingly likely that an FPGA fabric may be
placed within microprocessors for creating specialised accelerators, where
incremental \textit{hardware updates} may be applied to custom processing
pipelines. This is of vital importance in performance and safety critical
systems, such as: self-driving cars.

Coupled with the recent massive growth of open-source FPGA tooling, high-level
hardware design languages, and publically available Risc-V processor designs, it
has become substantially easier to design, simulate, and deploy a highly
customised accelerator. Our high-level hardware design languages allow our
designs to be parameterised, a particularly important feature when designing
hardware for an embedded device where we have tight silicon area, power and
thermal constraints. Open source development is particularly good for designing
embedded systems, where semi-professionals may share, develop, and verify their
own hardware components.

Linear algebra operations are widely used in embedded devices for machine
learning, signal processing and other compute tasks. The existing pressures
within the sphere of hardware design compel us to develop an accelerator for
this purpose, where we hope to make use of open source code and high-level
hardware design languages to accelerate the development process.

Previously, it was thought that SIMD and vector instructions would offer a
significant enough performance improvement that they would supercede the
development cost of an accelerator. We seek to demonstrate that in this open
source environment, with the aid of publically available code and high-level
hardware design languages, the development cost of an accelerator has fallen
substantially. Crucially, vector instructions come at the cost of hardware
interoperability, where we are constrained to a particular processor model that
has the specific ISA extension implemented. It is likely that our highly specialised
compute task will only make use of a subset of our vector instructions, and
therefore we waste unnecessary silicon.  An accelerator developed as a processor
peripheral may be used across different processor types, and ISAs, whilst
simultaneously providing a superior performance enhancement compared to vector
instructions This provides the designer with greater choice and flexibility,
particularly important when designing an embedded device.

\section{Contributions}
This thesis is split into three main chapters. In the first chapter we perform
vital prerequisite work to obtain a minimal system to base our later work on.
We fix latent problems within the floating point unit of the Bluespec Inc.
Piccolo processor, attach our own proof-of-concept memory mapped peripheral to
the AXI4 bus, and design Risc-V bare-metal firmware to interface with this
peripheral. We make use of a modified Verilator testbench to test our design
and rapidly develop the peripheral. Lastly, we perform synthesis, run place \&
route algorithms and generate a bitstream to verify the feasibility of our
design, demonstrating that we can reach clock frequencies of 10MHz without
encountering timing violations.

In our second chapter, we elaborate on our design principles and methodology for
designing our linear algebra accelerator. We implement and benchmark the behaviour
of differently designed functional units of increasing complexity. We also delve
into the memory modifications required to support multiple functional units and
exploit the further parallelism offered by hardware. We determine that the
principal bottleneck in the speed of our accelerator is the rate at which we can
issue instructions.

In our third chapter, we address a range of methods to target the previously
encountered bottleneck. We examine ways of compressing our protocol, and
contemplate alternative designs to our accelerator. We attempt to buffer issued
instructions and reduce the latency between the processor and accelerator to
make them more tightly coupled.

Throughout the development process we minimise our dependence on closed source
and proprietary components and attempt to strongly adhere to open source
principles. We take advantage of this through a rapid development and
prototyping procedure that would not be possible if we are constrained to using
the traditional HDL computer aided design (CAD) tools (Xilinx/Quartus).


\chapter{Background}
\section{Risc-V Instruction Set}
The set of operations a CPU may perform is known as the instruction set
architecture (ISA). An ISA provides the means for a programmer to interface with
a processor, effectively allowing software to communicate with existing
hardware. The ISA also specifies the method of memory addressing, whether
addresses need to be aligned, and the format of processor instructions. Most
desktop computers make use of the x86\_64 ISA, with embedded devices and mobile
phones commonly using the ARM ISA\cite{arm-manual}.

Risc-V\cite{Waterman:EECS-2014-54} is an open standard instruction set architecture (ISA). It is provided
under open source licenses, where anybody may use the ISA with no restrictions
over the use of intellectual property (IP). In contrast, the ARM ISA, which is widely
used in embedded devices, is proprietary. If the ARM ISA is used
for commercial purposes, the company expects a licencing deal to be made and
royalties to be paid. Licencing deals typically cost millions of dollars and can
take months to negotiate. These IP restrictions hinder the process of academic
research, and significantly raise the barrier of entry for processor design,
where only large companies like Qualcomm can compete.

As such, there is no incentive for processor designs to be made open source.
Obtaining the source code of most ARM processors is both difficult and
impractical due to these strict and costly licencing requirements. Modern ARM
instruction sets are bloated, needing to maintain necessary backwards
compatibility --- in many cases a modern ARM CPU will contain redundant
instructions, and therefore redundant logic and complexity that is unnecessary.

In contrast, the Risc-V ISA is modular, and can be partitioned into various
extensions. Of notable interest are the (\textbf{I}) integer instructions,
(\textbf{A}) atomic instructions, (\textbf{C}) compressed instructions,
(\textbf{M}) instructions for integer multiplication and division, and
(\textbf{F}) floating point instructions. Consequently, a functional 32-bit
processor for an embedded system with hardware floating point support requires
only \textbf{129} instructions, with the \textbf{IMAFC} extensions. This
increases the ease of processor design, and avoids the creation of unnecessary
and superfluous hardware.

% A - 11 (atomic instructions)
% C - 36 (compressed 16-bit instructions)
% RV32I - 49 (32-bit integers)
% M - 8 (integer x and /)
% F - 25 (32-bit floating point instructions)
\section{Simulating Hardware} 
The fastest way to test and verify the design of hardware is through simulations.
Creating a silicon application-specific integrated circuit (ASIC) remains an
incredibly costly and time consuming process. There is no room for rapid
prototyping, and hundreds of thousands of units need to be created and sold at a
profit for the process to be economically feasible. As a consequence, there is
very limited room for fabricating an integrated circuit for prototyping purposes.

\subsection{Verilator}
Verilator\cite{dg-verilator} is a piece of open source software that compiles Verilog into C++. A
testbench can then be created, where the generated C++ class representing the
top-level module in hardware, can be stepped forwards in time with the clock
toggled from low to high accordingly, and signals propagated through simulated
hardware accordingly.

In order to run our verilator simulation, we include our verilator-compiled C++
headers and libraries that were generated from our hardware into our C++
testbench. We then proceed to initialise the verilator class representing the
top-level module of our hardware design. It is then possible to toggle the clock
signal and propagate our simulated hardware forwards in time.
\begin{figure}[h]
\begin{lstlisting}[language=C++,style=customcpp,xleftmargin=.05\textwidth]
#include <verilated.h>
#include "VTop.h"
uint64_t main_time = 0;
double sc_time_stamp() { // Called by the $time function in Verilog
    return main_time;
}
int main (int argc, char **argv, char **env) {
    Verilated::commandArgs(argc, argv);
    VTop* top = new VTop; // Create top-level module
    while(!Verilated::gotFinish()) {
        if ((main_time % 10) == 5) { // 10 loops per cycle
            top->CLK = 1; // Raise clock at 5, 15, 25, ...
        }
        else if ((main_time % 10) == 0) {
            top->CLK = 0; // Lower clock at 0, 10, 20, ...
        }
        top->eval(); // Propagate changes
        main_time++;
    }
    top->final(); // end simulation
    delete top;
    return 0;
}

\end{lstlisting}
\caption{An example C++ verilator testbench}
\end{figure}

While verilator suceeds in simulating hardware logic, it does not take physical
considerations into account. For instance, every hardware design has a route through
combinatorial logic known as the critical path, this is the circuitry where
speed of propagation leads to a bound on the maximum clock rate that is
achieveable. Verilator provides no information about this.

% https://www.veripool.org/papers/Verilator_Fast_Free_Me_DVClub10_pres.pdf

\subsection{FPGAs \& Synthesis}
% TODO: image
The most effective way to simulate hardware is by using a Field Programmable
Gate Array (FPGA). An FPGA consists of a large lattice of programmable logic
blocks interwoven by a grid of configurable wires and contact points. The
programmable logic blocks range from boolean logic lookup tables (LUTs), clocks,
registers, DSP units and block RAM. The lattice of programmable logic cells can
be flashed, where the individual hardware units are programmed, and connected
together by configurable wires such that the circuit described in hardware is
physically created. A processor simulated on an FPGA can typically be clocked on
the order of MHz. An FPGA can therefore offer over a thousand-fold performance
increase compared to conventional software simulations of hardware circuits.

Unlike software simulations using verilator, an FPGA can suffer from physical
limitations of hardware layout. A processor that isn't pipelined sufficiently
may have a shorter critical path, preventing the generated hardware on an FPGA
from reaching a sufficiently high clock rate. Furthermore, additional care must
be taken to prevent signal timing violations, and clock domain crossing within
the FPGA.

% TODO: Clock domain crossing?
% TODO: Types of FPGA(?)

\subsubsection{Synthesis}
% TODO: FPGA picture
The process by which an FPGA is programmed with some hardware design is known
as synthesis. The goal of the synthesis process is to produce a bitstream from a
hardware design. The bitstream contains the necessary information required to
program the logic blocks of an FPGA and connect them appropriately.

The first stage of synthesis involves compiling Verilog HDL into a netlist.
A netlist is a complete description of a digital circuit at the logic gate
level, containing all of the information a schematic would, but in a structured
format that is easier for a computer to process.

The second stage involves a process known as place and route. Different FPGAs
have different architectures and designs, with varying numbers of logic cells
and logic cell types. Place and route involved running an algorithm, targetting
a specific FPGA, which figures out a sufficiently optimal placement of logic
cells and connections. Lastly, the design is verified for timing consistency,
and the bitstream is generated.

\subsubsection{Symbiflow}
% Symbiflow - the future. (Find paper) 
The tools required for synthesis are closed source and proprietary, requiring
licenses for use in commercial purposes. Both Xilinx and Altera, the major FPGA
vendors, employ a greater number of software engineers than hardware engineers
within their FPGA divisions to develop their complex closed-source proprietary
software for synthesis.

The Symbiflow project\cite{symbiflow} is an open-source computer aided design (CAD) flow used to
program commercial FPGAs. The proprietary bitstream format for the Xilinx
Arty-A7 has been fully reverse engineered.
The Symbiflow project is capable of performing synthesis, placement, routing
and bitstream generation for this device, where no proprietary licence is required. 

\section{Hardware Design Languages}
Designing hardware in traditional Verilog is a highly inefficient process. There
are no data structures to group together inputs and outputs of hardware modules
that represent different communication channels. Therefore wiring a submodule
appropriately is an arduous process, prone to errors. Code that is highly
modularised, counterintuitively, becomes harder to read and understand.
Moreover, if a single wire needs to be modified, it needs to be explicitly
stated in every instance of a module, a time consuming process.

\begin{figure}[h]
\tiny
\centering
\begin{BVerbatim}
  mkUART uart0(.CLK(CLK),
         .RST_N(RST_N),
         .put_from_console_put(uart0$put_from_console_put),
         .set_addr_map_addr_base(uart0$set_addr_map_addr_base),
         .set_addr_map_addr_lim(uart0$set_addr_map_addr_lim),
         .slave_araddr(uart0$slave_araddr),
         .slave_arburst(uart0$slave_arburst),
         .slave_arcache(uart0$slave_arcache),
         .slave_arid(uart0$slave_arid),
         .slave_arlen(uart0$slave_arlen),
         .slave_arlock(uart0$slave_arlock),
         .slave_arprot(uart0$slave_arprot),
         .slave_arqos(uart0$slave_arqos),
         .slave_arregion(uart0$slave_arregion),
         .slave_arsize(uart0$slave_arsize),
         .slave_arvalid(uart0$slave_arvalid),
         .slave_awaddr(uart0$slave_awaddr),
         .slave_awburst(uart0$slave_awburst),
         .slave_awcache(uart0$slave_awcache),
         .slave_awid(uart0$slave_awid),
         .slave_awlen(uart0$slave_awlen),
         .slave_awlock(uart0$slave_awlock),
         .slave_awprot(uart0$slave_awprot),
         .slave_awqos(uart0$slave_awqos),
         .slave_awregion(uart0$slave_awregion),
         .slave_awsize(uart0$slave_awsize),
         .slave_awvalid(uart0$slave_awvalid),
         .slave_bready(uart0$slave_bready),
         .slave_rready(uart0$slave_rready),
         .slave_wdata(uart0$slave_wdata),
         .slave_wlast(uart0$slave_wlast),
         .slave_wstrb(uart0$slave_wstrb),
         .slave_wvalid(uart0$slave_wvalid),
         .EN_server_reset_request_put(uart0$EN_server_reset_request_put),
         .EN_server_reset_response_get(uart0$EN_server_reset_response_get),
         .EN_set_addr_map(uart0$EN_set_addr_map),
         .EN_get_to_console_get(uart0$EN_get_to_console_get),
         .EN_put_from_console_put(uart0$EN_put_from_console_put),
         .RDY_server_reset_request_put(uart0$RDY_server_reset_request_put),
         .RDY_server_reset_response_get(uart0$RDY_server_reset_response_get),
         .RDY_set_addr_map(),
         .slave_awready(uart0$slave_awready),
         .slave_wready(uart0$slave_wready),
         .slave_bvalid(uart0$slave_bvalid),
         .slave_bid(uart0$slave_bid),
         .slave_bresp(uart0$slave_bresp),
         .slave_arready(uart0$slave_arready),
         .slave_rvalid(uart0$slave_rvalid),
         .slave_rid(uart0$slave_rid),
         .slave_rdata(uart0$slave_rdata),
         .slave_rresp(uart0$slave_rresp),
         .slave_rlast(uart0$slave_rlast),
         .get_to_console_get(uart0$get_to_console_get),
         .RDY_get_to_console_get(uart0$RDY_get_to_console_get),
         .RDY_put_from_console_put(uart0$RDY_put_from_console_put),
         .intr(uart0$intr));
\end{BVerbatim}
\normalsize
\caption{A UART peripheral within an SoC module written in Verilog RTL. All AXI4 connections must be
  individually manually connected to the bus interconnect.}
\label{fig:uart_wires}
\end{figure}

The traditional HDLs were built to be hardware simulation languages and were not
intended for the purpose of designing very complex hardware, such as processors.
Modern software development programming languages have abstractions, such as
structures and functions, improving productivity and encouraging the reuse of
constructs.

To figure out how a module behaves, one must constantly move around the file,
observing how the state changes when the clock changes, or if registers flip
within an \texttt{always\_ff} block, in many cases used to control a finite state
machine. Simultaneously one must also keep track of how the combinatorial state
of the hardware changes, by looking at an \texttt{always\_comb} block.

Due to the difficulties concerning development time, and for purposes of
efficiency, more expressive hardware design languages have been created, namely,
ChiselHDL\cite{chisel} and Bluespec Verilog\cite{bluespec}.

% The Risc-V Rocket processor is designed in
% ChiselHDL, whilst the Risc-V Piccolo, Flute and TOOOBA processors are written in
% BSV. The creation of a soft-core processor is a demonstration of the ability of
% these languages to be applied in practice and make the process of processor
% design more efficient and structured. Bluespec Verilog offers significant
% advantages over ChiselHDL.

% In Chapter 2, we make use of the
% libraries available within BSV to perform floating point computations with ease.
% In Chapter 3, we exploit the advanced scheduling capabilities of BSV to issue
% computations to different hardware units, and make extensive use of pipelining
% for greater concurrency in our floating point computations. We later discuss the
% Rocket and Piccolo processors written about in a later section.


% \subsection{ChiselHDL \& Bluespec Verilog}
% Chisel (Constructing Hardware In a Scala Embedded Language), is a new
% programming language for hardware design that is based on Scala, that is
% executed on a JVM.

% Chisel is capable of generating low-level Verilog from high-level hardware
% design patterns that are written in Scala.

% Chisel allows inputs and outputs of modules to be bundled together, and
% modules to be composed with ease. Code may be written in Scala to construct
% hardware and therefore speed up the development process.

% % Chisel generator / example
% % Chisel I/O / Module + Bundle of Bundles

% This is best demonstrated with the AXI4 protocol, 
% the mess of wire connections, as seen above with standard Verilog, is no longer
% an occurence due to the object-oriented nature of Chisel.
% \tiny
% \begin{verbatim}
% class AXIPeripheral extends Module {
%   val io = new Bundle {
%     val slave = new AXISlaveIF(8, 32)
%   }
%   [...]
% }
% \end{verbatim}
% \normalsize


\chapter{Related Work} 
\section{BlueVec}
The BlueVec\cite{bluevec} co-processor is a vector processor aimed at accelerating neural
network inference designed to address the computation botleneck of the problem
of the \emph{memory wall}.
The memory wall refers to the growing discrepancy between memory bandwidth and
compute power. It is increasingly common for an accelerator system to become
memory-bound, where the latency/clock cycle cost of moving data to and from
memory may even exceed the reduction in compute cycles from having a custom
accelerator pipeline.

% TODO: Explain Memory wall further?
The BlueVec co-processor relies on the observation that in many situations where
a custom accelerator pipeline is implemented to accelerate some computation
through performing it in hardware, a significant improvement may still be obtained
by vectorizing the computation, and implementing the computation within
software making use of SIMD instructions. The paper demonstrates the example of
neural network inference, where the BlueVec co-processor is used in the context
of a digit recognition problem.

The BlueVec co-processor is interacted with through the use of a custom
instruction set extension for the Nios II. The Altera Nios II is a soft-core processor
that is designed to be synthesised on an Altera FPGA. It has a 32-bit RISC-based
architecture, and while the internals are closed source, it is possible to add
custom instruction set extensions within hardware. Altera provides their
proprietary embedded design suite (EDS), where it is possible to compile C/C++
software targetting the Nios RISC architecture, and making use of the custom
instructions that are implemented in hardware.

% The ability to add custom instructions to the Nios processor provides one with
% sufficient versatility to fine tune the designed hardware to perform specific
% tasks very efficiently, making use of the parallellisation-based strengths of
% designing things in hardware.

% One of the key ways in which the BlueVec processor tackles the problem of the
% memory wall is by ensuring that memory is written to and read from in a
% streamlined fashion. Vectorised data is stored in contiguous arrays within
% memory, and the BlueVec co-processor highlights that it makes efficient use of
% external memory bandwidth through the use of burst memory access to stream data
% from external memory at a very high rate.

% TODO: Lane-local memories / Pipelining
% Each thread has its own LANE. --- Similar to multi-channel memory.

% In order to maximise the clock frequency of the BlueVec processor, and therefore
% maximise throughput, the BlueVec processor pipelines operand fetching,
% instruction execution and result writeback. This reduces the length of the
% critical path.

% TODO : Link to BSV
The BlueVec co-processor consists of approximately 1000 lines of Bluespec
Verilog and was developed over the course of only a few months. The quick, and
minimalist development practices used during its construction is a testament to
the benefits of the use of BSV over standard Verilog HDL.

One of the principal disadvantages of the BlueVec co-processor is its reliance
on proprietary synthesis tools (Altera Quartus), and the closed source soft-core
processor. The development practices, and soft-core Nios processor involved in
the construction of BlueVec lock us into the Altera software ecosystem. An
Altera Quartus licence in particular is required to synthesise the hardware on
an Altera FPGA and run the associated benchmarks in the paper. We do not have
the ability to easily move the design to a Xilinx, or iCE FPGA, as not only is
the soft-core Nios processor closed source, we must also rely on the
automatically generated Avalon bus interconnect from the Altera Qsys SoC
designer. As such, a substantial amount of work is required for the BlueVec
processor to be moved onto a different platform.

% TODO: Reliability considerations - vendor black boxes. Bugs (if found) -
% impossible to correct. (Another section!?)

Furthermore, we highlight the loss of interoperability. We are constrained to the
Nios instruction set. There is no way for an ARM, MIPS, Risc-V, or x86\_64 based
system to interface with, and make use of the vector co-processor. With the
growth of ARM processors used in embedded devices, and the recent emergence of
Risc-V, this lack of interoperability is a costly downside to the performance
improvements demonstrated by BlueVec for vector processing.

% TODO: Comment on Benchmarks
% TODO: Bluevec - fails to hit capacity - CPU can't issue one instruction per
% cycle

% NOTE: BlueVec - Literature Review
% NOTE: General argument --- elaborate upon motivations
% NOTE: BlueVec --- Nios. Custom instructions ---> Loss of interoperability
% NOTE: Are there truly multiple functional units? Instructions STALL the cpu.
% NOTE: Unadjustable silicon consumption --- less control!

% Google TPU? & Nvidia GPUs? --- Look into

% TODO VLIW?

% Lots of GPU acceleration of linear algebra acceleration 
% Big takeaway - unless super specific problem --- communication bottleneck
% existed
% Whole computation must be performed in GPU memory --- constrained

% Tightly coupled Accelerator+CPU. General programmability + fixed hardware. Low
% latency between each.

% TODO: Look at Linpack library
% https://link.springer.com/article/10.1007/s11390-011-0184-1

%\section{GPU processing}
% The work by ... reveals that ...

% GPUs are slow for small matrices --- CUDA cores not fully occupied 
% https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/joe.2018.9178

% TODO: Cite SIMD
\section{SIMD ISA extensions}
The SIMD acronym refers to processor instructions that operate on multiple
registers of data (Single Instruction Multiple Data). We therefore save
processor cycles through a technique known as DLP (Data level parallelism),
where fewer processor instructions are required in order to perform the same
mathematical operation. High-performance desktop and server systems that use the
\texttt{x86\_64} ISA have the AVX-512 and SSE instruction set extensions\cite{x86-simd} for
vector processing, while the ARM ISA has the Neon extension for vector
processing on mobile devices.

% TODO: Cite Vector ISA
The Risc-V ISA has the (\textbf{V}) extension to support vector instructions.
That is, instructions that operate on a contiguous region of memory. The ISA
extension specifies the existence of vector registers which may be some power of
2 larger than a traditional 4-byte (32-bit) register. Mathematical operations
may then be simultaneously applied to groups of numbers loaded in these vector
registers.

Crucially, the Risc-V vector extension\cite{riscv-v-spec} has not yet been ratified. As such, its
features may change, and it is unwise to implement the instructions as the ISA
may undergo breaking changes. Ratification is a time consuming process. The
Risc-V foundation needs to be certain that the definition of the ISA extension
is infallible, where considerations of backwards compatability and the necessity of
the proposed instructions are verified by a group of experts.

Moreover, the vector ISA extension consists of 184
additional instructions, exceeding the preexisting 129 instructions in the
RV32IMAFC combination. A very substantial modification to the decode and
execution sections of the processor will need to be made at the cost of a great
deal of silicon space. Even if the ISA is implemented, compilers do not yet
support producing vector instructions in generated machine code.

\section{Open Source Cores}
Since the publication of the Risc-V ISA, a variety of different open source
processors have been designed such as: Rocket\cite{rocket}, or
Piccolo\cite{piccolo}. The source code of these processors is publically
available on GitHub. Collaboration and open source processor design has benefits
in hardware security. This is of particular relevance in the light of the severe
hardware-level speculative execution vulnerabilities found in the majority of
ARM and x86\_64 processors in production, namely Spectre\cite{spectre}, and
Meltdown\cite{meltdown}. With processor designers and manufacturers paying less
attention to verifying the security of their hardware whilst competing with each
other to maximise processor performance, the design of processors is not subject
to enough scrutiny within the development process. Open source processor
development provides the opportunity for processor designs to be verified by a
community of different organisations, increasing the likelihood that
architectural level attacks can be spotted and averted.

Moreover, when designing a peripheral, it is vital to possess an understanding
of the processor that it is connected to. Using an open source processor gives
us greater freedom, allowing us to pry into the internals and analyse the
specifics of processor behaviour if things behave in an unexpected manner.

Since we seek to minimise the use of proprietary, and closed-source libraries,
we avoid using proprietary soft-cores. Notably, Xilinx has its Microblaze
processor, while Altera has its Nios II cores. We may base our accelerator on
one of these, but we are immediately locked into either the Xilinx or Altera
proprietary hardware development (CAD) ecosystems. It is not possible to
simulate an SoC of one of these processors attached to our peripheral without
their proprietary tooling.

Moreover, the proprietary simulation tools offer similar or worse performance
compared to Verilator. Verilator provides us with less-overhead and greater
freedom within our simulations, giving us direct control to our testbench with
C++. In contrast, the proprietary xsim, and modelsim have an associated overhead
of generating a clock and initialising our memory using the bloated the clunky
proprietary CAD tools to simulate our SoC.

% \subsection{Rocket \& RoCC}
% The Rocket\cite{rocket} processor is written in ChiselHDL\cite{chisel}. It is
% packed with a utility known as the Rocket Chip Generator, which generates an SoC
% consisting of the Rocket CPU and the necessary peripherals required for a
% functioning system.

% % TODO: Elaborate: Freedom means physical + commercial use has been demonstrated
% % The Freedom E310 is a Risc-V SoC designed by the company SiFive, the Risc-V ASIC
% % SoC designed by SiFive is 

% The Rocket processor has the Rocket Custom Co-processor (RoCC) interface. This
% allows an accelerator to be implemented through the use of custom instructions.
% By having the hardware of the processor itself fetch and execute instructions as
% opposed to a firmware and memory-mapped approach, the RoCC system saves
% processor cycles. This is accomplished by eliminating the associated overhead of
% reading/writing from memory via an AXI4 bus.

% % TODO: verify this claim
% One of the principal disadvantages of the RoCC is its tight coupling to the CPU.
% The RoCC constrains us to using the Rocket processor in particular, and alters
% the processor's decode and execution units. Because of instruction set
% dependency, we are constrained to Risc-V processors, and therefore making
% sacrifices in terms of interoperability. 

% Further arguments may be made in terms of silicon complexity. Communicating with
% an accelerator through an AXI4 bus allows the silicon of the accelerator to be
% physically separated from the CPU. This provides us with greater benefits in
% terms of heat dissipation in the circumstance that an ASIC is created.

% One of the key disadvantages of ChiselHDL is that it does not offer the same
% degree of scheduling abstraction that exists with BSV. While Scala gives us the
% ability to generate arbitrary and parameterised hardware, it does not provide
% the benefits of designing hardware which executes atomic rules and the
% processing/execution of complex dependency tree.

\subsection{Bluespec Risc-V CPUs}
The Bluespec designed processors take full advantage of the modular nature of
processor design that is made possible through the use of Bluespec Verilog\cite{bluespec}.
Seperate processor ISA extensions, the processor register width, and the AXI4
bus size may be set appropriately on compilation of the BSV code into Verilog
RTL. The ability of BSV to support atomic rules, and specify complex scheduling
relations makes it particularly suited for processor design.

Bluespec Inc. has created three example processors. The Piccolo processor
consists of a 3-stage in-order pipeline, and is designed primarily for small
embedded systems. The Flute processor contains a 5-stage in-order pipeline, and
contains an MMU, allowing it to support virtual memory and boot a linux OS.
The TOOOBA processor, based on the MIT Riscy-OOO\cite{riscy-ooo}, has a superscalar architecture
and supports out-of-order execution.

All of these designed processors have the advantage of providing an example
testbench created in BSV. The HDL may be compiled and firmware may be executed
within simulations, this provides us with a functioning start point to base our
work on.

%\chapter{Design and Implementation}
% NOTE: Main contribution to (the field?)
% NOTE: Original implementation
% NOTE: Somehow demonstrate - extra-curricular reading?!
% NOTE: Compare the design to BlueVec!?

% NOTE: Challenging goals - substantial deliverables
% NOTE: Design decisions

% RV32imafc - bare metal ELF
% Spike, proxy kernel?
% Cost of movement... memory wall
% MMAP - ARM & RiscV

\chapter{SoC and Testbench Setup: Design \& Implementation}
% TODO: Summarise contribution

\section{SoC Design}
% TODO: SoC Image/Picture
\subsection{Piccolo SoC}
Bluespec Verilog offers superior scheduling capabilities compared to ChiselHDL,
and standard Verilog. The creation of hardware that performs more complex tasks
is therefore made significantly easier. As such, we decide to use the Piccolo
processor from the Bluespec Inc. family within our simulation and testbench.

We choose the Piccolo\cite{piccolo} processor over the Flute\cite{flute} processor since we have no need
for virtual memory, therefore an MMU is superfluous. While the Toooba\cite{toooba} processor
would offer superior performance, due to its inherent complexity it would
consume far too much silicon space for the purpose of synthesis. We seek to
minimise our silicon area to open up the opportunity for the later task of
performing synthesis on an FPGA. This will lead to a lower LUT count, and give
us greater flexibility when designing and synthesising our own accelerator as we
have greater FPGA resources to make use of.

% TODO: Disadvantages of Piccolo(?)
% AXI4 bus configurability
% CPU 64/32-bit
% ISA extension customisability
% IMAFC --- ALSO D, M ... etc
% Will not boot a Linux Kernel, but FreeRTOS. Simple bare metal.
% Ignore context switching

\section{Risc-V Firmware}
\subsection{Risc-V Assembly \& Firmware Structure}
In order to compile C and assembly language into machine code for our bare-metal
firmware, we make use of the GNU compiler collection (GCC) rather than
LLVM\cite{riscv-gcc-vs-llvm}. Crucially, our firmware is not an operating
system, we must therefore perform a substantial setup procedure before executing
our own code.


% The naming convention for compilers is \texttt{arch-vendor-os-libc-gcc}. The
% standard, desktop version of gcc \texttt{x86\_64-pc-linux-gnu-gcc} is not
% capable of producing Risc-V machine code. Moreover, it is targetted at the linux
% operating system, and uses the linux GNU LibC and supports linux system calls.
% In order to compile our Risc-V assembly and C into Risc-V machine code, we must
% use a cross-compiler. We build a compiler on our host x86\_64 system to run on
% an x86\_64 machine, that is capable of producing a Risc-V ELF binary.

The \texttt{riscv64-unknown-elf-gcc} compiler is designed to produce a
bare-metal Risc-V binary. Instead of using GNU LibC the C library used is
Newlib, which is designed for embedded systems. We now have access to the
standard C functions for IO \texttt{printf}, and math functions, such as
\texttt{pow}, and \texttt{sqrt}. The relevant machine code for these functions
are statically linked and are contained within the \texttt{.text} section of our
produced ELF binary.

\subsubsection{UART/Serial Output}
% TODO: UART cite
The most important firmware debugging feature to get working is the ability to
print to console. The Piccolo techbench SoC kindly contains a NS16550 UART
peripheral that is connected to the AXI4 bus interconnect as a slave device.  

This UART peripheral is memory mapped, and we write to its internal registers
from within our firmware to obtain a console output. This is vital for debugging
purposes, as it allows us to extract and verify the computed results of our
accelerator.

Crucially, the UART peripheral provided by Bluespec Inc. is only suitable for
simulation purposes. Data written into its transmit register are printed
directly to console. 

\subsubsection{Linker File}
Since we are compiling our C into a bare-metal ELF, we must specify the layout
of the machine code within the binary using a linker file\cite{linker-blog}.
It is vital to consider the distinction between load memory address (LMA) and
virtual memory address (VMA). The LMA is the location of our section in our
binary, while the VMA is the location
referenced within our machine code. In the situation where we perform synthesis,
we may only initialise our ROM. When our machine code and static variables are
compiled into a binary, our initialised static variables \texttt{(.data)} must
be copied over from our ROM into RAM. The LMA of our \texttt{.data} section must
be within our ROM, and the VMA must be within our RAM. Furthermore, we must also
zero our uninitialised static variables \texttt{(.bss)}, with the VMA in our
RAM. We must also specify the start VMA address of our stack.

The other sections of the linker file are as follows: The \texttt{.vector}
section contains the instructions that are run after the processor is booted.
The \texttt{.stack} section is an empty region at the beginning of memory
where the \texttt{\_stack\_start} symbol is defined. Lastly, the \texttt{.text}
section contains our machine code and read-only data, such as constants.

\begin{figure}[h]
  \centering
\scriptsize
\begin{BVerbatim}
OUTPUT_FORMAT("elf32-littleriscv", "elf32-littleriscv", "elf32-littleriscv")
OUTPUT_ARCH(riscv)
ENTRY(crtStart)
MEMORY {
  ROM      (rx):  ORIGIN = 0x80000000, LENGTH = 0x10000  /* 64KB */
  RAM      (rwx): ORIGIN = 0x80800000, LENGTH = 0x10000 /* 64KB */
}

_stack_size = 0x4000; /* 64KB stack = 0x10000, 1MB stack = 0x100000 */
SECTIONS {
  .vector ORIGIN(ROM) : {
    *crt.o(.start_jump);
  } > ROM AT > ROM

  .text : {
    *(.text);
    *(.rodata .rodata.*) /* Constants */
    *(.srodata .srodata.*) /* read-only initialised data */
    *(.text.*); /* seperate sections for static linked NewLib functions */
    *(.eh_frame)
    _rom_data_start = .;
  } > ROM /* LMA = VMA @ ROM */
  
  .data : { /* initialised static variables */
    . = ALIGN(8) ;
    _ram_data_start = .;
    *(.rdata) /* global static data */
    *(.data .data.*) /* initialised data */
    PROVIDE( __global_pointer$ = . + 0x800 );
    *(.sdata .sdata.*) /* small initialised data */
    . = ALIGN(8);
    _ram_data_end = .;
  } > RAM AT > ROM /* VMA @ RAM, LMA @ ROM */
  _ram_data_size = _ram_data_end - _ram_data_start;

  .bss (NOLOAD) : { /* uninitialised static variables */
		. = ALIGN(4);
		_bss_start = .;
    *(.sbss*) /* small uninitialised data */
    *(.bss .bss.*) /* uninitialised data */
    *(COMMON) /* common symbols */
		. = ALIGN(4);
		_bss_end = .;
  } > RAM /* LMA = VMA @ RAM */

  .stack (NOLOAD) : { /* stack location */
    . = ALIGN(16);
    _stack_end = .;
    . = . + _stack_size;
    . = ALIGN(16);
    _stack_start = .;
  } > RAM /* LMA = VMA @ RAM */
}
\end{BVerbatim}
\normalsize
\caption{Our commented linker file responsible for the memory layout of our ELF
  firmware binary}
\end{figure}

Within our linker file, we make use of the \texttt{(AT)} keyword, to specify an
alternative LMA which differs from our VMA. We must write firmware to copy over
our static variables from ROM into RAM, and zero our uninitialised variables in RAM.

Other sections of code are not included. The heap is unspecified, as it is
unnecessary for the purposes of lightweight firmware. Since we are compiling C,
we omit the C++ sections for storing static constructors \texttt{(.ctors)} and
destructors \texttt{(.dtors)}.

\begin{figure}[h]
  \centering
  \scriptsize
\begin{BVerbatim}
Section .vector         : addr         80000000 to addr         8000000e; size 0x       e (= 14) bytes
Section .text           : addr         80000010 to addr         80006694; size 0x    6684 (= 26244) bytes
Section .data           : addr         80800000 to addr         80800430; size 0x     430 (= 1072) bytes
Section .bss            : addr         80800430 to addr         80800c34; size 0x     804 (= 2052) bytes
Section .stack          : addr         80800c34 to addr         80804c40; size 0x    400c (= 16396) bytes
\end{BVerbatim}
  \normalsize
  \caption{Relevant sections of our compiled ELF file are shown. Crucially, we
    have a $16$~KB stack.}
\end{figure}

% One of the first problems encountered was a CPU trap due to a stack overflow.
% The size of the stack may be modified in the linker file. The \texttt{embeddedartistry}
% printf implementation that avoids using heap memory stores a considerable amount
% of data on the stack. As such, we set the stack to $16$~KB. A larger stack
% reduces the likelihood of a stack overflow, particularly if our libraries used
% are dependent on a lot of stack allocations.

\subsubsection{LibC \& printf}
The standard printf function defined within the \texttt{stdio.h} header will
include the declaration of printf compiled within the NewLib library. It will
then be statically linked to our final binary, and have the code section
corresponding to the print function included within your final ELF file.

The standard printf function makes use of heap memory. Due to memory
constraints, the location of the heap is not specified within the linker file.
We therefore seek an implementation of the printf function that relies only on
the stack. We use the embeddedartistry \cite{ea-printf} implementation.

\subsubsection{crt.s --- RiscV Assembly}
The initial location of our program counter after it is moved from the boot rom
is \texttt{0x80000000}. This is the start address of our RAM. For
instruction-level control over CPU behaviour on boot, we place our own section
of assembly code at the beginning of RAM. We base our assembly boot behavior on
that of the firmware created for the VexRiscV \cite{vexriscv} processor.

Of notable importance is setting the stack pointer \texttt{sp}, to ensure that
stack memory is allocated in the correct region of memory. We also set the
global pointer to allow the global pointer relative addressing of variables in our
data section. This is done as follows.

\begin{figure}[h]
\lstset{language=[RISC-V]Assembler, style=customrv,xleftmargin=0.1\textwidth}
\begin{lstlisting}
.global crtStart
.global main

.section	.start_jump,"ax",@progbits
crtStart:
  lui  x2,      %hi(crtInit)
  addi x2, x2,  %lo(crtInit)
  jalr x1, x2   // jump to label crtInit
  nop

.section .main
crtInit:
  .option push
  .option norelax
  la gp, __global_pointer$ // Set global pointer for GP-relative addressing
  .option pop
  la sp, _stack_start // Set stack pointer to beginning of stack
  call initialize_ram
  call main
infinitLoop:
  j infinitLoop

\end{lstlisting}
\lstset{}
\caption{Risc-V assembly instructions for the fimware setup/boot process}
\end{figure}

\section{Testbench Setup}
\subsection{Debugging Piccolo: FPU}
\subsubsection{Fixing Piccolo FPU typeerrors}
% TODO: hyperlink:  
The latest commit of the Piccolo processor, at the time of development,
(\href{https://github.com/bluespec/Piccolo/tree/a4e34ef2f2ba4e82a95faa2bbe3dd832ca3c51a0}{a4e34ef}),
had a type error in the floating point unit (FPU). The compiler experiences ambiguity between the RoundMode enumerations declared
within the BSV FloatingPoint library (\texttt{FloatingPoint.bsv}), and the
enumeration within \texttt{ISA\_Decls}. We hypothesise that a previous version
of the Bluespec Verilog compiler was capable of either resolving this ambiguity
by falling back to the library definition of the datatype, or prioritised the
library one to allow the processor to compile. Due to a lack of continuous
integration, or testing, this issue has not yet been fixed.
% TODO: Submit a pull request

% NOTE: Compile log
\begin{figure}[h]
\scriptsize
\begin{verbatim}
compiling Piccolo/src_Core/CPU/FPU.bsv
Warning: Unknown position: (S0080)
   1 warnings were suppressed.
Error: "Piccolo/src_Core/CPU/FPU.bsv", line 52, column 30: (T0080)
Type error at the use of the following function:
   mkFloatingPointDivider

The expected return type of the function:
   g__#(ClientServer::Server#(Tuple3#(FPU::FSingle, FPU::FSingle, ISA_Decls::RoundMode), FPU::FpuR))

The return type according to the use:
   c__#(ClientServer::Server#(Tuple3#(FloatingPoint::FloatingPoint#(d__, e__),
      FloatingPoint::FloatingPoint#(d__, e__),
      FloatingPoint::RoundMode),
      Tuple2#(FloatingPoint::FloatingPoint#(d__, e__), FloatingPoint::Exception)))
\end{verbatim}
\normalsize
\caption{Bluespec Inc. Piccolo processor: floating point ISA extension compiler errors.}
\end{figure}

In order to successfully compile the Piccolo processor with floating point
support, we must eliminate the ambiguity manually by specifying which package,
or library the datatype is declared within. We enter, \texttt{FPU.bsv} and prefix the
RoundMode type with \texttt{FloatingPoint::RoundMode} when it is used within the
arguments of a floating point library module. Additionally, we must enter
\texttt{FPU\_Core.bsv} and modify the function that converts between both
enumerations. The \texttt{fv\_getRoundMode} function must be modified to return
\texttt{FloatingPoint::RoundMode}.

After these changes, we observe that the Piccolo processor with the
floating point Risc-V ISA extension successfully compiles into Verilog RTL from
Bluespec Verilog. We may then simulate the processor performing floating point
precision mathematics using our verilator test bench.

\subsubsection{mstatus CSR}
Once the FPU is successfully compiled and the processor is compiled with
floating point support, the processor will trap and throw an exception when any
floating point instruction is executed. When a processor exception takes place,
the processor moves the program counter back to the start of the boot ROM, this
is observed in the simulation output as an infinite loop, where all instructions
after the first floating point instruction will never execute, as the processor
will constantly trap and reset itself, perpetually restarting the program.

With an open source processor, we have the ability to inspect and modify the
source code, and recompile the processor. We can instruct the CPU to print its
internal state when a trap occurs, notably: information about the trap, and the
type of exception that has been thrown. With this information, we can look
through the codebase of the Piccolo processor, and identify what is causing the
processor to fail to execute floating point instructions, despite the fact that
the FPU module is included within the processor.

We enter \texttt{CPU.bsv}, and modify the rule responsible for modifying the
behaviour of the CPU when the trap condition is met. We display additional
information about the trap, and instruct the simulation to end when a trap
occurs. This is where BluespecVerilog is incredibly useful. The
\texttt{Trap\_Info} structure within BSV contains the \texttt{FShow} typeclass.
This contains information for printing the structure in a clean and easy-to-read
way, specifically for debugging purposes. Additionally, the
\texttt{fshow\_trap\_Exc\_Code} function will print the human-readable name of
the exception code enumeration. As such, we do not need to constantly switch
between looking at our program output, and figuring out what the output
represents in our source code files.

\begin{figure}[h]
\begin{verbatim}
// CPU.bsv
   rule rl_trap ((rg_state == CPU_TRAP)
     && (stage1.out.ostatus != OSTATUS_BUSY));

      [...]

      $display("DEBUG: CPU TRAP: ", fshow(rg_trap_info));
      $display("Exc_Code: ", fshow_trap_Exc_Code(exc_code));
      $finish(0);
   endrule: rl_trap
\end{verbatim}
\caption{Changes made to the CPU.bsv trap rule to prevent infinite looping.}
\end{figure}

The information we obtain about our processor trap can be matched to the
specific address in assembly code that causes the error. In this particular
case, the \texttt{flw} instruction is responsible for triggering an
\texttt{ILLEGAL\_INSTRUCTION} exception.
\begin{figure}[h]
\begin{lstlisting}[language=C,style=customc,xleftmargin=.05\textwidth],
  float a = 1.0f;
  // Assembly:
  // 80002276: lui a5, 0x80001
  // 8000227a: flw fa5,424(a5)
  // 8000227e: fsw fa5,-28(s0)
  // Prints:
  // Trap_Info { epc: 'h8000227a, exc_code: 'h2, tval: 'h1a87a787 }
  // Exc_Code: ILLEGAL_INSTRUCTION
\end{lstlisting}
\caption{Analysis of our processor trap and finding the instruction (flw) that causes
it.}
\end{figure}

We look through our processor code, and identify the locations where this
particular exception code is set. We observe that it is set in the
\texttt{alu\_outputs\_base} struct within \texttt{EX\_ALU\_functions.bsv}. This
struct is copied within the ALU function that corresponds to loading data into
registers, \texttt{fv\_LD}. If the \texttt{flw} instruction is called, a
particular bit on the \texttt{mstatus} control-status register (CSR) is read. In
our case, the fs bit is found to be disabled.
\begin{figure}[h]
\begin{verbatim}
 // EX_ALU_functions.bsv
 function ALU_Outputs fv_LD (ALU_Inputs inputs);
    [...]
    // FP loads are not legal unless the MSTATUS.FS bit is set
    Bool legal_FP_LD = True;
 `ifdef ISA_F
    if (opcode == op_LOAD_FP)
       legal_FP_LD = (fv_mstatus_fs (inputs.mstatus) != fs_xs_off);
 `endif
    let alu_outputs = alu_outputs_base;

    alu_outputs.control   = ((legal_LD && legal_FP_LD) ? CONTROL_STRAIGHT
                                                      : CONTROL_TRAP);
    [...]
    return alu_outputs;
 endfunction
\end{verbatim}
\caption{Finding out why the processor traps by looking at the condition by
  which the flw instruction fails.}
\end{figure}

Compared to debugging standard Verilog, the structure of Bluespec Verilog
provides us with a greater ability to debug, and identify relevant codepaths.

We further observe that all FP ALU functions check the fs bit of the control
status register (CSR).
This can be seen from the ALU \texttt{fv\_LD} function that runs when the
\texttt{flw} instruction is executed. We later learn, that this is an intentional design descision,
mentioned obscurely, as per the privileged Risc-V
specification\cite{Waterman:EECS-2016-161} (Chapter 3.1.10). The fs bit on the
\texttt{mstatus} CSR must be set in order for floating-point instructions to be
enabled. We can see how this is done in bare-metal C, by looking at the source
code for the Spike\cite{spike} Risc-V proxy kernel, we implement the relevant parts of this code within our firmware
initialisation sequence. Once implemented, we observe that floating point
instructions operate correctly.
% https://github.com/riscv/riscv-pk/blob/master/machine/minit.c
% https://github.com/riscv/riscv-pk/blob/master/machine/encoding.h 
\begin{figure}[h]
\begin{lstlisting}[language=C,style=customc]
// encoding.h
#define MSTATUS_FS          0x00006000
#define write_csr(reg, val) ({ \
  asm volatile ("csrw " #reg ", %0" :: "rK"(val)); })

// minit.c
static void mstatus_init()
{
  uintptr_t mstatus = 0;
  // Enable FPU
  if (supports_extension('F'))
    mstatus |= MSTATUS_FS;
  [...]
  write_csr(mstatus, mstatus);
  [...]
}
\end{lstlisting}
\caption{Parts of firmware required to enable the Risc-V floating point unit
  taken from the Risc-V Spike emulator.}
\end{figure}

% TODO: Citation
% The RISC-V Instruction Set Manual, Volume II: Privileged Architecture,
% Document Version 20190608-Priv-MSU-Ratified, Editors Andrew Waterman and
% Krste Asanovic, RISC-V Foundation, June 2019
% https://riscv.org/wp-content/uploads/2017/05/riscv-privileged-v1.10.pdf
% TODO: Citation (Spike) + RiscV PK

\subsection{AXI4 Test Peripheral}
When building a peripheral, it is important to first work constructively from a
minimum functioning unit. In order for our embedded processor to communicate
with with our peripheral, we make use of memory-mapped I/O. We map a small
unoccupied memory address range to our peripheral, and give it a small internal
region of memory where 4-byte integers can be read/written from.

\subsubsection{What is AXI4?}
A microprocessor communicates with memory and other peripherals through a bus.
The AXI4 bus protocol\cite{axi4:wiki} is designed by ARM, and is royalty-free, where a licence
is not required for academic or commercial use.

% https://forums.xilinx.com/t5/Design-and-Debug-Techniques-Blog/AXI-Basics-1-Introduction-to-AXI/ba-p/1053914 

% AXI4 RAM: https://github.com/alexforencich/verilog-axi/blob/master/rtl/axi_ram.v 

% 10 wire write address channel
% 5 wire write data channel
% 4 wire write response channel
% 10 wire read address channel
% 6 wire read data channel
% 35 wires in total, excluding clock & reset

Due to the inherent complexity of the AXI4 protocol, large combinatorial finite
state machines are typically required to ensure the read and write requests of
the AXI4 bus are processed correctly.
The state of over a dozen wires needs to be considered by the programmer on
every clock cycle. Developing a peripheral in this manner in standard Verilog
RTL is a challenging feat.

Moreover, when a peripheral for the module is initialised, all 35 wires
corresponding to signals in the AXI4 protocol need to be connected to the AXI4
interconnect. All of these connections need to be manually specified, as
previously shown in Figure \ref{fig:uart_wires}. Bluespec Verilog offers a
significant degree of abstraction when processing AXI4 read and write requests,
and also when attaching the peripheral to the AXI4 bus interconnect.

\begin{figure}[h]
  \scriptsize
\begin{verbatim}
   Fabric_AXI4_IFC  fabric <- mkFabric_AXI4;
   Core_IFC #(N_External_Interrupt_Sources)  core <- mkCore;
   UART_IFC   uart0  <- mkUART;
   Test_IFC   test <- mkTest;
   mkConnection (core.cpu_imem_master,  fabric.v_from_masters [imem_master_num]); // instruction memory
   mkConnection (core.cpu_dmem_master,  fabric.v_from_masters [dmem_master_num]); // data memory
   mkConnection (fabric.v_to_slaves [uart0_slave_num],  uart0.slave); // UART0 serial peripheral
   mkConnection (fabric.v_to_slaves [test_slave_num],  test.slave); // Test peripheral
   // Connect to BlockRAM
\end{verbatim}
  \normalsize
  \caption{Connecting a UART peripheral to the AXI4 bus within BSV}
\end{figure}


% Proprietary CAD tools, such as Xilinx Vivado, typically have a built-in block
% design tool. The built in design tool allows the developer to see the SoC as a
% whole, and visually connect components together. These design tools generate the
% necessary Verilog HDL representing the SoC and the necessary connections between
% the module components.

% Our SoC can be created by creating the necessary IP blocks for our accelerator,
% and processor, and making use of the proprietary AXI interconnect, UART module
% and block memory controller. The block design tools created by Xilinx and Altera
% are a solution to the mess of specifying wires and connections when writing
% standard Verilog HDL in a modular fashion. The design may be then simulated
% directly using the xsim command within Xilinx Vivado.

Our testbench SoC written in Bluespec Verilog has no visual block editor, like
with CAD tools such as Xilin Vivado. The entire network consisting of the
processor core, peripherals and AXI4 interconnect fabric are all written in BSV,
and the modules are connected together manually. This process is significantly
simpler than with Verilog RTL, as the overloaded module/typeclass
\texttt{mkConnection} can be used to forward arbitrary structures of data
between modules of particular interfaces.

An instance of \texttt{mkConnection} exists for the AXI4 master and slave
interfaces that are also implemented in BSV themselves. Each instance of
\texttt{mkConnection} is a module that forwards data between all 35 AXI4 wires.

\subsubsection{AXI4 Alignment/Padding}
Due to the structure of the AXI4 bus and the way a processor writes to memory,
data stored in memory must be aligned depending on its size.
A 32-bit AXI4 bus may only access at most a single aligned
4-byte block per read/write request. As a consequence, it is possible to fail
writing/reading from memory when crossing the block boundary.

% TODO: Include image
For example, writing to an unaligned 4-byte region at the address $0x2-0x6$ crosses
the $0x4$ boundary:
\begin{figure}[h]
  \centering
\scriptsize
\begin{BVerbatim}
*((uint32_t*)0xC0001002) = 0x1337;
Trap_Info { epc: 'h800022d6, exc_code: 'h6, tval: 'hc0001002 }
TRAP EXC: STORE_AMO_ADDR_MISALIGNED
*((uint32_t*)0xC0001000) = 0x1337;
\end{BVerbatim}
  \caption{Demonstration of an AXI4 write trap and success.}
\end{figure}

This is vital to keep in mind throughout writing to peripheral memory. An
integer must be aligned to 4 bytes, a short to 2 bytes, and a char to the
nearest byte. 

% \subsubsection{Struct Padding}
% % TODO: Introduce the need for struct padding/packing
% This is of particular importance to consider later when we issue commands to the
% accelerator. By default, a C struct is padded and aligned to conform to the
% alignment of the way memory is accessed. When a struct is written to memory, it
% always begins at the nearest 4 byte boundary. All datatypes within a struct ar
% aligned to the nearest 4, For instance:

% \begin{verbatim}
% struct paddedstuff_t {
%     uint16_t A; // short (2B) --> (4B) PADDED to ALIGN to nearest 4 bytes
%     uint32_t B; // int (4B)
% }; // 8 BYTES
% struct packedstuff_t {
%     uint16_t A; // short (2B)
%     uint32_t B; // int (4B) ---> Will write at base+0x2
% } __attribute((packed)); // 6 BYTES
% \end{verbatim}


\chapter{SoC \& Testbench Setup: Evaluation}
\section{Clock rate within simulation}
\subsection{Methodology}
The Piccolo processor was chosen due to its relative simplicity. With a simple
3-stage in-order pipeline, the generated C++ should be relatively simple
compared to the 5-stage pipeline in the Flute processor and the out-of-order
superscalar architecture of the TOOOBA processor. As a consequence, we would
therefore increase the likelihood that we obtain a reasonable clock rate in
simulations, and have a faster testbench compile-time.

We modify the existing Piccolo testbench program to output the clock rate when
the testbench is receives an interrupt signal, or when the simulation has come
to an end. This is done as follows, we keep track of the time throughout our
simulation and divide the number of simulated cycles by the time elapsed.
\begin{figure}[h]
\begin{lstlisting}[language=C,style=customc]
using namespace std;
using namespace std::chrono;

steady_clock::time_point tp_begin;

void print_clock_rate() {
  uint64_t cycle_count = main_time/10;
  steady_clock::time_point tp_end = steady_clock::now();
  uint64_t time_elapsed = duration_cast<seconds>(tp_end-tp_begin).count();
  cout << "Clock Rate: " << cycle_count/time_elapsed << " Hz" << endl;
}

void sigint_callback_handler(int signum) {
  cout << "Interrupted" << endl;
  print_clock_rate();
  SIG_DFL(signum);
  //exit(signum);
}

int main (int argc, char **argv, char **env) {
    Verilated::commandArgs (argc, argv);    // remember args
    signal(SIGINT, sigint_callback_handler);
    // TESTBENCH INIT...
    tp_begin = steady_clock::now();
    // TESTBENCH START... (LOOP)
    // TESTBENCH FINISH
    print_clock_rate();
    return 0;
}
\end{lstlisting}
\caption{Modifications to the standard Verilator testbench to output the clock rate.}
\end{figure}


\subsection{Results}
On interrupting our program we measure a clock rate of $22406$~Hz. This is
sufficient for running a simulation of interactions with a peripheral. Moreover,
we are confined to using bare-metal firmware, as our clock rate is too low to
boot an operating system.

\section{Simulating an AXI4 Peripheral}
\subsection{Methodology}
We make use of a verilator testbench to simulate our SoC. We write some simple
test firmware to write into the peripheral, and trigger its execution.

When a special execution bit is set, we set a special accelerator busy bit and
unset the execution bit. In a single cycle, within the accelerator, if the
busy bit is set, we double the array and unset the busy bit. This allows the
firmware to stall until the busy bit is unset, and the computation has been
completed.

\begin{figure}[h]
\begin{lstlisting}[language=C,style=customc]
#define TEST_CONTROL_ADDR 0xC0001000UL
#define TEST_DATA_ADDR 0xC0001008UL
#define TEST_RANGE 4
void main() {
  for(int i=0; i<TEST_RANGE; i++) {
    *((uint32_t volatile*)TEST_DATA_ADDR+i) = i;
    uint32_t data = *((uint32_t*)TEST_DATA_ADDR+i);
    printf("%d: %d\n", i, data);
  }

  println("Set Control Bit..");
  *((uint8_t volatile*)TEST_CONTROL_ADDR) |= 1; // Set execute!

  while(*((uint8_t volatile*)TEST_CONTROL_ADDR) & 2 == 0) {}
  println("Stalling.."); // Printing Stalling... stalls anyway...

  println("Stall complete!");
  for(int i=0; i<TEST_RANGE; i++) {
    uint32_t data = *((uint32_t volatile*)TEST_DATA_ADDR+i);
    printf("%d: %d\n", i, data);
  }
}
\end{lstlisting}
\caption{The firmware for communicating with the AXI4 Test Peripheral.}
\end{figure}

\subsection{Results}
% NOTE: Console log

\begin{figure}[h]
\centering
  \small
\begin{BVerbatim}
0: 0
1: 1
2: 2
3: 3
Set Control Bit..
Stalling..
Stall complete!
0: 0
1: 2
2: 4
3: 6
\end{BVerbatim}
\normalsize
\caption{Simulation output of our verilator testbench for our minimal SoC.}
\end{figure}

We successfully write numbers to our accelerator memory and double them all in a
single clock cycle by issuing a command to our accelerator. Now that we have
demonstrated a functioning testbench, we proceed to the synthesis stage. 

\section{Synthesis}
\begin{figure}[b]
  \centering
  \fbox{\includegraphics[width=\textwidth]{./img/soc_dg.png}}
  \caption{Block diagram of SSITH\_P1 based SoC created for the purpose of
    synthesis, containing GPIO, UART, RAM and ROM memory-mapped peripherals.}
\end{figure}

We synthesise, place \& route and generate a bitstream of a minimal SoC using
Xilinx Vivado. Additionally, Vivado provides us with more detailed information
about timing violations and a place and route diagram revealing the structure of
the hardware to us in a visual manner. This is invaluable, and is on balance why
we decide to avoid using the open source Symbiflow toolchain for the purpose of
synthesis.

We reconstruct our SoC using the block design tool within Vivado Studio. We use
the \texttt{SSITH\_P1} core provided by the repository.

We perform a place and route (P\&R) procedure, to reveal the LUT occupancy and verify
that the design passes timing checks at a 10 MHz clock rate. We generate an \texttt{.mml}
file containing the information of our ROM cells from the P\&R process and
proceed to generate a bitstream where we have initialised the appropriate ram
cells with our firmware for flashing our FPGA.

From our diagram/table of FPGA occupancy (Figure \ref{fig:soc_pr}/Table
\ref{table:soc_pr}), we see that the majority of space on our FPGA is
free. This is ample room for synthesis of an external accelerator.
\begin{table}[b]
  \centering
  \begin{tabular}{l|r|r|r}
    \toprule
    Type  & Used & Available & \% \\
    \midrule
    LUT   &	20817&	63400	&32.83 \\
    LUTRAM&	759	 &19000	  &3.99 \\
    FF	  &16704 &126800	&13.17 \\
    BRAM	&37.5	 &135	    &27.78 \\
    DSP	  &15	   &240	    &6.25 \\ 
    IO	  &16	   &210	    &7.62\\ 
    BUFG	&2	   &32	    &6.25\\
    \bottomrule
  \end{tabular}
  \caption{Xilinx Arty A7-100T FPGA occupancy statistics}
  \label{table:soc_pr}
\end{table}
% TODO: Appendix
% TODO: MML file --- flash BlockRAM
% TODO: P&R Diagram - highlighted!

With a successful P\&R at 10MHz, with no timing violations, we have a clock rate
increase of 400-fold compared to our simulations at 24kHz.

The most pronounced timing violation constrains our clock rate to 10MHz as
opposed to maximum of 100MHz is due to the sparsity of our BlockRAM. We can see
that our orange ROM modules (orange highlights on red rectangles), are
distributed quite sparsely. Our system clock \texttt{(sys\_clk)} originates at
the very centre of our chip, and the signal needs to propagate to greater
distance to clock the sparse ROM. This becomes an issue at clock frequencies
that exceed 10MHz.

Now that we have demonstrated the feasibility our SoC design through
simulations, synthesis, place \& route and bitstream generation targetting a
Xilinx Arty A7-100T, we may build upon our minimal functioning self-contained
SoC to construct our own linear algebra peripheral.

\begin{figure}[h]
  \centering
  \fbox{\includegraphics[width=.65\textwidth]{./img/soc_imp.png}}
  \caption{Xilinx Arty A7-100T Place and Route diagram. Debug hub (Red), Processor (Grey), AXI Interconnect (Pink), AXI GPIO (Green), AXI Serial
    (Brown), AXI RAM (Yellow), AXI ROM (Orange)}
  \label{fig:soc_pr}
\end{figure}

\chapter{Linear Algebra Peripheral: Design \& Implementation}
\section{Overview}
\subsection{Background}
Mathematically, the result of a matrix multiplication may be expressed
accordingly:
\begin{align*}
  C &= AB \\
  C_{ij} &= \sum_k A_{ik}B_{kj}
\end{align*}

Matrix multiplication may be expressed in a programmatic form as such:
\begin{center}
\texttt{C[i][j] = sum zipWith (op*) (row A i) (col B j)}
\end{center}
When matrices are stored in memory, they can only be indexed with a single
dimension. A multi-dimensional matrix is represented as a multi-dimensional
array, this array is represented in memory in as a flattened contiguous
one-dimensional array, with the rows of the matrix next to each other laid of
sequentially in memory.

Consequently we access the element on the xth row, and yth column of a matrix as
follows:
$$ \text{IndexOf}\,\, A[x][y] = x + y * \text{width}(A) $$

This methodology of accessing the x, and y elements of a stored matrix is used
throughout the accelerator.

Block RAM (BRAM) is a contiguous region of memory that exists within the FPGA. In
Bluespec Verilog, block RAM is created through the use of something known as a
Register File (\texttt{RegFile}). Crucially, this is distinct from the Bluespec
array-access methodology of \texttt{Vector\#(N, Reg\#(type))}. A vector of
registers are seperate registers, which need not necessary be contiguous in
hardware, that can be array-accessed in a contiguous fashion within Bluespec.

Our peripheral contains a RegFile of internal BRAM which is memory mapped to
the CPU via an AXI4 bus slave interface. 
% RegFile vs Vector#(Reg#()) vs BRAM

\subsection{Design}
\subsubsection{Hybrid/Monolithic}
% TODO: Citations - argue around, base ideas off.
We propose two potential ways of implementing matrix multiplication. We could
pursue a monolithic approach and implement the entire matrix multiplication
operation purely in hardware. Alternatively, we could perform a hybrid approach,
where each element of the matrix \texttt{C[i][j]} may be computed by issuing a
separate command to the accelerator. This is performed under the assumption that the
creation and scheduling of instructions for each element in the final matrix is
more easily performed within the firmware of the embedded processor without a
significant perfomance penalty.

\subsubsection{Memory Regions}
We split our accelerator memory into seperate memory-mapped regions. We
interface with our accelerator through the AXI4 bus by writing and reading from
memory appropriately.  
\begin{table}[h]
  \centering
  \begin{tabular}{lll}
    \toprule
    Offset Address Range & Size & Purpose \\
    \midrule
    \texttt{0x0000-0x0020} & 32 Bytes & Status \\
    \texttt{0x0020-0x0040} & 32 Bytes & Control \\
    \texttt{0x0040-0x1000} & $\approx$ 16 KB & Memory \\
    \bottomrule
  \end{tabular}
  \caption{Memory regions within our linear algebra accelerator.}
\end{table}

Like the test peripheral, our data memory consists of a memory mapped
\texttt{RegFile} to make use of FPGA BRAM cells. In the later section, we modify
the structure of the memory to support multiple functional units, where the same
address will map to multiple RegFiles.

\subsubsection{Status}
The first bit of our status register in memory exists to monitor the execution
of the accelerator. In the situation that all functional units are occupied, a
command that is issued to the accelerator will be ignored. It is therefore
important to keep track of the internals of the acclerator and make sure that it
is not at maximum utilisation when commands are issued. Within our firmware, we
must check that there is at least one free functional unit before issuing a
command, or else our commands will drop and the computation of our final matrix
will not be correctly performed.

\subsubsection{Control}
We write our control command/instruction to the control register. This is
discussed in greater detail in the following section. Once our command is
written, it is executed by setting the execute status bit within the control
register. The accelerator should then dispatch the appropriate command to one of
its functional units if it is free.

\section{Command Issuing \& Decoding Protocol}
\subsection{Format}
In the hybrid model of our accelerator, an individual command must be able to
compute an individual element of our final matrix, C. As discussed earlier,
matrices are stored linearly in memory. We provide initial addresses, offsets,
and strides for vector access of the respective row in matrix A, and column in
matrix B, and the relevant output address and offset for our element of matrix
C. From this information, our accelerator can instruct a functional unit to
perform the appropriate dot product.

\subsection{Decoding}
One of the principal problems encountered between issuing a command containing
a struct of multi-byte data to the accelerator from the embedded processor. By
default, the unsigned integer type in BSV is in big endian, while the
embedded core stores data in little endian. The motivation for embedded
processors to use little endian byte ordering is to ensure that the address of a
variable is unchanged when a typecast is performed. This saves processor cycles,
and results in improved performance. Little endian variables are automatically
truncated/extended when an appropriate typecast is made. \\


% TODO: Link to:
Within BSV, we make use of the \href{https://github.com/jeffreycassidy/BlueLink/blob/master/Core/Endianness.bsv}{\texttt{Endianness.bsv}} library provided by the
open source BlueLink\cite{bluelink} project in order to interpret the processor-written struct
fields with a little endian encoding.

\begin{figure}[h]
\centering 
\scriptsize
\begin{BVerbatim}
typedef struct {
   LittleEndian#(Bit#(32)) addr;
   LittleEndian#(Bit#(8)) offset;
   LittleEndian#(Bit#(8)) stride;
   } MatUnitPtr deriving (Eq, Bits, FShow);

typedef struct {
   LittleEndian#(Bit#(8)) unit;
   LittleEndian#(Bit#(8)) count;
   MatUnitPtr ptr_a;
   MatUnitPtr ptr_b;
   MatUnitPtr ptr_c;
} MatUnitArgs deriving (Eq, Bits, FShow);

\end{BVerbatim}
\normalsize
\caption{Bluespec Verilog structures for our co-processor communication
  protocol. }
\end{figure}

We write a C struct into our control region of memory. In order for the data
within the instructions to be interpreted correctly in hardware, the
byte-ordering needs to be reversed. BSV provides us with a systematic and clean
way of doing this within hardware. The alternative would be to reverse the bytes
in software, and therefore lose cycles. \\

\begin{figure}[h]
  \centering 
  \scriptsize
\begin{lstlisting}[language=C,style=customc,xleftmargin=.3\textwidth]
struct MatUnitPtr {
  uintptr_t addr;
  uint8_t offset;
  uint8_t stride;
} __attribute__((packed));
struct MatUnitArgs {
  uint8_t unit;
  uint8_t count;
  struct MatUnitPtr ptr_a;
  struct MatUnitPtr ptr_b;
  struct MatUnitPtr ptr_c;
} __attribute__((packed));
\end{lstlisting}
  \normalsize
  \caption{Packed structure in C, corresponding to that of BSV, that is written directly to the
    control region of our accelerator memory. }
\end{figure}


% TODO: C padding - citations
Naturally, C structures are padded and aligned, where N-byte variables are
aligned to the nearest N-byte block. Variables in a struct are extended in size
in accordance with this padding and alignment criteria. A structure in BSV is
unpadded, and as such, it is vital to ensure that the structure has no padding
within our C code for when it is written to the peripheral.

\section{Hardware floating point operations}
\subsection{Bluespec Verilog FloatingPoint}
We have a decision to implement calculations with rational numbers using either
fixed-point, or floating-point. While GCC has built-in support for fixed-point
arithmetic and fixed-point hardware is more simple to implement, we decide to
implement our accelerator with floating-point arithmetic.

BSV contains a FloatingPoint library. Modules already exist for addition and
multiplication which are polymorphic, extending the Server interface. The most
simple is the floating point adder module. The Server interface in Bluespec
makes use of an internal Put interface for a request, and a Get interface for a
response.

The floating point modules are parametised by the precision of our desired floating
point format. Naturally, we use the IEEE754 32-bit floating point standard, with a 1-bit sign,
23-bit significand and 8-bit exponent. We therefore make use of our own
single-precision floating-point datatype \texttt{FSingle}. For the sake of
convenience, we also define the datatype for the associated response, containing
a two element tuple of our result, and an exception.

By making use of the \texttt{Server} interface, we can we can put a request containing a
3-element tuple of our operands and the appropriate floating point rounding mode
to our floating point adder. After 6 clock cycles, we obtain a response. A test
implementation of this is shown below: \\

\begin{figure}[h]
  \centering 
\scriptsize
\begin{verbatim}
import GetPut :: *;
import ClientServer :: *;
import FloatingPoint :: *;

typedef FloatingPoint#(8,23) FSingle;
typedef Tuple2#( FSingle, FloatingPoint::Exception ) FpuR;

module mkFPAddTest(Empty);
   Reg#(UInt#(8)) cycle <- mkReg(0);
   rule rl_cycle(cycle < 255);
      cycle <= cycle + 1;
   endrule

   Server# (Tuple3# (FSingle, FSingle, RoundMode)
            , FpuR ) fpu_add <- mkFloatingPointAdder;
   rule rl_start(cycle == 0);
      FSingle opd1 = 1.0;
      FSingle opd2 = -2.0;
      fpu_add.request.put(tuple3(opd1, opd2, defaultValue));
      $display("%2d: Start", $time);
   endrule

   rule rl_end;
      match { .res, .exc } <- fpu_add.response.get();
      $display("%2d: Result: %h", $time, pack(res)); 
      $finish(0);
   endrule
endmodule

// OUTPUT:
// 10: Start
// 70: Result: bf800000 (-1.0)
\end{verbatim}
\normalsize
\caption{Example BSV code for a floating point addition testbench and the
  associated simulation output obtained from a BlueSim simulation.}
\end{figure}

% TODO: Pipeline --- diagram
We propose that the pipelined nature of the floating point units offered by the
Bluespec FloatingPoint library should offer a significant speedup. Provided that
the pipeline of the unit is fully occupied, it is possible to retrieve the result
of a floating point operation in every cycle. This feature will later be
exploited to greatly reduce the number of compute cycles within an individual
functional unit.

% TODO: Move to evaluation (?)
In order to simulate our simple module, we make use of Bluesim. Our Bluespec
Verilog is compiled directly into an executable, with no need to write a
testbench program in C++ as is required by Verilator. 

\section{Functional Unit Design}
The base compute unit that executes an issued command is known as a functional
unit. In the context of matrix multiplication, a single functional unit is
responsible for calculating a single element in the final matrix $C = AB$, by
calculating the dot product between the ith row of A, and the jth column of B,
as follows: $C_{ij} = A_{ik}B_{kj} = \text{row}(A,i) \cdot \text{col}(B,j)$.
This involves adding the pairwise result of multiplications to an accumulator.

\subsection{Fused Multiply-Accumulate (FMA)}
We make use of a floating-point fused multiply-accumulate (FMA) module. These are used
for addition and multiplication within the FPU of a processor. This allows the
floating-point operation $(a*b)+c$ to be performed with only a single rounding
step, thus saving compute cycles.

An FMA unit saves cycles compared to an individual floating-point multiplier and
adder circuit, as it only performs the floating-point rounding procedure once
both the multiplication and addition procedures have completed.

The simplest functional unit consisting of a fused-multiply accumulate circuit
should take $11$~cycles per element for an $N$~element vector.

% Cannot exploit pipelined nature of unit.
% Faster than individual FP multiply, then add. 10 cycles vs 14 cycles.

% TODO: move to eval
% 10: FSM Start
% 20: Init
% 40: PUT: 00000000 40800000 40800000
% 40: STATE_LOCK
% 140: 0: MulAcc Result: 41800000
% 140: STATE_READY
% 150: PUT: 41800000 40800000 40800000
% 150: STATE_LOCK
% 250: 1: MulAcc Result: 42000000
% 250: STATE_READY
% 260: PUT: 42000000 40800000 40800000
% 260: STATE_LOCK
% 360: 2: MulAcc Result: 42400000
% 360: STATE_READY
% 370: PUT: 42400000 40800000 40800000
% 370: STATE_LOCK
% 470: 3: MulAcc Result: 42800000
% 470: STATE_COMPLETE
% 480: Output: 42800000


% TODO: FMA diagram
% TODO: Explain how FMA works and advantages...
% TODO: Explain design decisions

\subsection{Pipelined Multiplication with Addition (PMA)}
We observe that the pipeline is not at maximum occupancy using an FMA unit. This
is because we need to feed the accumulator result back into the multiplier when
computing the accumulated product of the next two elements. As a consequence, we
waste $10N$~cycles throughout the computation for computing each element due to
the fact that the pipelined nature of FP hardware is not fuly utilised.

We observe that by having an internal buffer of memory within our functional
unit, we can pipeline our floating point multiplications. By subsequently
accumulating our result using a floating point adder, we observe that addition
will take $6N$ cycles, while the pipelined multiplication step will take $11+N$
cycles.

\begin{align*}
  T &= (11 + N) + 6N \\
  T &= 11 + 7N
\end{align*}
\subsection{Pipelined Multiplication \& Pairwise Addition (PMPA)}
In an attempt to maximise the occupancy of the addition pipeline, we propose a
more hardware-intensive strategy that makes use of pipelining for both the
multiplication and accumulation steps.

As before, in our first stage, we perform pipelined multiplication of our
elements into a buffer. This will take $11+N$ cycles.

We then perform reductive pairwise addition of the elements in the buffer.
The pairwise addition procedure has the advantage that we may fully occupy the
FP Adder pipeline when iterating through pairs of elements.
It should take $\log_2{N}$ reductive steps, where each step consumes
$6+\frac{N_{i}}{2}$ cycles, the sum of the cost of flushing the
addition pipeline and pairwise adding all elements in our internal buffer.

\begin{align*}
  T &= 6\log_2{N} + \left(\frac{N}{2} + \frac{N}{4} + ... + 1\right) + \text{const.}\\
  T &= 6\log_2{N} + N + \text{const.}
\end{align*}

The recursive algorithm only works cleanly when the size of our buffer is a
power of two. A naive implementation would therefore waste processing cycles
performing pairwise additions of the elements for the entire buffer size, rather
than the size of our input vectors. Instead, after each step, we guarantee that
our buffer is multiple of two, and collect together an array of remainder
elements. The power of this algorithm can be understood through binary
representations.

\begin{align*}
  N_i = 255_{10} = 11111111_{2}
\end{align*}

After each pass, we extract a remainer if it exists, then bitshift $N_i$. At
most, the binary representation of the number of elements in our input vector is
the number of 1s in the binary representation of our input vector $N$. Therefore, there will
be a maximum of $\log_{2}{N}$ remainder values to place in a seperate buffer and
accumulate. The accumulation process will therefore take at most $6\log_{2}{N}$ cycles.

Therefore, the total cycle consumption is:
\begin{align*}
  T &= (N) + (6\log_2{N} + N) + (6\log_2{N}) + \text{const.} \\
  T &= 2N + 12\log_2{N}
\end{align*}

In this way, we've managed to very substantially reduce the linear constant
factor through the use of pipelining. This is where Bluespec Verilog shines,
scheduling complex pipelining behaviour through the extensive use of FIFOs, and
avoiding hardware race conditions is an incredibly challenging feat. It would be
an excruciatingly difficult process to write this in standard Verilog HDL. 

\subsection{Summary of Predicted Performance}
% Unit Design: TODO: Include diagrams
We observe that through further pipelining optimisations we substantially reduce
the amount of cycles required to calculate a matrix element using an individual
functional unit. Generally speaking, by using a more complicated functional unit
design, we can obtain a lower linear coefficient with the cost of a greater
constant term.

\begin{align*}
  T_{\text{FMA}}(N) &= 11N \\
  T_{\text{PMA}}(N) &= 7N + \text{const.}  \\
  T_{\text{PMPA}}(N) &= 2N + 6(\log_2{N} + \log_2{B}) + \text{const.}
\end{align*}

\subsection{Multiple Functional Units \& Multi-ported memory}
We observe that the rate at which we issue instructions to the accelerator
exceeds the compute speed of an individual functional unit. Crucially, our firmware stalls
its execution and waits for the accelerator to complete its task before issuing
a new instruction. In this circumstance, we are compute-bound.

We seek to exploit the parallelism offered in hardware design by duplicating our
functional units, sequentially issuing commands to different functional units
without stalling our processor's execution. By keeping our processor occupied,
we maximise throughput. With multiple functional units executing their
instructions simultaneously, we should be able to obtain a compute speed
improvement of a multiple of the number of functional units we have.

A standard BRAM RegFile in BSV has 2 read ports and 1 write port. As such, in a
single clock cycle it is only possible to read from two random access locations,
and write to one location of a RegFile. Thus, we must modify the architecture of
our memory to support multiple functional units.

\begin{figure}[h]
\scriptsize
\begin{verbatim}
 Error: "MultiPortBRAM.bsv", line 18, column 37: (G0002)
 `pmem_mem_0.sub' needs more than 5 ports for the following uses:
 `pmem_mem_0.sub a__h30554' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h31995' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32028' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32061' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32094' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32127' at "MultiPortBRAM.bsv", line 18, column 37
\end{verbatim}
\normalsize
\caption{BSV compiler error when over 5 random access reads are scheduled to
  perform in a single clock cycle. A RegFile typically only has 2 read ports.}
\end{figure}

We observe that the standard BSV RegFile, has a maximum of 5 read ports, and 1
write port. We exploit the ability to parameterise our modules in BSV, and
create our own custom MultiPortedMemory module. Our vector of RegFiles will
allocate seperate RegFiles at different locations on the FPGA fabric. They may
be then accessed in parallel.

The BSV interface for our MultiPortBRAM module is similar to that of the RegFile
module. However, the type has an extra parameter, $n$, representing the number of memory
channels. The multi-ported memory has only one write port, but has $2n$ read
ports. This is because it contains $n$ channels of RegFile BRAM and can
distribute its reads to $n-1$ functional units and the AXI bus within a single
clock cycle appropriately.

\begin{figure}[h]
  \centering
\scriptsize
\begin{BVerbatim}
interface MultiPortBRAM_IFC#(type addr, type data, numeric type n);
   method Action upd(addr a, data x);
   method ActionValue#(data) sub(Bit#(8) c, addr a);
endinterface
module mkMultiPortBRAM#(Integer low, Integer high)(MultiPortBRAM_IFC#(addr, data, n))
provisos(
   Bits#(addr, addr_sz),
   Bits#(data, data_sz),
   Literal#(addr)
   );
   // Create N channels of memory
   Vector#(n, RegFile#(addr, data)) mem <- replicateM(mkRegFileWCF(fromInteger(low), fromInteger(high)));
   method Action upd(addr a, data x);
      for(Integer i=0; i<valueOf(n); i=i+1)
         mem[i].upd(a, x); // Update ALL internal memory channels
   endmethod
   // Read from the channel with index 'c'.
   method ActionValue#(data) sub(Bit#(8) c, addr a);
      return mem[c].sub(a);
   endmethod
endmodule
endpackage
\end{BVerbatim}
\normalsize
\caption{Multi channel BRAM hardware module, parameterised by the number of
  channels, n, and the low and high allocation indices to determine the internal
BlockRAM size.}
\end{figure}
When we write to our accelerator memory through the AXI4 bus or from one of our
functional units, we write to all memory channels. A write operation can only be
performed at one random access memory address per cycle.

Our new multi-ported memory is read at a different channel by each different
functional unit. As such, we are capable of reading memory at the maximum rate
possible. Our individual functional units do not use too many RegFile read ports.

% TODO: Discuss somewhere else?
% This leads to one of the key development problems experienced, rule conflicts.
% By default, all rules with a true predicate execute on each cycle unless there
% is a conflict between them.


\chapter{Linear Algebra Peripheral: Evaluation}
\section{Methodology}
\subsection{Benchmarking \& Counting cycles}
From the Risc-V specification, we take a look at the mcycles and minstret
registers. We observe that it is possible to read from CSRs by running the
appropriate instructions within Risc-V assembly. We therefore make use of inline
assembly. The Risc-V specification highlights the \texttt{mcycles} CSR, that is
responsible for storing the number of cycles that the processor has performed.
This can be accessed from our firmware, and used to provide an estimate of the
runtime of one of our algorithms. Crucially, it is an overestimate and there is
some inherent overhead of cycles taken to read from the CSR itself, this
cycle overhead itself can differ depending on whether the stack is pre-allocated
or not when the instruction is ran or other factors dependent on how the
compiler decides to optimise the C firmware when it is compiled into Risc-V
instructions within machine code.

\begin{figure}[h]
\begin{lstlisting}[language=C,style=customc]
#define read_csr_safe(reg) ({ register long __tmp asm("a0");  \
      asm volatile ("csrr %0, " #reg : "=r"(__tmp));          \
      __tmp; })
void dummy_function() {
  uint32_t time0, time1;
  time0 = read_csr_safe(cycle);
  // [...]
  time1 = read_csr_safe(cycle);
  printf("Run cycles: %d\n", time1-time0);
}
\end{lstlisting}
  \caption{Methodology for determining the number of cycles elapsed from within
    our bare-metal firmware, making use of the mcycles CSR and inline assembly.}
\end{figure}

\section{Results}
\subsection{Functional Unit Type}
We seek to benchmark the execution time of the individual functional unit types.
Clearly, for a large $N$ with large matrices, our pipelined multiplication \&
pairwise addition unit is the most cycle efficient. Yet it is vital to consider
the cases where the constant offset plays a role. This is particularly relevant
for 'flatter' matrices, where the size of our input vectors entering our
functional units are smaller.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./py/c2_fut.pdf}
  \caption{Cycle counts of performing a hardware dot product of varying input
    vector sizes for three different functional unit designs. We benchmark the
    cycle count of computations for our fused multiply accumulate (FMA),
    pipelined multiplicaton then addition (PMA) and pipelined multiplication and
    pairwise addition (PMPA) hardware modules.}
  \label{fig:c2_fut}
\end{figure}

Our predicted cycle counts for each of our functional units are close to the
values obtained in simulation.
From our data (Figure \ref{fig:c2_fut}), we observe that the cycle time of our \texttt{PMA} unit is $9N$,
as opposed to our theoretical prediction of $7N$. This is due to additional overhead in
switching the FSM of the accelerator after the pipelined multiplications are
complete. The cycle time of our \texttt{PMPA} unit is $4N$ instead of $2N$ for
the same reason relating to the switching of our FSM. Fortunately, our predicted
cycle count for our \texttt{FMA} unit of $11N$ is correct.

We observe that our cycle count for our \texttt{PMPA} unit is no longer
monotonially increasing. Interestingly, the cycle count drops very significantly when our input vector
size is a power of 2. This is because the cycle overhead of summation of
remainders for each 1 in the binary representation of the input vector size
disappears once the size of our input vectors are powers of 2. This overhead
significantly exceeds the extra time taken to compute the dot product result
with an extra element, thus explaining why the cycle count decreases.

Alternatively, with more complex logic and conditional checks. Perhaps by
dummy-writing zero cells in our buffer. We may, in principle, be able to avoid
the $6\log_2{N}$ overhead associated with accumulating our remainders by appending
a zero to our pairwise reduced vector whenever it is odd.

Crucially, our \texttt{PMPA} module significantly outperforms the other
functional units. This can be attributed to the extensive use of pipelining in
order to perform the computation. Consequently, the hardware maximises the
amount of computation throughput and occupancy of our floating point units
multiplier and adder per cycle.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./py/c2_fut_zoom.pdf}
  \caption{A enlarged view of cycle counts of performing a hardware dot product of varying input
    vector sizes for small matrices.}
  \label{fig:c2_fut_zoom}
\end{figure}

As predicted, for small matrices, $3 \times 3$ or below, (Figure
\ref{fig:c2_fut_zoom}), our \texttt{FMA} circuit is superior due to the lower
constant term associated with simpler hardware. Our pairwise addition steps
within our \texttt{PMPA} unit fail to fully saturate our addition pipeline.
After each pairwise pass we must wait for the floating point unit to flush
before repeating the process. Since the number of elements in our input vector
is small, we never reach full pipeline occupancy and obtain the speed
improvements we otherwise would with larger input vector sizes. For matrices
that are larger than this, the highly pipelined \texttt{PMPA} module is
superior.

The greatest disadvantage of functional units that are not the \texttt{FMA}
module is the reliance on an internal \texttt{RegFile}, which will be
synthesised as BlockRAM. Crucially, we must maintain this internal buffer of
memory. Moreover, the dot product of input vectors exceeding the size of our
internal buffer cannot be calculated in a single pass of our functional unit.

If we seek to multiply matrices that exceed the size of our internal buffers, we
must write appropriate firmware to split the computation up, and we run into our
bottleneck associated with the rate at which we can issue instructions.

We observe that our PMPA and PMA functional units could still be improved.
Currently, the internal structure of the PMPA and PMA units is that of a finite
state machine where the units are either performing the multiplication, or the
addition step. In principle, with additional work and a lot of care to prevent
rule conflicts and race conditions, it would be possible to extract a further
speed improvement by performing both of these tasks in parallel, as the addition
procedure may indeed be started before the multiplication process is complete.

\subsection{Functional Unit Parallelism}
We make use of the fused multiply-accumulate functional unit for benchmarking
purposes. This ensures that our individual functional units are saturated, and
that our parallelism bottleneck is reached with a greater number of functional
units. Our pipelined multiplication with pairwise addition unit is significantly
faster and will therefore reach the bottleneck with fewer functional units, as a
result we will need to multiply larger matrices together to obtain the same
demonstration of this bottleneck.

We seek to benchmark the multiplication of large matrices. We multiply a
$128\times 4$ and $4\times 128$ matrix with our accelerator. We copy our
matrices from our ROM to our accelerator. We then generate commands
within our firmware and dispatch them to our accelerator. We record the number
of cycles taken to compute our result within our firmware,
through using the \texttt{mcycles} CSR, and record the cycle time for our
hardware peripheral to complete the computation within the verilator simulation.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./py/c2_fu_unbuf.pdf}
  \caption{Cycle count of matrix multiplication of a $4 \times 128$ and $128
    \times 4$ matrices for a varying number of functional units. Compiled with
    arguments: \texttt{riscv64-unknown-elf -O2 -fno-inline -march=rv32imafc -mabi=ilp32f}}
  \label{fig:c2_fu_unbuf}
\end{figure}

\begin{table}[h]
  \centering
  \begin{tabular}{r|rr}
    \toprule
    Units & Peripheral & Processor \\
    \midrule
    0&?&21383\\
    1&14658&15155\\
    2&7572& 7957\\
    4&4247& 4844\\
    8&3986& 4570\\
    12&4133& 4707\\
    16&3946& 4581\\
    \bottomrule
  \end{tabular}
  \caption{Cycle count recorded within our hardware (Peripheral) and within our
    firmware (Processor) for our matrix multiplication benchmark. Overhead: Copy cycles: 4982}
  \label{table:c2_fu_unbuf}
\end{table}

From our plots (Figure \ref{fig:c2_fu_unbuf}) we observe that our cycle count
stops decreasing after 4 functional units are used simultaneously. As a
consequence, we are not reaching maximum utilisation of our compute resources on
our accelerator. This is because the rate at which we can issue instructions
becomes a bottleneck.

We observe a $\approx 300$~cycle discrepancy between our cycle counts when the
operation completes on our accelerator and our firmware. This is the overhead
associated with the firmware reading accelerator memory to determine if there
are available compute resources to issue an additional instruction.

Nevertheless, we do obtain a significant performance improvement. If we pre-load
our data onto the accelerator, we observe a $4\times$ cycle count improvement
with 4 functional units when multiplying large matrices. Upon further
optimisation and the use of \texttt{PMPA} units and a more optimised
communication protocol, we will most certainty obtain superior results. 

\subsection{Synthesis}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./img/accel_dg.png}
  \caption{The matrix accelerator attached to the SoC created in Chapter 1 for
    the purpose of synthesis within Xilinx Vivado.}
  \label{fig:accel_dg}
\end{figure}

Care was taken to ensure that the resources of the FPGA would not be exceeded,
by limiting the total number of functional units to 4, and therefore the memory
channels to 5. With $64$~KB of memory per functional unit, we would use $320$~KB of FPGA
BlockRAM. This is almost the entire majority of remaining cells once the CPU ROM
and RAM has been flashed. Crucially, we reduce the size of our memory regfiles
to $16$~KB, making our peripheral BlockRAM consumption fall to $80$~KB.

In the process of synthesis we discover one of the disadvantages of
parameterised hardware design. A very substantial amount of hardware may end up
being generated when only a single number is changed. Crucially, the abstraction
introduced results in a decrease of intuition for the 'finite' resource
allocation of the physical FPGA chip.

We perform synthesis with our peripheral and package it as an IP block using
Xilinx Vivado. Vivado detects that it is an AXI4 slave interface, and it is
simple and easy to connect the peripheral to the SoC, as can be seen in Figure
\ref{fig:accel_dg}.

However, we encounter a crucial disconnect between our open source development
methodology and the proprietary block design tool. The AXI4 bus interconnect within
Vivado has a 32-bit data width, whilst the AXI fabric from our simulations has a
64-bit width. Furthermore, there are discrepancies in the \texttt{ID\_WIDTH}
property. A degree of additional work and analysis of the specifics of
generating Xilinx-compatible AXI4 peripherals from Bluespec must be conducted.

\chapter{Communication Bottleneck: Further Work}
From our earlier results, we observe that at some threshold, a greater number of
functional units does not necessarily translate into a greater accelerator
throughput. 

\section{Command Buffering}
% \subsection{Firmware/Stack}
% To address the issue of not being able to issue commands fast enough, we
% pre-generate all of our commands in the stack such that we don't waste cycles
% generating commands in-between issuing them. For firmware buffering, $20$ bytes of
% memory must be reserved for each instruction issued. It is easy to see that this
% is impractical. If our final matrix is $64\times 64$, we would consume a very
% substantial $80$~KB on the stack for buffering these instructions. We are
% limited both in terms of stack size, and BlockRAM. 

% % TODO: include firmware
% \begin{table}
%   \centering
%   \begin{tabular}{r|rr}
%     \toprule
%     Units & Peripheral & Processor\\
%     \midrule
%     0&?&21383 \\
%     1&14658&15155 \\
%     2&7458& 7957 \\
%     4&4050& 4651 \\
%     8&3855& 4351 \\
%     12&3989& 4520 \\
%     16&3955& 3329 \\
%     \bottomrule
%   \end{tabular}
%   \caption{Cycle count recorded within our hardware and firmware when buffering
%     instructions within firmware. Buffering: Overhead: Buffer cycles: 696. Copy cycles: 4982}
% \end{table}

% We observe very minor performance improvements, but nothing too substantial.
% Whatever performance increase that is incurred may equally be attributed to
% compiler optimisations, as we instruct GCC to optimise our produced machine
% code with the \texttt{-O2} parameter. 

% One of the problems with firmware buffering is that the memory wall of issuing
% commands to the accelerator still exists, we still rely on the AXI4 bus latency
% to write to accelerator memory.

% \subsection{Hardware}
For command buffering, we pre-emptively issue commands into an 'instruction'
region of hardware and set an associated 'program counter' on execution.
Instructions are then issued each cycle removing the associated AXI4 memory bus
latency. We may have seperate regions of preloaded commands, and store only the
start addresses. As a consequence we need only set the program counter and mark
execution to perform an entire matrix multiplication.

One of the principal issues encountered during development was infinite stalling
due to conflicts between BSV rules. The rule which decodes an
instruction reads from and writes to a \texttt{server\_busy} vector of
registers. The rule which writes the result of a functional unit computation to
memory also reads from and writes to the same \texttt{server\_busy} vector in the
same cycle. As a result, while the execution of an instruction is stalling
within hardware (as opposed to firmware), the conflicting rule that actually
writes into memory and marks the computation as complete does not fire. We must
further experiment with using \texttt{FIFOs}, and \texttt{CReg} concurrent registers
to avoid this sort of rule conflict. Further time must be spent to fix this rule
conflict.

\section{Firmware Stalling/Interrupts}
One of the potential criticisms of the technique of constantly polling a region
of memory is that hardware interrupts were designed for the very purpose of
removing the need for this to happen.

On an ARM system, the hardware interrupt latency is $\approx 12$
cycles\cite{arm-irl}, around the same number of cycles to perform an uncached
memory request over the AXI4 bus. On a Risc-V system, interrupt behavior is
exceedingly complicated. A Risc-V processor
has, by default, a memory-mapped peripheral known as a Platform Level Interrupt
Controller (PLIC). The PLIC is responsible for multiplexing multiple interrupt
requests of different priorities. While the PLIC registers interrupts and stores
the necessary information for prioritising them, it is the role of the firmware
to actually perform an associated task when an interrupt occurs.

From the SiFive interrupt cookbook\cite{sifive-ir}, we see:
\begin{lstlisting}[language=C,style=customc]
void machine_external_interrupt()
{
   //get the highest priority pending PLIC interrupt
   uint32_t int_num = plic.claim_complete;
   //branch to handler
   plic_handler[int_num]();
   //complete interrupt by writing interrupt number back to PLIC
   plic.claim_complete = int_num;
}
\end{lstlisting}

Crucially, reading the highest priority interrupt from the PLIC is a memory
read operation via the AXI4 bus and has a latency equal to that of polling our
accelerator memory. There is then the additional overhead of marking the
interrupt claim to be complete, and running the interrupt handler. Within our
interrupt handler we would need to then read from our accelerator memory to
determine which of our functional units are busy before issuing a new command.
There is also the additional overhead of moving the variables in all of our
registers into the stack when the interrupt function is called. We observe that
the latency of processing interrupts greatly supercedes that of looping and
polling a region of memory to determine whether an operation within the
accelerator has completed.

\section{Instruction jumping}
One of the more novel ideas for getting a marginal speed-up in issuing commands
is to reduce the processor branching overhead by incoorporating the stalling
logic into the processor itself. When making use of a for loop for branching,
processor cycles are wasted continuously loading the branch instruction, and
flushing the pipeline. We observe that this overhead can be removed by moving
the program counter into a region of memory within our peipheral that jumps back
to itself. This region can then be overwritten to jump back to the address after
the stalled section in C firmware when appropriate. Thus, we eliminate the
overhead of fetching a branch intruction to loop within firmware.

For testing purposes, we attempt to execute some example code within our
accelerator memory. We  

\begin{figure}[h]
\begin{lstlisting}[language=C,style=customc,xleftmargin=0.05\textwidth]
// MMIO PC
void __attribute__((noinline, section(".dummy_section"))) func_test() {
  print("Testing!\n");
}

// Linker script modified to mark start and end address of our function above
extern unsigned char dummysec_start[];
extern unsigned char dummysec_end[];
void main() {

  uint32_t func_size = dummysec_end-dummysec_start;
  /* printf("Func Size: 0x%X\n", func_size); */
  /* for(int i=0; i<func_size+4>>2; i++){ */
  /*   uint32_t* ft = (uint32_t*)&func_test + i; */
  /*   printf("\t%X - %X\n", ft, *ft); */
  /* } */
  memcpy((void*)(ACCEL_STAT_ADDR+8), &func_test, func_size);
  /* printf("Func Size: 0x%X\n", func_size); */
  /* for(int i=0; i<func_size+4>>2; i++){ */
  /*   uint32_t* ft = (uint32_t*)(ACCEL_STAT_ADDR+8) + i; */
  /*   printf("\t%X - %X\n", ft, *ft); */
  /* } */
  /* // Move func_test TO stupid SoC chip! */
  //asm volatile ("call % " :: "r"(&func_test));

  ((void (*)(void))0x80010C98)(); // call @ &func_test within RAM -- works
  ((void (*)(void))0xC0002008)(); // call @ ACCEL_STAT_ADDR+8 within Accelerator
  // Processor stalls indefinitely...
  print("Success");
}
// OUTPUT:
// Testing!
\end{lstlisting}
  \caption{Source code to trick the program counter to move into accelerator memory.}
\end{figure}

The processor stalls because the instruction cache does not know how to forward
the instruction fetch request to accelerator memory. Within our processor
design, our accelerator is marked as I/O memory, and is therefore inaccessible
to the instruction cache. Since we have based our work on the open source
Piccolo processor. It would be possible, with additional work, to enter the
internals and trick the instruction cache (\texttt{MMU\_Cache.bsv}) to fetch the
instruction at a particular address within our peripheral. This would not be
possible with a closed source system.

\section{Further Work}
\subsection{Protocol efficiency/optimisation}
One of the most obvious ways of improving the communication rate is to compress
or improve the communication procotol. One of the ways in which we improve the
protocol is by packing our struct. As a consequence, our instruction that is
issued to the accelerator has fewer bytes, and fewer cycles are required to copy it.

To a degree, it is possible to compress our instructions. The addresses passed
to the accelerator are 32-bits. It is possible to instead directly provide the
16-bit offset instead. If we align our accelerator in our memory map
appropriately, the offset may be inexpensively obtained through a typecast
(\texttt{0xC0002900 (4B) -> 0x2900 (2B)}). This would reduce the size of our
instructions by 6 bytes.

With more time, the monolithic approach could be pursued where the
instructions to compute each element in our final matrix are generated and
scheduled automatically within our hardware. This would perform the same
speed-up as hardware buffering, and would avoid the associated memory cost.

% \subsection{Memory Striping}
% One of the big problems with our accelerator is the design of the multi-ported
% memory. By duplicating our memory writes across $n$ channels for $n-1$
% functional units, we occupy a very significant amount of BlockRAM on our
% accelerator.

% % TODO: Synthesis
% % The process of synthesis becomes a very delicate balance between...
% Given more time, we could instead stripe our memory into separate banks, with a
% low probability that more than two indexes are read from the same bank in the
% same cycle. While this is unlikely due to the nature of memory accesses for matrix
% multiplication, this can be made easier by also duplicating the striped memory
% banks as shown previously.

% % TODO: Analysis: Math to show memory consumption.
% One of the most important considerations when implementing this form of striped
% memory is how to apply the associated back-pressure and arbitration when two functional units
% are reading from the same memory bank in the same channel. One of the units
% would need to be stalled. It is likely that the processing and scheduling logic
% for this would lead to wasted cycles, and therefore a reduction in compute
% speed. It would be an interesting exercise to see how this reduction in compute
% speed compares to the savings in BlockRAM.

\chapter{Summary and Conclusions} 
\section{Summary}
We have successfully made use of open source resources to synthesise a minimal
functioning Risc-V SoC. This system acts as a suitable baseline and allows us to
rapidly develop and prototype a linear algebra co-processor along with the
associated firmware to communicate between the co-processor and our Piccolo
Risc-V core. We successfully fixed the problems within the source code of the
Piccolo FPU. With a vague and unclear ISA specificaton, we used our ability to
inspect the internals of the open source processor to determine how to prevent
our processor from trapping when a floating point instructon is executed.

We achieve a significant performance improvement without sacrificing floating point
accuracy through exploiting the pipelined nature of the floating point Bluespec
Verilog libraries and designing our own highly pipelined multiplication pairwise
addition (PMPA) functional unit, which is almost three fold faster than a
conventional fused-multiply accumulate unit that is used within the floating
point units of conventional microprocessors. Through designing our own
parametised multi-channel memory module we take advantage of the inherent
parallelism offered by hardware and demonstrate a further significant computation
speed increase through the use of multiple functional units. However, it is
vital to consider that our multi-channel memory is highly inefficient,
especially because FPGA BlockRAM is a sparse resource. We observe that the
limiting factor preventing further speed-up is the rate at which we can issue
instructions to our co-processor and we contemplate ways of increasing this.

\section{Conclusions}
% Vector ISA vs Accelerator
We struggle to provide an associated Risc-V vector ISA benchmark within our
simulations as it has not yet been ratified. The SSE instructions on x86
systems can compute 4 values per cycle, so a cycle count reduction by 2-4 fold
would be expected of the vector ISA for it to be competitive. We observe that in
our earlier sections, we observe at most a 4 fold performance increase when
multiplying large matrices. With further work addressing the communication
bottleneck, and an ability to issue instructions at a faster rate and make the
protocol more efficient, we have evidence to suggest that with further work
addressing this bottleneck, we can obtain a greater performance increase than a
vector ISA.

% Benefits and disadvantages of BSV
We suceeded in our aim of making good use of high level hardware design
languages. BSV makes it simple to add additional functionality to our
accelerator. The command struct may be replaced by a tagged union and different
commands may be decoded into different instructions with different arguments.
For instance: if we want to accelerate neural network inference, we may pass
commands that apply activation functions to large vectors of data in parallel.
With the data operated on all sharing the same memory module within the
peripheral, there would be no overhead associated with data movement between
issuing different instructions. Thus, our design can easily be built upon by
others and extended.

One of the shortcomings associated with the use of Bluespec Verilog observed
through the project, was that it is more difficult to keep an intuitive track of
the amount of FPGA resources used upon synthesis. We have limited BlockRAM on
our FPGA and this was particularly apparent when adding more functional units.
Due to the duplication of accelerator memory across multiple channels and the
parameterisation of our functional unit modules, small changes in configuration
or constants may have a significant impact on the amount of hardware that is
generated.

We underestimated the predicted cycle counts for our pipelined functional units
by $2N$. This is partly because cycles were wasted while the FSM was changing
state. There may also be an additional overhead where a server request cannot be
given to a floating point unit at the same cycle a response is received.
Crucially, despite the fact that Bluespec offers superior scheduling support,
the only way to accurately determine the cycle counts of performing operations
with our hardware is through \texttt{\$display(...)} statements within
simulations. It is therefore difficult to adhere to strict timing requirements
using this language.

% Failure of synthesis
We have verified the feasibility of our minimal SoC through synthesis and clearly
demonstrated the performance advantages through simulations. However, one of the
principal shortcomings were the problems encountered in attaching our
accelerator to the rest of our SoC within Xilinx Vivado. While open source tools
were used in the development process and for simulations, we must still use the
proprietary tools for synthesis, and the format of the generated Verilog RTL for
an AXI4 peripheral written in BSV is incompatible with the Xilinx Vivado block
design tool.

In retrospect, for the purpose of generating bitstreams and performing place and
route algorithms, it was unwise to fully seperate the development process from
the computer aided design tools (Xilinx Vivado). This project reveals that the
process of synthesis and SoC design are very strongly coupled to each other, and
our accelerator should have ideally be designed and synthesised on a CAD
platform to ensure that synthesis is successful at each stage. In this situation
our peripheral would have had to conform to the appropriate AXI4 data bus width
supported by Vivado Studio from the very beginning. 



% NOTE: Clearly present argument and demonstrate that success criteria were
% met
% NOTE: Critical thought and interpretation of results
% NOTE: substantiate claims of success/novelty

% NOTE: Reflect on lessons learnt
% 'V' not ratified --- comparison would be nice

% Future Work
% Further work towards Synthesis --- make use of external DDR3 memory
% Implement Risc-V 'V' extensions

\appendix
\singlespacing

\printbibliography
% Bibliography:
% RISC-V GCC, ISA, Spike
% BSV Reference Guide

\end{document}
