%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%% 
%% 
%% SMH, May 2010. 

\documentclass[a4paper,8pt]{report}
% \documentclass[a4paper,12pt,twoside,openright]{report}


%% 
%% EDIT THE BELOW TO CUSTOMIZE
%% 

\def\authorname{Khalid Aleem\xspace}
\def\authorcollege{Trinity College\xspace}
\def\authoremail{ka476@cam.ac.uk}
\def\dissertationtitle{A Risc-V based co-processor peripheral to accelerate
  linear algebra operations.}
%\def\dissertationtitle{A Risc-V based co-processor to accelerate linear algebra operations.}
% TODO: LOOK INT LACore...
% Maximising throughput for a Risc-V linear algebra co-processor.
% XXX: Project title changes: no later than 21 May 2021 16:00 
\def\wordcount{3,860}


% \usepackage[dvips]{epsfig,graphics} 
\usepackage{epsfig,graphicx,verbatim,parskip,tabularx,setspace,xspace}

\usepackage[english]{babel}
\usepackage{biblatex}
\addbibresource{bibliography.bib}

\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{fancyvrb,listings}

\usepackage[a4paper, margin=2cm]{geometry}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily\color{black},
  keywordstyle=\bfseries\color{blue!40!black},
  commentstyle=\itshape\color{purple!40!black},
  stringstyle=\color{green!40!black},
  morekeywords={uint32_t,uint8_t,uintptr_t,intptr_t}
}

\lstdefinestyle{customcpp}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  xleftmargin=\parindent,
  language=C++,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily\color{black},
  keywordstyle=\bfseries\color{blue!40!black},
  commentstyle=\itshape\color{purple!40!black},
  stringstyle=\color{green!40!black},
  morekeywords={uint64_t,uint32_t,uint8_t}
}

\input{lst-riscv.tex}

%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing


\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 
\section{Motivations}
% NOTE: Clear motivation - justify the benefits of success
% NOTE: Contextualise what i'm building
% NOTE: Analysis of requirements and justified & documented use of tools!



% TODO: Altera: https://dl.acm.org/doi/10.1145/2847263.2857658
% https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf 
% Incredibly expensive: EUV
With the end of Moore's law approaching, transistors can no longer be further
miniaturised and single-digit nanometre lithography techniques remain incredibly
expensive. In this environment, it is increasingly common to have accelerators
within system-on-chips (SoCs), where hardware-level specialisation is vital for
good performance in compute tasks. Given the recent acquisition of Xilinx by
AMD, and Altera by Intel in 2015, the dominant microprocessor manufacturers have
both entered the realm of reprogrammable hardware. It therefore seems
increasingly likely that an FPGA fabric may be placed within microprocessors for
creating specialised accelerators, where incremental \textit{hardware updates}
may be applied to custom processing pipelines. This is of vital importance in
performance and safety critical systems, such as: self-driving cars.
Hardware-level vulnerabilities, such as: Spectre and Meltdown, can give a
malicious adversary a substantial attack surface that would not exist if the
vulnerable sections of hardware could be reprogrammed appropriately.

Coupled with the recent massive growth of open-source FPGA tooling, high-level
hardware design languages, and publically available Risc-V processor designs, it
has become substantially easier to design, simulate, and deploy a highly
customised accelerator. Our high-level hardware design languages allow our
designs to be parameterised, a particularly important feature when designing
hardware for an embedded device where we have tight silicon area, power and
thermal constraints.

Linear algebra operations are widely used for machine learning purposes in
embedded devices. The existing pressures within the sphere of hardware design
compel us to develop an accelerator for this purpose, where we may make use of
publically available open source tooling. This provides us with a strong
foundation to base our work off.

Previously, it was thought that SIMD and vector instructions would offer a
significant enough performance improvement that they would supercede the
development cost of an accelerator. Crucially, this comes at a cost of
interoperability, where we are constrained to a particular processor model that
has the ISA extension implemented and therefore a greater silicon area and power
consumption. An accelerator developed as a processor peripheral may be used
across different processor types, and ISAs, whilst simultaneously providing a
superior performance enhancement compared to vector instructions. This provides
the designer with greater choice and flexibility, particularly important when
designing an embedded device.

\section{Contributions}
This thesis is split into three main chapters. In the first chapter we perform
vital prerequisite work to obtain a minimal system to base our later work on.
We fix latent problems within the floating point unit of the Bluespec Inc.
Piccolo processor, attach our own proof-of-concept memory mapped peripheral to
the AXI4 bus, and design Risc-V bare-metal firmware to interface with this
peripheral. We make use of a modified Verilator testbench to verify the
correctness of our design and rapidly develop the peripheral. Lastly, we perform
synthesis, run place \& route algorithms and generate a bitstream to verify the
feasibility of our design, demonstrating that we can reach clock frequencies of
10MHz without encountering timing violations.

In our second chapter, we elaborate on our design principles and methodology for
designing our linear algebra accelerator. We implement and benchmark the behaviour
of differently designed functional units of increasing complexity. We also delve
into the memory modifications required to support multiple functional units and
exploit the further parallelism offered by hardware. We determine that the
principal bottleneck in the speed of our accelerator is the rate at which we can
issue instructions.

In our third chapter, we address a range of methods to target the previously
encountered bottleneck. We examine ways of compressing our protocol, and
contemplate alternative designs to our accelerator. We attempt to buffer issued
instructions and reduce the latency between the processor and accelerator to
make them more tightly coupled.

Throughout the development process we minimise our dependence on closed source
and proprietary components and attempt to strongly adhere to open source
principles. We take advantage of this through a rapid development and
prototyping procedure that would not be possible if we are constrained to using
the traditional HDL computer aided design (CAD) tools (Xilinx/Quartus).


\chapter{Background}
% TODO: Hardware design in general(?)
\section{Risc-V Instruction Set}
% TODO: What is an ISA?
The set of operations a CPU may perform is known as the instruction set
architecture (ISA). An ISA provides the means for a
programmer to interface with a processor, allowing software to communicate with
hardware. The ISA also specifies the method of memory addressing, whether
addresses need to be aligned, and the structure of processor instructions. Most
desktop computers make use of the x86\_64 ISA, with embedded devices and mobile
phones commonly using the ARM ISA.


Risc-V is an open standard instruction set architecture (ISA). It is provided
under open source licenses, where anybody may use the ISA with no restrictions
over the use of intellectual property (IP). In contrast ARM ISA, which is widely
used in embedded devices, is proprietary. If the ARM ISA is used
for commercial purposes, the company expects a licencing deal to be made and
royalties to be paid. Licencing deals typically cost millions of dollars and can
take months to negotiate. These IP restrictions hinder the process of academic
research, and significantly raise the barrier of entry for processor design,
where only large companies like Qualcomm can compete.

As such, there is no incentive for processor designs to be made open source.
Obtaining the source code of most ARM processors is both difficult and
impractical due to these strict and costly licencing requirements. Modern ARM
instruction sets are bloated, needing to maintain necessary backwards
compatibility --- in many cases a modern ARM CPU will contain redundant
instructions, and therefore redundant logic and complexity that is unnecessary.

In contrast, the Risc-V ISA is modular, and can be partitioned into various extensions. Of
notable interest are the 49 (\textbf{I}) 32-bit integer instructions, 11
(\textbf{A}) atomic instructions, 36 (\textbf{C}) 16-bit compressed instructions, 8
(\textbf{M}) instructions for integer multiplication and division, and 25
(\textbf{F}) 32-bit precision floating point instructions. Consequently, a
functional 32-bit processor for an embedded system with hardware floating point
support requires only \textbf{129} instructions, with the \textbf{IMAFC}
extensions. This increases the ease of processor design, and avoids the creation
of unnecessary and superfluous hardware. 

% A - 11 (atomic instructions)
% C - 36 (compressed 16-bit instructions)
% RV32I - 49 (32-bit integers)
% M - 8 (integer x and /)
% F - 25 (32-bit floating point instructions)

Since the publication of the Risc-V ISA\cite{Waterman:EECS-2014-54}, a variety of different open source
processors have been designed. The source code of these processors is publically
available, with a range of different designs available on GitHub. Collaboration
and open source processor design has a range of additional benefits in hardware
security. This is of particular relevance in the light of severe hardware-level
vulnerabilities found in the majority of ARM and x86\_64 processors in
production, namely Spectre, and Meltdown. With processor designers and
manufacturers paying less attention to verifying the security of their devices
whilst competing with each other to maximise processor throughput, the design of
processors is not subject to enough scrutiny within the development process.
Open source processor development provides the opportunity for processor designs
to be verified by a community of different organisations, reducing the
likelihood that architectural level attacks can be performed. By basing our work
on an open source processor in the development of our peripheral, we benefit
substantially from these security considerations.

Moreover, when designing a peripheral, it is vital to possess an understanding
of the processor that it is connected to. Using an open source processor gives
us greater freedom, allowing us to pry into the internals, and analyse the
specifics of processor behaviour if things behave in an unexpected manner.

\section{Simulating Hardware} 
The fastest way to test and verify the design of hardware is through simulations.
Creating a silicon application-specific integrated circuit (ASIC) remains an
incredibly costly and time consuming process.
There is no room for rapid prototyping, and hundreds of thousands of units need
to be created and sold at a profit for the process to be economically
feasible. Fabricating a microprocessor at each stage in its development process
for testing purposes is too expensive. As a consequence, there is very
limited room for fabricating an integrated circuit for testing purposes in
industry, and almost no room for fabrication in academic circles.

\subsection{Verilator}
Verilator is a piece of open source software that compiles Verilog into C++. A
testbench can then be created, where the generated C++ class representing the
top-level module in hardware, can be stepped forwards in time with the clock
toggled from low to high accordingly, and signals propagated through simulated
hardware accordingly.

In order to run our verilator simulation, we include our verilator-compiled C++
headers and libraries that were generated from our hardware into our C++ test
bench. We then proceed to initialise the verilator class representing the
top-level module of our hardware design. It is then possible to toggle the clock
signal and propagate our simulated hardware forwards in time. This is done as
follows:
\begin{lstlisting}[language=C++,style=customcpp]
#include <verilated.h>
#include "VTop.h"
uint64_t main_time = 0;
double sc_time_stamp() { // Called by the $time function in Verilog
    return main_time;
}
int main (int argc, char **argv, char **env) {
    Verilated::commandArgs(argc, argv);
    VTop* top = new VTop; // Create top-level module
    while(!Verilated::gotFinish()) {
        if ((main_time % 10) == 5) { // 10 loops per cycle
            top->CLK = 1; // Raise clock at 5, 15, 25, ...
        }
        else if ((main_time % 10) == 0) {
            top->CLK = 0; // Lower clock at 0, 10, 20, ...
        }
        top->eval(); // Propagate changes
        main_time++;
    }
    top->final(); // end simulation
    delete top;
    return 0;
}

\end{lstlisting}

While verilator suceeds in simulating hardware logic, it does not take physical
considerations into account. For instance, every hardware design has a route through
combinatorial logic known as the critical path, this is the circuitry where
speed of propagation leads to a bound on the maximum clock rate that is
achieveable.

% https://www.veripool.org/papers/Verilator_Fast_Free_Me_DVClub10_pres.pdf

\subsection{FPGAs \& Synthesis}
% TODO: image
The most effective way to simulate hardware is by using a Field Programmable
Gate Array (FPGA). An FPGA consists of a large lattice of programmable logic
blocks interwoven by a grid of configurable wires and contact points. The
programmable logic blocks range from boolean logic lookup tables (LUTs), clocks,
registers, DSP units and block RAM. The lattice of programmable logic cells can
be flashed, where the individual hardware units are programmed, and connected
together by configurable wires such that the circuit described in hardware is
physically created. A processor simulated on an FPGA can typically be clocked on
the order of MHz. An FPGA can therefore offer over a thousand-fold performance
increase compared to conventional software simulations of hardware circuits.

Unlike software simulations using verilator, an FPGA can suffer from physical
limitations of hardware layout. A processor that isn't pipelined sufficiently
may have a shorter critical path, preventing the generated hardware on an FPGA
from reaching a sufficiently high clock rate. Furthermore, additional care must
be taken to prevent signal timing violations, and clock domain crossing within
the FPGA.

% TODO: Clock domain crossing?
% TODO: Types of FPGA(?)

\subsubsection{Synthesis}
% TODO: FPGA picture
The process by which an FPGA is programmed with some hardware design is known
as synthesis. The goal of the synthesis process is to produce a bitstream from a
hardware design. The bitstream contains the necessary information required to
program the logic blocks of an FPGA and connect them appropriately.

The first stage of synthesis involves compiling Verilog HDL into a netlist.
A netlist is a complete description of a digital circuit at the logic gate
level, containing all of the information a schematic would, but in a structured
format that is easier for a computer to process.

The second stage involves a process known as place and route. Different FPGAs
have different architectures and designs, with varying numbers of logic cells
and logic cell types. Place and route involved running an algorithm, targetting
a specific FPGA, which figures out a sufficiently optimal placement of logic
cells and connections. Lastly, the design is verified for timing consistency,
and the bitstream is generated.

\subsubsection{Symbiflow}
% Symbiflow - the future. (Find paper) 
% TODO: Cite: https://ieeexplore.ieee.org/document/9103284
The tools required for synthesis are closed source and proprietary, requiring
licenses for use in commercial purposes. Both Xilinx and Altera, the major FPGA
vendors, employ a greater number of software engineers than hardware engineers
within their FPGA divisions to develop their complex closed-source proprietary
software for synthesis.

The Symbiflow project is an open-source computer aided design (CAD) flow used to
program commercial FPGAs. The proprietary bitstream format for the Xilinx
Arty-A7 has been fully reverse engineered.
The Symbiflow project is capable of performing synthesis, placement, routing
and bitstream generation for this device, where no proprietary licence is required. 

\section{Hardware Design Languages}
Designing hardware in traditional Verilog is a highly inefficient process. There
are no data structures to group together inputs and outputs of hardware modules
that represent different communication channels. Therefore wiring a submodule
appropriately is an arduous process, prone to errors. Code that is highly
modularised, counterintuitively, becomes harder to read and understand.
Moreover, if a single wire needs to be modified, it needs to be explicitly
stated in every instance of a module, a time consuming process.

This can be seen as follows:
\tiny
\begin{verbatim}
  mkUART uart0(.CLK(CLK),
         .RST_N(RST_N),
         .put_from_console_put(uart0$put_from_console_put),
         .set_addr_map_addr_base(uart0$set_addr_map_addr_base),
         .set_addr_map_addr_lim(uart0$set_addr_map_addr_lim),
         .slave_araddr(uart0$slave_araddr),
         .slave_arburst(uart0$slave_arburst),
         .slave_arcache(uart0$slave_arcache),
         .slave_arid(uart0$slave_arid),
         .slave_arlen(uart0$slave_arlen),
         .slave_arlock(uart0$slave_arlock),
         .slave_arprot(uart0$slave_arprot),
         .slave_arqos(uart0$slave_arqos),
         .slave_arregion(uart0$slave_arregion),
         .slave_arsize(uart0$slave_arsize),
         .slave_arvalid(uart0$slave_arvalid),
         .slave_awaddr(uart0$slave_awaddr),
         .slave_awburst(uart0$slave_awburst),
         .slave_awcache(uart0$slave_awcache),
         .slave_awid(uart0$slave_awid),
         .slave_awlen(uart0$slave_awlen),
         .slave_awlock(uart0$slave_awlock),
         .slave_awprot(uart0$slave_awprot),
         .slave_awqos(uart0$slave_awqos),
         .slave_awregion(uart0$slave_awregion),
         .slave_awsize(uart0$slave_awsize),
         .slave_awvalid(uart0$slave_awvalid),
         .slave_bready(uart0$slave_bready),
         .slave_rready(uart0$slave_rready),
         .slave_wdata(uart0$slave_wdata),
         .slave_wlast(uart0$slave_wlast),
         .slave_wstrb(uart0$slave_wstrb),
         .slave_wvalid(uart0$slave_wvalid),
         .EN_server_reset_request_put(uart0$EN_server_reset_request_put),
         .EN_server_reset_response_get(uart0$EN_server_reset_response_get),
         .EN_set_addr_map(uart0$EN_set_addr_map),
         .EN_get_to_console_get(uart0$EN_get_to_console_get),
         .EN_put_from_console_put(uart0$EN_put_from_console_put),
         .RDY_server_reset_request_put(uart0$RDY_server_reset_request_put),
         .RDY_server_reset_response_get(uart0$RDY_server_reset_response_get),
         .RDY_set_addr_map(),
         .slave_awready(uart0$slave_awready),
         .slave_wready(uart0$slave_wready),
         .slave_bvalid(uart0$slave_bvalid),
         .slave_bid(uart0$slave_bid),
         .slave_bresp(uart0$slave_bresp),
         .slave_arready(uart0$slave_arready),
         .slave_rvalid(uart0$slave_rvalid),
         .slave_rid(uart0$slave_rid),
         .slave_rdata(uart0$slave_rdata),
         .slave_rresp(uart0$slave_rresp),
         .slave_rlast(uart0$slave_rlast),
         .get_to_console_get(uart0$get_to_console_get),
         .RDY_get_to_console_get(uart0$RDY_get_to_console_get),
         .RDY_put_from_console_put(uart0$RDY_put_from_console_put),
         .intr(uart0$intr));
\end{verbatim}
\normalsize

The traditional HDLs were built to be hardware simulation languages and were not
intended for the purpose of designing very complex hardware, such as processors.
Modern software development programming languages have abstractions, such as
structures and functions, improving productivity and encouraging the reuse of
constructs.

To figure out how a module behaves, one must constantly move around the file,
observing how the state changes when the clock changes, or if registers flip
within an \texttt{always\_ff} block, in many cases used to control a finite state
machine. Simultaneously one must also keep track of how the combinatorial state
of the hardware changes, by looking at an \texttt{always\_comb} block.

Due to the difficulties concerning development time, and for purposes of
efficiency, more expressive hardware design languages have been created, namely,
ChiselHDL and Bluespec Verilog. The Risc-V Rocket processor is designed in
ChiselHDL, whilst the Risc-V Piccolo, Flute and TOOOBA processors are written in
BSV. The creation of a soft-core processor is a demonstration of the ability of
these languages to be applied in practice and make the process of processor
design more efficient and structured.

% TODO: Cite https://people.eecs.berkeley.edu/~krste/papers/chisel-dac2012.pdf 
\subsection{ChiselHDL \& Bluespec Verilog}
Chisel (Constructing Hardware In a Scala Embedded Language), is a new
programming language for hardware design that is based on Scala, that is
executed on a JVM.

Chisel is capable of generating low-level Verilog from high-level hardware
design patterns that are written in Scala.

Chisel allows inputs and outputs of modules to be bundled together, and
modules to be composed with ease. Code may be written in Scala to construct
hardware and therefore speed up the development process.

% Chisel generator / example
% Chisel I/O / Module + Bundle of Bundles

This is best demonstrated with the AXI4 protocol, 
the mess of wire connections, as seen above with standard Verilog, is no longer
an occurence due to the object-oriented nature of Chisel.
\tiny
\begin{verbatim}
class AXIPeripheral extends Module {
  val io = new Bundle {
    val slave = new AXISlaveIF(8, 32)
  }
  [...]
}
\end{verbatim}
\normalsize

Bluespec Verilog offers significant advantages over ChiselHDL. These are
explained more thoroughly in later sections. In Chapter 2, we make use of the
libraries available within BSV to perform floating point computations with ease.
In Chapter 3, we exploit the advanced scheduling capabilities of BSV to issue
computations to different hardware units, and make extensive use of pipelining
for greater concurrency in our floating point computations. We later discuss the
Rocket and Piccolo processors written about in a later section.
% TODO: Cite https://dl.acm.org/doi/abs/10.1109/MEMCOD.2004.1459818 

\chapter{Related Work} 
\section{BlueVec}
% TODO: Cite https://www.cl.cam.ac.uk/~swm11/papers/FPL2013-BlueVec.pdf
The BlueVec co-processor is a vector processor aimed at accelerating neural
network inference designed to address the computation botleneck of the problem
of the \emph{memory wall}.
The memory wall refers to the growing discrepancy between memory bandwidth and
compute power. It is increasingly common for an accelerator system to become
memory-bound, where the latency/clock cycle cost of moving data to and from
memory may even exceed the reduction in compute cycles from having a custom
accelerator pipeline.

% TODO: Explain Memory wall further?
The BlueVec co-processor relies on the observation that in many situations where
a custom accelerator pipeline is implemented to accelerate some computation
through performing it in hardware, a significant improvement may still be obtained
by vectorizing the computation, and implementing the computation within
software making use of SIMD instructions. The paper demonstrates the example of
neural network inference, where the BlueVec co-processor is used in the context
of a digit recognition problem.

The BlueVec co-processor is interacted with through the use of a custom
instruction set extensin for the Nios II. The Altera Nios II is a soft-core processor
that is designed to be synthesised on an Altera FPGA. It has a 32-bit RISC-based
architecture, and while the internals are closed source, it is possible to add
custom instruction set extensions within hardware. Altera provides their
proprietary embedded design suite (EDS), where it is possible to compile C/C++
software targetting the Nios RISC architecture, and making use of the custom
instructions that are implemented in hardware.

The ability to add custom instructions to the Nios processor provides one with
sufficient versatility to fine tune the designed hardware to perform specific
tasks very efficiently, making use of the parallellisation-based strengths of
designing things in hardware.

One of the key ways in which the BlueVec processor tackles the problem of the
memory wall is by ensuring that memory is written to and read from in a
streamlined fashion. Vectorised data is stored in contiguous arrays within
memory, and the BlueVec co-processor highlights that it makes efficient use of
external memory bandwidth through the use of burst memory access to stream data
from external memory at a very high rate.

% TODO: Lane-local memories / Pipelining
% Each thread has its own LANE. --- Similar to multi-channel memory.

In order to maximise the clock frequency of the BlueVec processor, and therefore
maximise throughput, the BlueVec processor pipelines operand fetching,
instruction execution and result writeback. This reduces the length of the
critical path.

% TODO : Link to BSV
The BlueVec co-processor consists of approximately 1000 lines of Bluespec
Verilog and was developed over the course of only a few months. The quick, and
minimalist development practices used during its construction is a testament to
the benefits of the use of BSV over standard Verilog HDL.

One of the principal disadvantages of the BlueVec co-processor is its reliance
on proprietary synthesis tools (Altera Quartus), and the closed source soft-core
processor. The development practices, and soft-core Nios processor involved in
the construction of BlueVec lock us into the Altera software ecosystem. An
Altera Quartus licence in particular is required to synthesise the hardware on
an Altera FPGA and run the associated benchmarks in the paper. We do not have
the ability to easily move the design to a Xilinx, or iCE FPGA, as not only is
the soft-core Nios processor closed source, we must also rely on the
automatically generated Avalon bus interconnect from the Altera Qsys SoC
designer. As such, a substantial amount of work is required for the BlueVec
processor to be moved onto a different platform.

% TODO: Reliability considerations - vendor black boxes. Bugs (if found) -
% impossible to correct. (Another section!?)

Furthermore, we highlight the loss of interopability. We are constrained to the
Nios instruction set. There is no way for an ARM, MIPS, Risc-V, or x86\_64 based
system to interface with, and make use of the vector co-processor. With the
growth of ARM processors used in embedded devices, and the recent emergence of
Risc-V, this lack of interopability is a costly downside to the performance
improvements demonstrated by BlueVec for vector processing.

% TODO: Comment on Benchmarks
% TODO: Bluevec - fails to hit capacity - CPU can't issue one instruction per
% cycle

% NOTE: BlueVec - Literature Review
% NOTE: General argument --- elaborate upon motivations
% NOTE: BlueVec --- Nios. Custom instructions ---> Loss of interopability
% NOTE: Are there truly multiple functional units? Instructions STALL the cpu.
% NOTE: Unadjustable silicon consumption --- less control!

% Google TPU? & Nvidia GPUs? --- Look into

% TODO VLIW?

% Lots of GPU acceleration of linear algebra acceleration 
% Big takeaway - unless super specific problem --- communication bottleneck
% existed
% Whole computation must be performed in GPU memory --- constrained

% Tightly coupled Accelerator+CPU. General programmability + fixed hardware. Low
% latency between each.

% TODO: Look at Linpack library
% https://link.springer.com/article/10.1007/s11390-011-0184-1

%\section{GPU processing}
% The work by ... reveals that ...

% GPUs are slow for small matrices --- CUDA cores not fully occupied 
% https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/joe.2018.9178

% TODO: Cite SIMD
\section{SIMD ISA extensions}
The SIMD acronym refers to processor instructions that operate on multiple
registers of data (Single Instruction Multiple Data). We therefore save
processor cycles through a technique known as DLP (Data level parallelism),
where fewer processor instructions are required in order to perform the same
mathematical operation. High-performance desktop and server systems that use the
\texttt{x86\_64} ISA have the AVX-512 and SSE instruction set extension for
vector processing, while the ARM ISA has the Neon extension for vector
processing on mobile devices.

% TODO: Cite Vector ISA
The Risc-V ISA has the (\textbf{V}) extension to support vector instructions.
That is, instructions that operate on a contiguous region of memory. The ISA
extension specifies the existence of vector registers which may be some power of
2 larger than a traditional 4-byte (32-bit) register. Mathematical operations
may then be simultaneously applied to groups of numbers loaded in these vector
registers.

Crucially, the Risc-V vector extension has not yet been ratified. As such, its
features may change, and it is unwise to implement the instructions as the ISA
may undergo breaking changes. Ratification is a time consuming process. The
Risc-V foundation needs to be certain that the definition of the ISA extension
is infallible, where considerations of backwards compatability and the necessity of
the proposed instructions are verified by a group of experts.

Moreover, the vector ISA extension consists of 184
additional instructions, exceeding the preexisting 129 instructions in the
RV32IMAFC combination. A very substantial modification to the decode and
execution sections of the processor will need to be made at the cost of a great
deal of silicon space. Even if the ISA is implemented, compilers do not yet
support producing vector instructions in generated machine code.

\section{Open Source Cores}
Since we seek to minimise the use of proprietary, and closed-source libraries,
we avoid using proprietary soft-cores. Notably, Xilinx has its Microblaze
processor, while Altera has its Nios II cores. We may base our accelerator on
one of these, but we are immediately locked into either the Xilinx or Altera
proprietary hardware development (CAD) ecosystems. It is not even possible to
simulate an SoC of one of these processors attached to our peripheral without
their proprietary tooling.
Moreover, the proprietary simulation tools offer similar or worse performance
compared to Verilator. Verilator provides us with less-overhead and greater
freedom within our simulations, giving us direct control to our testbench with
C++. In contrast, the proprietary xsim, and modelsim have an associated overhead
of generating a clock and initialising our memory using the bloated the clunky
proprietary CAD tools to simulate our SoC.

\subsection{Rocket \& RoCC}
% TODO: talk about Rocket
% TODO: Cite Rocket Chip Generator:https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.pdf 
The Rocket processor is written in ChiselHDL. It is packed with a utility known
as the Rocket Chip Generator, which generates an SoC consisting of the Rocket
CPU and the necessary peripherals required for a functioning system. 

% TODO: Elaborate: Freedom means physical + commercial use has been demonstrated
% The Freedom E310 is a Risc-V SoC designed by the company SiFive, the Risc-V ASIC
% SoC designed by SiFive is 

The Rocket processor has the Rocket Custom Co-processor (RoCC) interface. This
allows an accelerator to be implemented through the use of custom instructions.
By having the hardware of the processor itself fetch and execute instructions as
opposed to a firmware and memory-mapped approach, the RoCC system saves
processor cycles. This is accomplished by eliminating the associated overhead of
reading/writing from memory via an AXI4 bus.

% TODO: verify this claim
One of the principal disadvantages of the RoCC is its tight coupling to the CPU.
The RoCC constrains us to using the Rocket processor in particular, and alters
the processor's decode and execution units. Because of instruction set
dependency, we are constrained to Risc-V processors, and therefore making
sacrifices in terms of interoperability. 

Further arguments may be made in terms of silicon complexity. Communicating with
an accelerator through an AXI4 bus allows the silicon of the accelerator to be
physically separated from the CPU. This provides us with greater benefits in
terms of heat dissipation in the circumstance that an ASIC is created.

One of the key disadvantages of ChiselHDL is that it does not offer the same
degree of scheduling abstraction that exists with BSV. While Scala gives us the
ability to generate arbitrary and parameterised hardware, it does not provide
the benefits of designing hardware which executes atomic rules and the
processing/execution of complex dependency tree.

\subsection{Bluespec Risc-V CPUs}
% TODO: MIT TOOOBA project
% TODO: Cite paper
The Bluespec designed processors take full advantage of the modular nature of
processor design that is made possible through the use of Bluespec Verilog.
Seperate processor ISA extensions, the processor register width, and the AXI4
bus size may be set appropriately on compilation of the BSV code into Verilog
RTL. The ability of BSV to support atomic rules, and specify complex scheduling
relations makes it particularly suited for processor design.

Bluespec Inc. has created three example processors. The Piccolo processor
consists of a 3-stage in-order pipeline, and is designed primarily for small
embedded systems. The Flute processor contains a 5-stage in-order pipeline, and
contains an MMU, allowing it to support virtual memory and boot a linux OS.
The TOOOBA processor, based on the MIT Riscy-OOO, has a superscalar architecture
and supports out-of-order execution.

All of these designed processors have the advantage of providing an example
testbench created in BSV. The HDL may be compiled and firmware may be executed
within simulations, this provides us with a functioning start point to base our
work on.

%\chapter{Design and Implementation}
% NOTE: Main contribution to (the field?)
% NOTE: Original implementation
% NOTE: Somehow demonstrate - extra-curricular reading?!
% NOTE: Compare the design to BlueVec!?

% NOTE: Challenging goals - substantial deliverables
% NOTE: Design decisions

% RV32imafc - bare metal ELF
% Spike, proxy kernel?
% Cost of movement... memory wall
% MMAP - ARM & RiscV

\chapter{SoC and Testbench Setup: Design \& Implementation}
% TODO: Summarise contribution

\section{SoC Design}
% TODO: SoC Image/Picture
\subsection{Piccolo SoC}
Bluespec Verilog offers superior scheduling capabilities compared to ChiselHDL,
and standard Verilog. The creation of hardware that performs more complex tasks
is therefore made significantly easier. As such, we decide to use the Piccolo
processor from the Bluespec Inc. family within our simulation and testbench.

We choose the Piccolo processor over the Flute processor since we have no need
for virtual memory, therefore an MMU is superfluous. While the Toooba processor
would offer superior performance, due to its inherent complexity it would
consume far too much silicon space for the purpose of synthesis. We seek to
minimise our silicon area to open up the opportunity for the later task of
performing synthesis on an FPGA. This will lead to a lower LUT count, and give
us greater flexibility when designing and synthesising our own accelerator as we
have greater FPGA resources to make use of.

% TODO: Disadvantages of Piccolo(?)
% AXI4 bus configurability
% CPU 64/32-bit
% ISA extension customisability
% IMAFC --- ALSO D, M ... etc
% Will not boot a Linux Kernel, but FreeRTOS. Simple bare metal.
% Ignore context switching

\section{Risc-V Firmware}
\subsection{Risc-V Assembly \& Firmware Structure}
The firmware which runs on an embedded Risc-V core is significantly different to
software that runs on top of an operating system. In order to compile C and
assembly language into machine code, we make use of the GNU compiler collection
(GCC).

The naming convention for compilers is \texttt{arch-vendor-os-libc-gcc}. The
standard, desktop version of gcc \texttt{x86\_64-pc-linux-gnu-gcc} is not
capable of producing Risc-V machine code. Moreover, it is targetted at the linux
operating system, and uses the linux GNU LibC and supports linux system calls.
In order to compile our Risc-V assembly and C into Risc-V machine code, we must
use a cross-compiler. We build a compiler on our host x86\_64 system to run on
an x86\_64 machine, that is capable of producing a Risc-V ELF binary.

The \texttt{riscv64-unknown-elf-gcc} compiler is designed to produce a
bare-metal Risc-V binary. Instead of using GNU LibC, the C library used is
Newlib. Newlib is C library that is designed for embedded systems. With Newlib,
we now have access to the standard C functions for IO \texttt{printf}, and math
functions, such as \texttt{pow}, and \texttt{sqrt}. The relevant machine code
for these functions are statically linked, and the code within our final binary
ELF is entirely self-contained. This can be seen in the section layout of our
produced ELF file.

Due to the 32-bit configuration of the Piccolo processor that we are using, we
must restrict GCC to using 32-bit instructions, and 

Relevant sections of our final ELF file: We have a $64$~KB stack, and statically
linked functions from newlib.  
\tiny
\begin{verbatim}
Section .stack         : addr         800000f8 to addr         80010100; size 0x   10008 (= 65544) bytes
Section .text.pow       : addr         80013444 to addr         8001352e; size 0x      ea (= 234) bytes
Section .text.__ieee754_pow: addr         8001352e to addr         80013fee; size 0x     ac0 (= 2752) bytes
Section .text.__ieee754_sqrt: addr         80013fee to addr         800141b4; size 0x     1c6 (= 454) bytes
Section .text.memcpy    : addr         8001707c to addr         80017164; size 0x      e8 (= 232) bytes
\end{verbatim}
\normalsize

\subsubsection{UART/Serial Output}
% TODO: UART cite
The most important firmware debugging feature to get working is the ability to
print to console. The Piccolo techbench SoC kindly contains a NS16550 UART
peripheral that is connected to the AXI4 bus interconnect as a slave device.  

This UART peripheral is memory mapped, and we write to its internal registers
from within our firmware to obtain a console output. This is vital for debugging
purposes, as it allows us to extract and verify the computed results of our
accelerator.

Crucially, the UART peripheral provided by Bluespec Inc. is only suitable for
simulation purposes. Data written into its transmit register are printed
directly to console. 

% TODO: Cite Linker File guide
\subsubsection{Linker File}
Since we are compiling our C into a bare-metal ELF, we must specify the layout
of the machine code within the binary using a linker file.
It is vital to consider the distinction between load memory address (LMA) and
virtual memory address (VMA). The LMA is the location of our section in our
binary, while the VMA is the location
referenced within our machine code. In the situation where we perform synthesis,
we may only initialise our ROM. When our machine code and static variables are
compiled into a binary, our initialised static variables \texttt{(.data)} must
be copied over from our ROM into RAM. The LMA of our \texttt{.data} section must
be within our ROM, and the VMA must be within our RAM. Furthermore, we must also
zero our uninitialised static variables \texttt{(.bss)}, with the VMA in our
RAM. We must also specify the start VMA address of our stack.

The main sections of the linker file are as follows: The \texttt{.vector}
section contains the instructions that are run after the processor is booted.
The \texttt{.stack} section is an empty region at the beginning of memory
where the \texttt{\_stack\_start} symbol is defined. The \texttt{.data} section
contains initialised data, that is typically accessed through global pointer
(\texttt{gp}) relative addressing. The \texttt{.bss} section contains
uninitialised data. Lastly, the \texttt{.text} section contains our machine
code.

We intentionally place the stack below our \texttt{.text} code section for security
purposes. The stack grows downwards, and as a result it cannot write into our
code section in the situation that a stack overflow occurs.

\scriptsize
\begin{verbatim}
OUTPUT_FORMAT("elf32-littleriscv", "elf32-littleriscv", "elf32-littleriscv")
OUTPUT_ARCH(riscv)
ENTRY(crtStart)
MEMORY {
  ROM      (rx):  ORIGIN = 0x80000000, LENGTH = 0x10000  /* 64KB */
  RAM      (rwx): ORIGIN = 0x80800000, LENGTH = 0x10000 /* 64KB */
}

_stack_size = 0x4000; /* 64KB stack = 0x10000, 1MB stack = 0x100000 */
SECTIONS {
  .vector ORIGIN(ROM) : {
    *crt.o(.start_jump);
  } > ROM AT > ROM

  .text : {
    *(.text);
    *(.rodata .rodata.*) /* Constants */
    *(.srodata .srodata.*) /* read-only initialised data */
    *(.text.*); /* seperate sections for static linked NewLib functions */
    *(.eh_frame)
    _rom_data_start = .;
  } > ROM /* LMA = VMA @ ROM */
  
  .data : { /* initialised static variables */
    . = ALIGN(8) ;
    _ram_data_start = .;
    *(.rdata) /* global static data */
    *(.data .data.*) /* initialised data */
    PROVIDE( __global_pointer$ = . + 0x800 );
    *(.sdata .sdata.*) /* small initialised data */
    . = ALIGN(8);
    _ram_data_end = .;
  } > RAM AT > ROM /* VMA @ RAM, LMA @ ROM */
  _ram_data_size = _ram_data_end - _ram_data_start;

  .bss (NOLOAD) : { /* uninitialised static variables */
		. = ALIGN(4);
		_bss_start = .;
    *(.sbss*) /* small uninitialised data */
    *(.bss .bss.*) /* uninitialised data */
    *(COMMON) /* common symbols */
		. = ALIGN(4);
		_bss_end = .;
  } > RAM /* LMA = VMA @ RAM */

  .stack (NOLOAD) : { /* stack location */
    . = ALIGN(16);
    _stack_end = .;
    . = . + _stack_size;
    . = ALIGN(16);
    _stack_start = .;
  } > RAM /* LMA = VMA @ RAM */
}
\end{verbatim}
\normalsize

We make use of the \texttt{(AT)} keyword, to specify an alternative LMA which
differs from our VMA. We must write firmware to copy over our static variables
from ROM into RAM, and zero our uninitialised variables in RAM.

Other sections of code are not included. The heap is unspecified, as it is
unnecessary for the purposes of lightweight firmware. Since we are compiling C,
we omit the C++ sections for storing static constructors \texttt{(.ctors)} and
destructors \texttt{(.dtors)}.

One of the first problems encountered was a CPU trap due to a stack overflow.
The size of the stack may be modified in the linker file. The \texttt{embeddedartistry}
printf implementation that avoids using heap memory stores a considerable amount
of data on the stack. As such, we set the stack to $16$~KB, rather than the
original $4$~KB. A larger stack reduces the likelihood of a stack overflow,
particularly if our libraries used are dependent on a lot of stack allocations.

\subsubsection{LibC \& printf}
The standard printf function defined within the \texttt{stdio.h} header will
include the declaration of printf compiled within the NewLib library. It will
then be statically linked to our final binary, and have the code section
corresponding to the print function included within your final ELF file.

The standard printf function makes use of heap memory. Due to memory
constraints, the location of the heap is not specified within the linker file.
We therefore seek an implementation of the printf function that relies only on
the stack. 
% TODO: Cite https://github.com/embeddedartistry/printf

\subsubsection{crt.s --- RiscV Assembly}
The initial location of our program counter after it is moved from the boot rom
is \texttt{0x80000000}. This is the start address of our RAM. For
instruction-level control over CPU behaviour on boot, we place our own section
of assembly code at the beginning of RAM.

Of notable importance is setting the stack pointer \texttt{sp}, to ensure that
stack memory is allocated in the correct region of memory. We also set the
global pointer to allow global pointer relative addressing of variables in our
data section.


\lstset{language=[RISC-V]Assembler, style=customrv}
\begin{lstlisting}
.global crtStart
.global main

.section	.start_jump,"ax",@progbits
crtStart:
  lui  x2,      %hi(crtInit)
  addi x2, x2,  %lo(crtInit)
  jalr x1, x2   // jump to label crtInit
  nop

.section .text
crtInit:
  .option push
  .option norelax
  la gp, __global_pointer$ // Set global pointer for GP-relative addressing
  .option pop
  la sp, _stack_start // Set stack pointer to beginning of stack
  call main
infinitLoop:
  j infinitLoop

\end{lstlisting}
\lstset{}

\section{Testbench Setup}
\subsection{Debugging Piccolo: FPU}
\subsubsection{Fixing Piccolo FPU typeerrors}
% TODO: hyperlink: https://github.com/bluespec/Piccolo/tree/a4e34ef2f2ba4e82a95faa2bbe3dd832ca3c51a0 
The latest commit of the Piccolo processor, at the time of development, (a4e34ef), had
a type error in the floating point unit (FPU). 

The compiler experiences ambiguity between the RoundMode enumerations declared
within the BSV FloatingPoint library (\texttt{FloatingPoint.bsv}), and the
enumeration within \texttt{ISA\_Decls}. We hypothesise that a previous version
of the Bluespec Verilog compiler was capable of either resolving this ambiguity
by falling back to the library definition of the datatype, or prioritised the
library one to allow the processor to compile. Due to a lack of continuous
integration, or testing, this issue has not yet been fixed.
% TODO: Submit a pull request

% NOTE: Compile log
\begin{verbatim}
compiling Piccolo/src_Core/CPU/FPU.bsv
Warning: Unknown position: (S0080)
   1 warnings were suppressed.
Error: "Piccolo/src_Core/CPU/FPU.bsv", line 52, column 30: (T0080)
Type error at the use of the following function:
   mkFloatingPointDivider

The expected return type of the function:
   g__#(ClientServer::Server#(Tuple3#(FPU::FSingle, FPU::FSingle, ISA_Decls::RoundMode), FPU::FpuR))

The return type according to the use:
   c__#(ClientServer::Server#(Tuple3#(FloatingPoint::FloatingPoint#(d__, e__),
      FloatingPoint::FloatingPoint#(d__, e__),
      FloatingPoint::RoundMode),
      Tuple2#(FloatingPoint::FloatingPoint#(d__, e__), FloatingPoint::Exception)))
\end{verbatim}

In order to successfully compile the Piccolo processor with floating point
support, we must eliminate the ambiguity manually by specifying which package,
or library the datatype is declared within. We enter, \texttt{FPU.bsv} and prefix the
RoundMode type with \texttt{FloatingPoint::RoundMode} when it is used within the
arguments of a floating point library module. Additionally, we must enter
\texttt{FPU\_Core.bsv} and modify the function that converts between both
enumerations. The \texttt{fv\_getRoundMode} function must be modified to return
\texttt{FloatingPoint::RoundMode}.

After these changes, we observe that the Piccolo processor with the
floating point Risc-V ISA extension successfully compiles into Verilog RTL from
Bluespec Verilog. We may then simulate the processor performing floating point
precision mathematics using our verilator test bench.

\subsubsection{mstatus CSR}
Once the FPU is successfully compiled and the processor is compiled with
floating point support, the processor will trap and throw an exception when any
floating point instruction is executed. When a processor exception takes place,
the processor moves the program counter back to the start of the boot ROM, this
is observed in the simulation output as an infinite loop, where all instructions
after the first floating point instruction will never execute, as the processor
will constantly trap and reset itself, perpetually restarting the program.

With an open source processor, we have the ability to inspect and modify the
source code, and recompile the processor. We can instruct the CPU to print its
internal state when a trap occurs, notably: information about the trap, and the
type of exception that has been thrown. With this information, we can look
through the codebase of the Piccolo processor, and identify what is causing the
processor to fail to execute floating point instructions, despite the fact that
the FPU module is included within the processor.

We enter \texttt{CPU.bsv}, and modify the rule responsible for modifying the
behaviour of the CPU when the trap condition is met. We display additional
information about the trap, and instruct the simulation to end when a trap
occurs. This is where BluespecVerilog is incredibly useful. The
\texttt{Trap\_Info} structure within BSV contains the \texttt{FShow} typeclass.
This contains information for printing the structure in a clean and easy-to-read
way, specifically for debugging purposes. Additionally, the
\texttt{fshow\_trap\_Exc\_Code} function will print the human-readable name of
the exception code enumeration. As such, we do not need to constantly switch
between looking at our program output, and figuring out what the output
represents in our source code files.

\begin{verbatim}
// CPU.bsv
   rule rl_trap ((rg_state == CPU_TRAP)
     && (stage1.out.ostatus != OSTATUS_BUSY));

      [...]

      $display("DEBUG: CPU TRAP: ", fshow(rg_trap_info));
      $display("Exc_Code: ", fshow_trap_Exc_Code(exc_code));
      $finish(0);
   endrule: rl_trap
\end{verbatim}

The information we obtain about our processor trap can be matched to the
specific address in assembly code that causes the error. In this particular
case, the \texttt{flw} instruction is responsible for triggering an
\texttt{ILLEGAL\_INSTRUCTION} exception.
\begin{lstlisting}[language=C,style=customc],
  float a = 1.0f;
  // Assembly:
  // 80002276: lui a5, 0x80001
  // 8000227a: flw fa5,424(a5)
  // 8000227e: fsw fa5,-28(s0)
  // Prints:
  // Trap_Info { epc: 'h8000227a, exc_code: 'h2, tval: 'h1a87a787 }
  // Exc_Code: ILLEGAL_INSTRUCTION
\end{lstlisting}

We look through our processor code, and identify the locations where this
particular exception code is set. We observe that it is set in the
\texttt{alu\_outputs\_base} struct within \texttt{EX\_ALU\_functions.bsv}. This
struct is copied within the ALU function that corresponds to loading data into
registers, \texttt{fv\_LD}. If the \texttt{flw} instruction is called, a
particular bit on the \texttt{mstatus} control-status register (CSR) is read. In
our case, the fs bit is found to be disabled.
\begin{verbatim}
// EX_ALU_functions.bsv
function ALU_Outputs fv_LD (ALU_Inputs inputs);
   [...]
   // FP loads are not legal unless the MSTATUS.FS bit is set
   Bool legal_FP_LD = True;
`ifdef ISA_F
   if (opcode == op_LOAD_FP)
      legal_FP_LD = (fv_mstatus_fs (inputs.mstatus) != fs_xs_off);
`endif
   let alu_outputs = alu_outputs_base;

   alu_outputs.control   = ((legal_LD && legal_FP_LD) ? CONTROL_STRAIGHT
                                                      : CONTROL_TRAP);
   [...]
   return alu_outputs;
endfunction
\end{verbatim}

Compared to debugging standard Verilog, the structure of Bluespec Verilog
provides us with a greater ability to debug, and identify relevant codepaths.

We further observe that all FP ALU functions check the fs bit of the CSR register.
This can be seen from the ALU fv\_LD function that runs when the flw instruction
is executed. We later learn, that this is an intentional design descision as per
the Risc-V specification, Chapter 3, Section 1.11. The fs bit on the mstatus CSR must be
set in order for floating-point instructions to be enabled. We can see how this
is done in bare-metal C, by looking at the source code for the Spike Risc-V
proxy kernel, we implement the relevant parts of this code within our firmware
initialisation sequence. Once implemented, we observe that floating point
instructions operate correctly.
% https://github.com/riscv/riscv-pk/blob/master/machine/minit.c
% https://github.com/riscv/riscv-pk/blob/master/machine/encoding.h 
\begin{lstlisting}[language=C,style=customc]
// encoding.h
#define MSTATUS_FS          0x00006000
#define write_csr(reg, val) ({ \
  asm volatile ("csrw " #reg ", %0" :: "rK"(val)); })

// minit.c
static void mstatus_init()
{
  uintptr_t mstatus = 0;
  // Enable FPU
  if (supports_extension('F'))
    mstatus |= MSTATUS_FS;
  [...]
  write_csr(mstatus, mstatus);
  [...]
}
\end{lstlisting}

% TODO: Citation
% “The RISC-V Instruction Set Manual, Volume II: Privileged Architecture,
% Document Version 20190608-Priv-MSU-Ratified”, Editors Andrew Waterman and
% Krste Asanovi´c, RISC-V Foundation, June 2019
% https://riscv.org/wp-content/uploads/2017/05/riscv-privileged-v1.10.pdf
% TODO: Citation (Spike) + RiscV PK

\subsection{AXI4 Test Peripheral}
When building a peripheral, it is important to first work constructively from a
minimum functioning unit. In order for our embedded processor to communicate
with with our peripheral, we make use of memory-mapped I/O. We map a small
unoccupied memory address range to our peripheral, and give it a small internal
region of memory where 4-byte integers can be read/written from.

\subsubsection{What is AXI4?}
A microprocessor communicates with 

% TODO: Explain AXI4
The AXI4 protocol is designed by ARM, and is royalty-free, where a licence is
not required for academic or commercial use.

% https://forums.xilinx.com/t5/Design-and-Debug-Techniques-Blog/AXI-Basics-1-Introduction-to-AXI/ba-p/1053914 

% AXI4 RAM: https://github.com/alexforencich/verilog-axi/blob/master/rtl/axi_ram.v 

% 10 wire write address channel
% 5 wire write data channel
% 4 wire write response channel
% 10 wire read address channel
% 6 wire read data channel
% 35 wires in total, excluding clock & reset

Due to the inherent complexity of the AXI4 protocol, large combinatorial finite
state machines are required to ensure the read and write requests of the AXI4
bus are processed correctly.
The state of over a dozen wires needs to be considered by the programmer on
every clock cycle. Developing a peripheral in this manner in standard Verilog
RTL is a challenging feat.

Moreover, when a peripheral for the module is initialised, all 35 wires
corresponding to signals in the AXI4 protocol need to be connected to the AXI4
interconnect. All of these connections need to be manually specified. This can
be observed in Verilog RTL as follows:
\tiny
\begin{verbatim}
  mkUART uart0(.CLK(CLK),
         .RST_N(RST_N),
         .put_from_console_put(uart0$put_from_console_put),
         .set_addr_map_addr_base(uart0$set_addr_map_addr_base),
         .set_addr_map_addr_lim(uart0$set_addr_map_addr_lim),
         .slave_araddr(uart0$slave_araddr),
         .slave_arburst(uart0$slave_arburst),
         .slave_arcache(uart0$slave_arcache),
         .slave_arid(uart0$slave_arid),
         .slave_arlen(uart0$slave_arlen),
         .slave_arlock(uart0$slave_arlock),
         .slave_arprot(uart0$slave_arprot),
         .slave_arqos(uart0$slave_arqos),
         .slave_arregion(uart0$slave_arregion),
         .slave_arsize(uart0$slave_arsize),
         .slave_arvalid(uart0$slave_arvalid),
         .slave_awaddr(uart0$slave_awaddr),
         .slave_awburst(uart0$slave_awburst),
         .slave_awcache(uart0$slave_awcache),
         .slave_awid(uart0$slave_awid),
         .slave_awlen(uart0$slave_awlen),
         .slave_awlock(uart0$slave_awlock),
         .slave_awprot(uart0$slave_awprot),
         .slave_awqos(uart0$slave_awqos),
         .slave_awregion(uart0$slave_awregion),
         .slave_awsize(uart0$slave_awsize),
         .slave_awvalid(uart0$slave_awvalid),
         .slave_bready(uart0$slave_bready),
         .slave_rready(uart0$slave_rready),
         .slave_wdata(uart0$slave_wdata),
         .slave_wlast(uart0$slave_wlast),
         .slave_wstrb(uart0$slave_wstrb),
         .slave_wvalid(uart0$slave_wvalid),
         .EN_server_reset_request_put(uart0$EN_server_reset_request_put),
         .EN_server_reset_response_get(uart0$EN_server_reset_response_get),
         .EN_set_addr_map(uart0$EN_set_addr_map),
         .EN_get_to_console_get(uart0$EN_get_to_console_get),
         .EN_put_from_console_put(uart0$EN_put_from_console_put),
         .RDY_server_reset_request_put(uart0$RDY_server_reset_request_put),
         .RDY_server_reset_response_get(uart0$RDY_server_reset_response_get),
         .RDY_set_addr_map(),
         .slave_awready(uart0$slave_awready),
         .slave_wready(uart0$slave_wready),
         .slave_bvalid(uart0$slave_bvalid),
         .slave_bid(uart0$slave_bid),
         .slave_bresp(uart0$slave_bresp),
         .slave_arready(uart0$slave_arready),
         .slave_rvalid(uart0$slave_rvalid),
         .slave_rid(uart0$slave_rid),
         .slave_rdata(uart0$slave_rdata),
         .slave_rresp(uart0$slave_rresp),
         .slave_rlast(uart0$slave_rlast),
         .get_to_console_get(uart0$get_to_console_get),
         .RDY_get_to_console_get(uart0$RDY_get_to_console_get),
         .RDY_put_from_console_put(uart0$RDY_put_from_console_put),
         .intr(uart0$intr));
\end{verbatim}
\normalsize

Bluespec Verilog offers a significant degree of abstraction when processing AXI4
read and write requests, and also when attaching the peripheral to the AXI4 bus interconnect.
% TODO: Include code!

% TODO: Include picture of Vivado SoC
Proprietary CAD tools, such as Xilinx Vivado, typically have a built-in block
design tool. The built in design tool allows the developer to see the SoC as a
whole, and visually connect components together. These design tools generate the
necessary Verilog HDL representing the SoC and the necessary connections between
the module components.

Our SoC can be created by creating the necessary IP blocks for our accelerator,
and processor, and making use of the proprietary AXI interconnect, UART module
and block memory controller. The block design tools created by Xilinx and Altera
are a solution to the mess of specifying wires and connections when writing
standard Verilog HDL in a modular fashion. The design may be then simulated
directly using the xsim command within Xilinx Vivado.
% TODO: talk about why we avoid this (proprietary IP bugs)

In contrast, our testbench SoC written in Bluespec Verilog has no graphical
configuration. The entire network consisting of the processor core, peripherals
and AXI4 interconnect fabric are all written in BSV, and the modules are
connected together manually. This process is significantly simpler than with
Verilog RTL, as the overloaded module/typeclass \texttt{mkConnection} can be used to
forward arbitrary structures of data between modules of particular interfaces.
An instance of \texttt{mkConnection} exists for the AXI4 master and slave
interfaces that are also implemented in BSV themselves. The AXI4 fabric module
is the interconnect, and is parametised by the number of master and slave
interfaces it should have. Each instance of \texttt{mkConnection} is a module
that forwards data between all 35 AXI4 wires. As shown below:
% TODO: Add SoCMap/SoCTop
\scriptsize
\begin{verbatim}
   Fabric_AXI4_IFC  fabric <- mkFabric_AXI4;
   Core_IFC #(N_External_Interrupt_Sources)  core <- mkCore;
   UART_IFC   uart0  <- mkUART;
   Test_IFC   test <- mkTest;
   mkConnection (core.cpu_imem_master,  fabric.v_from_masters [imem_master_num]); // instruction memory
   mkConnection (core.cpu_dmem_master,  fabric.v_from_masters [dmem_master_num]); // data memory
   mkConnection (fabric.v_to_slaves [uart0_slave_num],  uart0.slave); // UART0 serial peripheral
   mkConnection (fabric.v_to_slaves [test_slave_num],  test.slave); // Test peripheral
   // Connect to BlockRAM
\end{verbatim}
\normalsize

% TODO: Perhaps include example AXI4 r/w

\subsubsection{AXI4 Alignment/Padding}
Due to the structure of the AXI4 bus and the way a processor writes to memory,
data stored in memory must be aligned depending on its size.
A 32-bit AXI4 bus may only access at most a single aligned
4-byte block per read/write request. As a consequence, it is possible to fail
writing/reading from memory when crossing the block boundary.

% TODO: Include image
For example, writing to an unaligned 4-byte region at the address $0x2-0x6$ crosses
the $0x4$ boundary:
\begin{verbatim}
*((uint32_t*)0xC0001002) = 0x1337;
Trap_Info { epc: 'h800022d6, exc_code: 'h6, tval: 'hc0001002 }
TRAP EXC: STORE_AMO_ADDR_MISALIGNED
*((uint32_t*)0xC0001000) = 0x1337;
\end{verbatim}

This is vital to keep in mind throughout writing to peripheral memory. An
integer must be aligned to 4 bytes, a short to 2 bytes, and a char to the
nearest byte. 

% \subsubsection{Struct Padding}
% % TODO: Introduce the need for struct padding/packing
% This is of particular importance to consider later when we issue commands to the
% accelerator. By default, a C struct is padded and aligned to conform to the
% alignment of the way memory is accessed. When a struct is written to memory, it
% always begins at the nearest 4 byte boundary. All datatypes within a struct ar
% aligned to the nearest 4, For instance:

% \begin{verbatim}
% struct paddedstuff_t {
%     uint16_t A; // short (2B) --> (4B) PADDED to ALIGN to nearest 4 bytes
%     uint32_t B; // int (4B)
% }; // 8 BYTES
% struct packedstuff_t {
%     uint16_t A; // short (2B)
%     uint32_t B; // int (4B) ---> Will write at base+0x2
% } __attribute((packed)); // 6 BYTES
% \end{verbatim}


\chapter{SoC \& Testbench Setup: Evaluation}
\section{Clock rate within simulation}
\subsection{Methodology}
The Piccolo processor was chosen due to its relative simplicity. With a simple
3-stage in-order pipeline, the generated C++ should be relatively simple
compared to the 5-stage pipeline in the Flute processor and the out-of-order
superscalar architecture of the TOOOBA processor. As a consequence, we would
therefore increase the likelihood that we obtain a reasonable clock rate in
simulations, and have a faster testbench compile-time.

We modify the existing Piccolo testbench program to output the clock rate when
the testbench is receives an interrupt signal, or when the simulation has come
to an end. This is done as follows, we keep track of the time throughout our
simulation and divide the number of simulated cycles by the time elapsed.
\begin{lstlisting}[language=C,style=customc]
using namespace std;
using namespace std::chrono;

steady_clock::time_point tp_begin;

void print_clock_rate() {
  uint64_t cycle_count = main_time/10;
  steady_clock::time_point tp_end = steady_clock::now();
  uint64_t time_elapsed = duration_cast<seconds>(tp_end-tp_begin).count();
  cout << "Clock Rate: " << cycle_count/time_elapsed << " Hz" << endl;
}

void sigint_callback_handler(int signum) {
  cout << "Interrupted" << endl;
  print_clock_rate();
  SIG_DFL(signum);
  //exit(signum);
}

int main (int argc, char **argv, char **env) {
    Verilated::commandArgs (argc, argv);    // remember args
    signal(SIGINT, sigint_callback_handler);
    // TESTBENCH INIT...
    tp_begin = steady_clock::now();
    // TESTBENCH START... (LOOP)
    // TESTBENCH FINISH
    print_clock_rate();
    return 0;
}
\end{lstlisting}


\subsection{Results}
On interrupting our program we measure a clock rate of $22406$~Hz. This is
sufficient for running a simulation of interactions with a peripheral. Moreover,
we are confined to using bare-metal firmware, as our clock rate is too low to
boot an operating system.

\section{Simulating an AXI4 Peripheral}
\subsection{Methodology}
We make use of a verilator testbench to simulate our SoC. We write some simple
test firmware to write into the peripheral, and trigger its execution.

When a special execution bit is set, we set a special accelerator busy bit and
unset the execution bit. In a single cycle, within the accelerator, if the
busy bit is set, we double the array and unset the busy bit. This allows the
firmware to stall until the busy bit is unset, and the computation has been
completed.

% NOTE: Firmware C code for accelerator
\begin{lstlisting}[language=C,style=customc]
#define TEST_CONTROL_ADDR 0xC0001000UL
#define TEST_DATA_ADDR 0xC0001008UL
#define TEST_RANGE 4
void main() {
  for(int i=0; i<TEST_RANGE; i++) {
    *((uint32_t volatile*)TEST_DATA_ADDR+i) = i;
    uint32_t data = *((uint32_t*)TEST_DATA_ADDR+i);
    printf("%d: %d\n", i, data);
  }

  println("Set Control Bit..");
  *((uint8_t volatile*)TEST_CONTROL_ADDR) |= 1; // Set execute!

  while(*((uint8_t volatile*)TEST_CONTROL_ADDR) & 2 == 0) {}
  println("Stalling.."); // Printing Stalling... stalls anyway...

  println("Stall complete!");
  for(int i=0; i<TEST_RANGE; i++) {
    uint32_t data = *((uint32_t volatile*)TEST_DATA_ADDR+i);
    printf("%d: %d\n", i, data);
  }
}
\end{lstlisting}


\subsection{Results}
% NOTE: Console log
\small
\begin{verbatim}
0: 0
1: 1
2: 2
3: 3
Set Control Bit..
Stalling..
Stall complete!
0: 0
1: 2
2: 4
3: 6
\end{verbatim}
\normalsize

We successfully write numbers to our accelerator memory and double them all in a
single clock cycle by issuing a command to our accelerator. Now that we have
demonstrated a functioning testbench, we proceed to the synthesis stage. 

\section{Synthesis}
\begin{figure}[b]
  \centering
  \fbox{\includegraphics[width=\textwidth]{./img/soc_dg.png}}
  \caption{Block diagram of SSITH\_P1 based SoC created for the purpose of
    synthesis, containing GPIO, UART, RAM and ROM memory-mapped peripherals.}
\end{figure}

While it is technically possible to synthesise a design using the Symbiflow
toolchain on an Arty A7-100T, this particular model has a very significant
number of LUTs and the open source place and route algorithm is slower and less
optimised than the proprietary one in Xilinx Vivado. Additionally, Vivado
provides us with more detailed information about timing violations and a place
and route diagram revealing the structure of the hardware to us in a visual
manner. This is invaluable, and is on balance why we decide to avoid using the open
source toolchain for the purpose of synthesis.  

We reconstruct our SoC using the block design tool within Vivado Studio. We use
the \texttt{SSITH\_P1} core provided by the repository. While it does not have
an FPU, it may be easily rebuilt to include one along with the Risc-V based
\textbf{F} ISA extension.

We perform a place and route (P\&R) procedure, to reveal the LUT occupancy and verify
that the design passes timing checks at a 10 MHz clock rate. We generate an \texttt{.mml}
file containing the information of our ROM cells from the P\&R process and
proceed to generate a bitstream where we have initialised the RAM appropriately
in order to flash our FPGA.

From our diagram/table of FPGA occupancy (Figure \ref{fig:soc_pr}/Table
\ref{table:soc_pr}), we see that the majority of space on our FPGA is
free. This is ample room for synthesis of an external accelerator.
\begin{table}[b]
  \centering
  \begin{tabular}{l|r|r|r}
    \toprule
    Type  & Used & Available & \% \\
    \midrule
    LUT   &	20817&	63400	&32.83 \\
    LUTRAM&	759	 &19000	  &3.99 \\
    FF	  &16704 &126800	&13.17 \\
    BRAM	&37.5	 &135	    &27.78 \\
    DSP	  &15	   &240	    &6.25 \\ 
    IO	  &16	   &210	    &7.62\\ 
    BUFG	&2	   &32	    &6.25\\
    \bottomrule
  \end{tabular}
  \caption{Xilinx Arty A7-100T FPGA occupancy statistics}
  \label{table:soc_pr}
\end{table}
% TODO: Appendix
% TODO: MML file --- flash BlockRAM
% TODO: P&R Diagram - highlighted!

With a successful P\&R at 10MHz, with no timing violations, we have a clock rate
increase of 400-fold compared to our simulations at 24kHz.

The most pronounced timing violation constraining our clock rate to 10MHz as
opposed to 100MHz is due to the sparsity of our BlockRAM. We can see that our
orange ROM modules (orange highlights on red rectangles), are distributed quite
sparsely. Our system clock \texttt{(sys\_clk)} originates at the very centre of
our chip, and the signal needs to propagate to greater distance to clock the
sparse ROM. This becomes an issue at clock frequencies that exceed 10MHz, or if
our firmware grows in size substantially, such that we need more ROM modules
that may be distributed even more sparsely and further away from the system clock.

Now that we have verified our SoC design through simulations, synthesis, place
\& route and bitstream generation targetting a Xilinx Arty A7-100T, we may build
upon our minimal functioning self-contained SoC to construct our own linear
algebra peripheral.

\begin{figure}[h]
  \centering
  \fbox{\includegraphics[width=.65\textwidth]{./img/soc_imp.png}}
  \caption{Xilinx Arty A7-100T Place and Route diagram. Debug hub (Red), Processor (Grey), AXI Interconnect (Pink), AXI GPIO (Green), AXI Serial
    (Brown), AXI RAM (Yellow), AXI ROM (Orange)}
  \label{fig:soc_pr}
\end{figure}

\chapter{Linear Algebra Peripheral: Design \& Implementation}
\section{Overview}
\subsection{Background}
Mathematically, the result of a matrix multiplication may be expressed
accordingly:
\begin{align*}
  C &= AB \\
  C_{ij} &= \sum_k A_{ik}B_{kj}
\end{align*}

Matrix multiplication may be expressed in a programmatic form as such:
\begin{center}
\texttt{C[i][j] = sum zipWith (op*) (row A i) (col B j)}
\end{center}
When matrices are stored in memory, they can only be indexed with a single
dimension. A multi-dimensional matrix is represented as a multi-dimensional
array, this array is represented in memory in as a flattened contiguous
one-dimensional array, with the rows of the matrix next to each other laid of
sequentially in memory.

Consequently we access the element on the xth row, and yth column of a matrix as
follows:
$$ \text{IndexOf}\,\, A[x][y] = x + y * \text{width}(A) $$

This methodology of accessing the x, and y elements of a stored matrix is used
throughout the accelerator.

Block RAM (BRAM) is a contiguous region of memory that exists within the FPGA. In
Bluespec Verilog, block RAM is created through the use of something known as a
Register File (\texttt{RegFile}). Crucially, this is distinct from the Bluespec
array-access methodology of \texttt{Vector\#(N, Reg\#(type))}. A vector of
registers are seperate registers, which need not necessary be contiguous in
hardware, that can be array-accessed in a contiguous fashion within Bluespec.

Our peripheral contains a RegFile of internal BRAM which is memory mapped to
the CPU via an AXI4 bus slave interface. 
% RegFile vs Vector#(Reg#()) vs BRAM

\subsection{Design}
\subsubsection{Hybrid/Monolithic}
% TODO: Citations - argue around, base ideas off.
We propose two potential ways of implementing matrix multiplication. We could
pursue a monolithic approach and implement the entire matrix multiplication
operation purely in hardware. Alternatively, we could perform a hybrid approach,
where each element of the matrix \texttt{C[i][j]} may be computed by issuing a
separate command to the accelerator. This is performed under the assumption that the
creation and scheduling of instructions for each element in the final matrix is
more easily performed within the firmware of the embedded processor without a
significant perfomance penalty.

\subsubsection{Memory Regions}
We split our accelerator memory into seperate memory-mapped regions. We
interface with our accelerator through the AXI4 bus by writing and reading from
memory appropriately.  
\begin{table}[h]
  \centering
  \begin{tabular}{lll}
    \toprule
    Offset Address Range & Size & Purpose \\
    \midrule
    \texttt{0x0000-0x0020} & 32 Bytes & Status \\
    \texttt{0x0020-0x0040} & 32 Bytes & Control \\
    \texttt{0x0040-0x1000} & $\approx$ 16 KB & Memory \\
    \bottomrule
  \end{tabular}
  \caption{Memory regions within our linear algebra accelerator.}
\end{table}

Like the test peripheral, our data memory consists of a memory mapped
\texttt{RegFile} to make use of FPGA BRAM cells. In the later section, we modify
the structure of the memory to support multiple functional units, where the same
address will map to multiple RegFiles.

\subsubsection{Status}
The first bit of our status register in memory exists to monitor the execution
of the accelerator. In the situation that all functional units are occupied, a
command that is issued to the accelerator will be ignored. It is therefore
important to keep track of the internals of the acclerator and make sure that it
is not at maximum utilisation when commands are issued. Within our firmware, we
must check that there is at least one free functional unit before issuing a
command, or else our commands will drop and the computation of our final matrix
will not be correctly performed.

\subsubsection{Control}
We write our control command/instruction to the control register. This is
discussed in greater detail in the following section. Once our command is
written, it is executed by setting the execute status bit within the control
register. The accelerator should then dispatch the appropriate command to one of
its functional units if it is free.

\section{Command Issuing \& Decoding Protocol}
\subsection{Format}
In the hybrid model of our accelerator, an individual command must be able to
compute an individual element of our final matrix, C. As discussed earlier,
matrices are stored linearly in memory. We provide initial addresses, offsets,
and strides for vector access of the respective row in matrix A, and column in
matrix B, and the relevant output address and offset for our element of matrix
C. From this information, our accelerator can instruct a functional unit to
perform the appropriate dot product.

\subsection{Decoding}
One of the principal problems encountered between issuing a command containing
a struct of multi-byte data to the accelerator from the embedded processor. By
default, the unsigned integer type in BSV is in big endian, while the
embedded core stores data in little endian. The motivation for embedded
processors to use little endian byte ordering is to ensure that the address of a
variable is unchanged when a typecast is performed. This saves processor cycles,
and results in improved performance. Little endian variables are automatically
truncated/extended when an appropriate typecast is made. \\


% TODO: Link to:
% https://github.com/jeffreycassidy/BlueLink/blob/master/Core/Endianness.bsv 
Within BSV, we make use of the \texttt{LittleEndian.bsv} library provided by the
open source BlueLink project in order to interpret the processor-written struct
fields with a little endian encoding.

\begin{figure}[h]
\centering 
\scriptsize
\begin{BVerbatim}
typedef struct {
   LittleEndian#(Bit#(32)) addr;
   LittleEndian#(Bit#(8)) offset;
   LittleEndian#(Bit#(8)) stride;
   } MatUnitPtr deriving (Eq, Bits, FShow);

typedef struct {
   LittleEndian#(Bit#(8)) unit;
   LittleEndian#(Bit#(8)) count;
   MatUnitPtr ptr_a;
   MatUnitPtr ptr_b;
   MatUnitPtr ptr_c;
} MatUnitArgs deriving (Eq, Bits, FShow);

\end{BVerbatim}
\normalsize
\caption{Bluespec Verilog structures for our co-processor communication
  protocol. }
\end{figure}

We write a C struct into our control region of memory. In order for the data
within the instructions to be interpreted correctly in hardware, the
byte-ordering needs to be reversed. BSV provides us with a systematic and clean
way of doing this within hardware. The alternative would be to reverse the bytes
in software, and therefore lose cycles. \\

\begin{figure}[h]
  \centering 
  \scriptsize
\begin{lstlisting}[language=C,style=customc,xleftmargin=.3\textwidth]
struct MatUnitPtr {
  uintptr_t addr;
  uint8_t offset;
  uint8_t stride;
} __attribute__((packed));
struct MatUnitArgs {
  uint8_t unit;
  uint8_t count;
  struct MatUnitPtr ptr_a;
  struct MatUnitPtr ptr_b;
  struct MatUnitPtr ptr_c;
} __attribute__((packed));
\end{lstlisting}
  \normalsize
  \caption{Packed structure in C, corresponding to that of BSV, that is written directly to the
    control region of our accelerator memory. }
\end{figure}


% TODO: C padding - citations
Naturally, C structures are padded and aligned, where N-byte variables are
aligned to the nearest N-byte block. Variables in a struct are extended in size
in accordance with this padding and alignment criteria.
A structure in BSV is unpadded, and as such, it is vital to ensure that the
structure has no padding within our C code for when it is written to the
peripheral.


\section{Hardware floating point operations}
\subsection{Bluespec Verilog FloatingPoint}
We have a decision to implement calculations with rational numbers using either
fixed-point, or floating-point. While GCC has built-in support for fixed-point
arithmetic and fixed-point hardware is more simple to implement, we decide to
implement our accelerator with floating-point arithmetic.

BSV contains a FloatingPoint library. Modules already exist for addition and
multiplication which are polymorphic, extending the Server interface. The most
simple is the floating point adder module. The Server interface in Bluespec
makes use of an internal Put interface for a request, and a Get interface for a
response.

The floating point modules are parametised by the precision of our desired floating
point format. Naturally, we use the IEEE754 32-bit floating point standard, with a 1-bit sign,
23-bit significand and 8-bit exponent. We therefore make use of our own
single-precision floating-point datatype \texttt{FSingle}. For the sake of
convenience, we also define the datatype for the associated response, containing
a two element tuple of our result, and an exception.

By making use of the \texttt{Server} interface, we can we can put a request containing a
3-element tuple of our operands and the appropriate floating point rounding mode
to our floating point adder. After 6 clock cycles, we obtain a response. A test
implementation of this is shown below: \\

\begin{figure}[h]
  \centering 
\tiny
\begin{verbatim}
import GetPut :: *;
import ClientServer :: *;
import FloatingPoint :: *;

typedef FloatingPoint#(8,23) FSingle;
typedef Tuple2#( FSingle, FloatingPoint::Exception ) FpuR;

module mkFPAddTest(Empty);
   Reg#(UInt#(8)) cycle <- mkReg(0);
   rule rl_cycle(cycle < 255);
      cycle <= cycle + 1;
   endrule

   Server# (Tuple3# (FSingle, FSingle, RoundMode)
            , FpuR ) fpu_add <- mkFloatingPointAdder;
   rule rl_start(cycle == 0);
      FSingle opd1 = 1.0;
      FSingle opd2 = -2.0;
      fpu_add.request.put(tuple3(opd1, opd2, defaultValue));
      $display("%2d: Start", $time);
   endrule

   rule rl_end;
      match { .res, .exc } <- fpu_add.response.get();
      $display("%2d: Result: %h", $time, pack(res)); 
      $finish(0);
   endrule
endmodule

// OUTPUT:
// 10: Start
// 70: Result: bf800000 (-1.0)
\end{verbatim}
\normalsize
\caption{Example BSV code for a floating point addition testbench and the
  associated simulation output obtained from a BlueSim simulation.}
\end{figure}

% TODO: Pipeline --- diagram
We propose that the pipelined nature of the floating point units offered by the
Bluespec FloatingPoint library should offer a significant speedup. Provided that
the pipeline of the unit is fully occupied, it is possible to retrieve the result
of a floating point operation in every cycle. This feature will later be
exploited to greatly reduce the number of compute cycles within an individual
functional unit.

% TODO: Move to evaluation (?)
In order to simulate our simple module, we make use of Bluesim. Our Bluespec
Verilog is compiled directly into an executable, with no need to write a
testbench program in C++ as is required by Verilator. 

\section{Functional Unit Design}
The base compute unit that executes an issued command is known as a functional
unit. In the context of matrix multiplication, a single functional unit is
responsible for calculating a single element in the final matrix $C = AB$, by
calculating the dot product between the ith row of A, and the jth column of B,
as follows: $C_{ij} = A_{ik}B_{kj} = \text{row}(A,i) \cdot \text{col}(B,j)$.
This involves adding the pairwise result of multiplications to an accumulator.

\subsection{Fused Multiply-Accumulate (FMA)}
We make use of a floating-point fused multiply-accumulate (FMA) module. These are used
for addition and multiplication within the FPU of a processor. This allows the
floating-point operation $(a*b)+c$ to be performed with only a single rounding
step, thus saving compute cycles.

An FMA unit saves cycles compared to an individual floating-point multiplier and
adder circuit, as it only performs the floating-point rounding procedure once
both the multiplication and addition procedures have completed.

The simplest functional unit consisting of a fused-multiply accumulate circuit
should take $11$~cycles per element for an $N$~element vector.

% Cannot exploit pipelined nature of unit.
% Faster than individual FP multiply, then add. 10 cycles vs 14 cycles.

% TODO: move to eval
% 10: FSM Start
% 20: Init
% 40: PUT: 00000000 40800000 40800000
% 40: STATE_LOCK
% 140: 0: MulAcc Result: 41800000
% 140: STATE_READY
% 150: PUT: 41800000 40800000 40800000
% 150: STATE_LOCK
% 250: 1: MulAcc Result: 42000000
% 250: STATE_READY
% 260: PUT: 42000000 40800000 40800000
% 260: STATE_LOCK
% 360: 2: MulAcc Result: 42400000
% 360: STATE_READY
% 370: PUT: 42400000 40800000 40800000
% 370: STATE_LOCK
% 470: 3: MulAcc Result: 42800000
% 470: STATE_COMPLETE
% 480: Output: 42800000


% TODO: FMA diagram
% TODO: Explain how FMA works and advantages...
% TODO: Explain design decisions

\subsection{Pipelined Multiplication with Addition (PMA)}
We observe that the pipeline is not at maximum occupancy using an FMA unit. This
is because we need to feed the accumulator result back into the multiplier when
computing the accumulated product of the next two elements. As a consequence, we
waste $10N$~cycles throughout the computation for computing each element due to
the fact that the pipelined nature of FP hardware is not fuly utilised.

We observe that by having an internal buffer of memory within our functional
unit, we can pipeline our floating point multiplications. By subsequently
accumulating our result using a floating point adder, we observe that addition
will take $6N$ cycles, while the pipelined multiplication step will take $11+N$
cycles.

\begin{align*}
  T &= (11 + N) + 6N \\
  T &= 11 + 7N
\end{align*}
\subsection{Pipelined Multiplication \& Pairwise Addition (PMPA)}
In an attempt to maximise the occupancy of the addition pipeline, we propose a
more hardware-intensive strategy that makes use of pipelining for both the
multiplication and accumulation steps.

As before, in our first stage, we perform pipelined multiplication of our
elements into a buffer. As before, this will take $11+N$ cycles

We then perform reductive pairwise addition of the elements in the buffer.
The pairwise addition procedure has the advantage that we may fully occupy the
FP Adder pipeline. We then perform pipelined addition of pairs of elements.
It should take $\log_2{N}$ reductive steps, where each step consumes
$6+\frac{N_{i}}{2}$ cycles, where the constant term is the cost of flushing the
addition pipeline.

\begin{align*}
  T &= 6\log_2{N} + \left(\frac{N}{2} + \frac{N}{4} + ... + 1\right) + \text{const.}\\
  T &= 6\log_2{N} + N + \text{const.}
\end{align*}

The recursive algorithm only works cleanly when the size of our buffer is a
power of two. A naive implementation would therefore waste processing cycles
performing pairwise additions of the elements for the entire buffer size, rather
than the size of our input vectors. Instead, after each step, we guarantee that
our buffer is multiple of two, and collect together an array of remainder
elements. The power of this algorithm can be understood through binary
representations.

\begin{align*}
  N_i = 255_{10} = 11111111_{2}
\end{align*}

After each pass, we extract a remainer if it exists, then bitshift $N_i$. At
most, the binary representation of the number of elements in our input vector is
the number of 1s in the binary representation of our input vector $B$. Therefore, there will
be a maximum of $\log_{2}{N}$ remainder values to place in a seperate buffer and
accumulate. The accumulation process will therefore take at most $6\log_{2}{N}$ cycles.

Therefore, the total cycle consumption is:
\begin{align*}
  T &= (N) + (6\log_2{N} + N) + (6\log_2{N}) + \text{const.} \\
  T &= 2N + 12\log_2{N}
\end{align*}

In this way, we've managed to very substantially reduce the linear constant
factor through the use of pipelining. This is where Bluespec Verilog shines,
scheduling complex pipelining behaviour through the extensive use of FIFOs, and
avoiding hardware race conditions is an incredibly challenging feat. It would be
an excruciatingly difficult process to write this in standard Verilog HDL. 

\subsection{Summary of Predicted Performance}
% Unit Design: TODO: Include diagrams
We observe that through further pipelining optimisations we substantially reduce
the amount of cycles required to calculate a matrix element using an individual
functional unit. Generally speaking, by using a more complicated functional unit
design, we can obtain a lower linear coefficient with the cost of a greater
constant term.

\begin{align*}
  T_{\text{FMA}}(N) &= 11N \\
  T_{\text{PMA}}(N) &= 7N + \text{const.}  \\
  T_{\text{PMPA}}(N) &= 2N + 6(\log_2{N} + \log_2{B}) + \text{const.}
\end{align*}

\subsection{Multiple Functional Units \& Multi-ported memory}
We observe that the rate at which we issue instructions to the accelerator
exceeds the compute speed of the functional unit. Crucially, our firmware stalls
its execution and waits for the accelerator to complete its task before issuing
a new instruction. Thus, we are compute-bound.

We seek to exploit the parallelism offered in hardware design by duplicating our
functional units, sequentially issuing commands to different functional units
without stalling our processor's execution. By keeping our processor occupied,
we maximise throughput. With multiple functional units executing their
instructions simultaneously, we should be able to obtain a compute speed
improvement of a multiple of the number of functional units we have.

A standard BRAM RegFile in BSV has 2 read ports and 1 write port. As such, in a
single clock cycle it is only possible to read from two random access locations,
and write to one location of a RegFile. Thus, we must modify the architecture of
our memory to support multiple functional units.

\begin{figure}[h]
\scriptsize
\begin{verbatim}
 Error: "MultiPortBRAM.bsv", line 18, column 37: (G0002)
 `pmem_mem_0.sub' needs more than 5 ports for the following uses:
 `pmem_mem_0.sub a__h30554' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h31995' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32028' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32061' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32094' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32127' at "MultiPortBRAM.bsv", line 18, column 37
\end{verbatim}
\normalsize
\caption{BSV compiler error when over 5 random access reads are scheduled to
  perform in a single clock cycle. A RegFile typically only has 2 read ports.}
\end{figure}

We observe that the standard BSV RegFile, has a maximum of 5 read ports, and 1
write port. We exploit the ability to parameterise our modules in BSV, and
create our own custom MultiPortedMemory module. Our vector of RegFiles will
allocate seperate RegFiles at different locations on the FPGA fabric. They may
be then accessed in parallel.

The BSV interface for our MultiPortBRAM module is similar to that of the RegFile
module. However, the type has an extra parameter, $n$, representing the number of memory
channels. The multi-ported memory has only one write port, but has $2n$ read
ports. This is because it contains $n$ channels of RegFile BRAM and can
distribute its reads to $n-1$ functional units and the AXI bus within a single
clock cycle appropriately.

\begin{figure}[h]
  \centering
\scriptsize
\begin{BVerbatim}
interface MultiPortBRAM_IFC#(type addr, type data, numeric type n);
   method Action upd(addr a, data x);
   method ActionValue#(data) sub(Bit#(8) c, addr a);
endinterface
module mkMultiPortBRAM#(Integer low, Integer high)(MultiPortBRAM_IFC#(addr, data, n))
provisos(
   Bits#(addr, addr_sz),
   Bits#(data, data_sz),
   Literal#(addr)
   );
   // Create N channels of memory
   Vector#(n, RegFile#(addr, data)) mem <- replicateM(mkRegFileWCF(fromInteger(low), fromInteger(high)));
   method Action upd(addr a, data x);
      for(Integer i=0; i<valueOf(n); i=i+1)
         mem[i].upd(a, x); // Update ALL internal memory channels
   endmethod
   // Read from the channel with index 'c'.
   method ActionValue#(data) sub(Bit#(8) c, addr a);
      return mem[c].sub(a);
   endmethod
endmodule
endpackage
\end{BVerbatim}
\normalsize
\caption{Multi channel BRAM hardware module, parameterised by the number of
  channels, n, and the low and high allocation indices to determine the internal
BlockRAM size.}
\end{figure}
When we write to our accelerator memory through the AXI4 bus or from one of our
functional units, we write to all memory channels. A write operation can only be
performed at one random access memory address per cycle.

Our new multi-ported memory is read at a different channel by each different
functional unit. As such, we are capable of reading memory at the maximum rate
possible. Our individual functional units do not use too many RegFile read ports.

% TODO: Discuss somewhere else?
% This leads to one of the key development problems experienced, rule conflicts.
% By default, all rules with a true predicate execute on each cycle unless there
% is a conflict between them.


\chapter{Linear Algebra Peripheral: Evaluation}
\section{Methodology}
\subsection{Benchmarking \& Counting cycles}
From the Risc-V specification, we take a look at the mcycles and minstret
registers. We observe that it is possible to read from CSRs by running the
appropriate instructions within Risc-V assembly. We therefore make use of inline
assembly. The Risc-V specification highlights the \texttt{mcycles} CSR, that is
responsible for storing the number of cycles that the processor has performed.
This can be accessed from our firmware, and used to provide an estimate of the
runtime of one of our algorithms. Crucially, it is an overestimate and there is
some inherent overhead of cycles taken to read from the CSR itself, this
cycle overhead itself can differ depending on whether the stack is pre-allocated
or not when the instruction is ran or other factors dependent on how the
compiler decides to optimise the C firmware when it is compiled into Risc-V
instructions within machine code.

\begin{figure}[h]
\begin{lstlisting}[language=C,style=customc]
#define read_csr_safe(reg) ({ register long __tmp asm("a0");  \
      asm volatile ("csrr %0, " #reg : "=r"(__tmp));          \
      __tmp; })
void dummy_function() {
  uint32_t time0, time1;
  time0 = read_csr_safe(cycle);
  // [...]
  time1 = read_csr_safe(cycle);
  printf("Run cycles: %d\n", time1-time0);
}
\end{lstlisting}
  \caption{Methodology for determining the number of cycles elapsed from within
    our bare-metal firmware, making use of the mcycles CSR and inline assembly.}
\end{figure}

\section{Results}
\subsection{Functional Unit Type}
We seek to benchmark the execution time of the individual functional unit types.
Clearly, for a large $N$ with large matrices, our pipelined multiplication \&
pairwise addition unit is the most cycle efficient. Yet it is vital to consider
the cases where the constant offset plays a role. This is particularly relevant
for 'flatter' matrices, where the size of our input vectors entering our
functional units are smaller.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./py/c2_fut.pdf}
  \caption{Cycle counts of performing a hardware dot product of varying input
    vector sizes for three different functional unit designs. We benchmark the
    cycle count of computations for our fused multiply accumulate (FMA),
    pipelined multiplicaton then addition (PMA) and pipelined multiplication and
    pairwise addition (PMPA) hardware modules.}
  \label{fig:c2_fut}
\end{figure}

Our predicted cycle counts for each of our functional units are close to the
values obtained in simulation.
From our data (Figure \ref{fig:c2_fut}), we observe that the cycle time of our \texttt{PMA} unit is $9N$,
as opposed to our theoretical prediction of $7N$. This is due to additional overhead in
switching the FSM of the accelerator after the pipelined multiplications are
complete. The cycle time of our \texttt{PMPA} unit is $4N$ instead of $2N$ for
the same reason relating to the switching of our FSM. Fortunately, our predicted
cycle count for our \texttt{FMA} unit of $11N$ is correct.

We observe that our cycle count for our \texttt{PMPA} unit is no longer
monotonially increasing. Interestingly, the cycle count drops very significantly when our input vector
size is a power of 2. This is because the cycle overhead of summation of
remainders for each 1 in the binary representation of the input vector size
disappears once the size of our input vectors are powers of 2. This overhead
significantly exceeds the extra time taken to compute the dot product result
with an extra element, thus explaining why the cycle count decreases.

Alternatively, with more complex logic and conditional checks. Perhaps by
dummy-writing zero cells in our buffer. We may, in principle, be able to avoid
the $6\log_2{N}$ overhead associated with accumulating our remainders by appending
a zero to our pairwise reduced vector whenever it is odd.

Crucially, our \texttt{PMPA} module significantly outperforms the other
functional units. This can be attributed to the extensive use of pipelining in
order to perform the computation. Consequently, the hardware maximises the
amount of computation throughput and occupancy of our floating point units
multiplier and adder per cycle.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./py/c2_fut_zoom.pdf}
  \caption{A enlarged view of cycle counts of performing a hardware dot product of varying input
    vector sizes for small matrices.}
  \label{fig:c2_fut_zoom}
\end{figure}

As predicted, for small matrices, $3 \times 3$ or below, (Figure
\ref{fig:c2_fut_zoom}), our \texttt{FMA} circuit is superior due to the lower
constant term associated with simpler hardware. Our pairwise addition steps
within our \texttt{PMPA} unit fail to fully saturate our addition pipeline.
After each pairwise pass we must wait for the floating point unit to flush
before repeating the process. Since the number of elements in our input vector
is small, we never reach full pipeline occupancy and obtain the speed
improvements we otherwise would with larger input vector sizes. For matrices
that are larger than this, the highly pipelined \texttt{PMPA} module is
superior.

The greatest disadvantage of functional units that are not the \texttt{FMA}
module is the reliance on an internal \texttt{RegFile}, which will be
synthesised as BlockRAM. Crucially, we must maintain this internal buffer of
memory. Moreover, the dot product of input vectors exceeding the size of our
internal buffer cannot be calculated in a single pass of our functional unit.

If we seek to multiply matrices that exceed the size of our internal buffers, we
must write appropriate firmware to split the computation up, and we run into our
bottleneck associated with the rate at which we can issue instructions.

We observe that our PMPA and PMA functional units could still be improved.
Currently, the internal structure of the PMPA and PMA units is that of a finite
state machine where the units are either performing the multiplication, or the
addition step. In principle, with additional work and a lot of care to prevent
rule conflicts and race conditions, it would be possible to extract a further
speed improvement by performing both of these tasks in parallel, as the addition
procedure may indeed be started before the multiplication process is complete.

\subsection{Functional Unit Parallelism}
We make use of the fused multiply-accumulate functional unit for benchmarking
purposes. This ensures that our individual functional units are saturated, and
that our parallelism bottleneck is reached with a greater number of functional
units. Our pipelined multiplication with pairwise addition unit is significantly
faster and will therefore reach the bottleneck with fewer functional units, as a
result we will need to multiply larger matrices together to obtain the same
demonstration of this bottleneck.

We seek to benchmark the multiplication of large matrices. We multiply a
$128\times 4$ and $4\times 128$ matrix with our accelerator. We copy our
matrices from our ROM to our accelerator. We then generate commands
within our firmware and dispatch them to our accelerator. We record the number
of cycles taken to compute our result within our firmware,
through using the \texttt{mcycles} CSR, and record the cycle time for our
hardware peripheral to complete the computation within the verilator simulation.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./py/c2_fu_unbuf.pdf}
  \caption{Cycle count of matrix multiplication of a $4 \times 128$ and $128
    \times 4$ matrices for a varying number of functional units. Compiled with
    arguments: \texttt{riscv64-unknown-elf -O2 -fno-inline -march=rv32imafc -mabi=ilp32f}}
  \label{fig:c2_fu_unbuf}
\end{figure}

\begin{table}[h]
  \centering
  \begin{tabular}{r|rr}
    \toprule
    Units & Peripheral & Processor \\
    \midrule
    0&?&21383\\
    1&14658&15155\\
    2&7572& 7957\\
    4&4247& 4844\\
    8&3986& 4570\\
    12&4133& 4707\\
    16&3946& 4581\\
    \bottomrule
  \end{tabular}
  \caption{Cycle count recorded within our hardware (Peripheral) and within our
    firmware (Processor) for our matrix multiplication benchmark. Overhead: Copy cycles: 4982}
  \label{table:c2_fu_unbuf}
\end{table}

From our plots (Figure \ref{fig:c2_fu_unbuf}) we observe that our cycle count
stops decreasing after 4 functional units are used simultaneously. As a
consequence, we are not reaching maximum utilisation of our compute resources on
our accelerator. This is because the rate at which we can issue instructions
becomes a bottleneck.

We observe a $\approx 300$~cycle discrepancy between our cycle counts when the
operation completes on our accelerator and our firmware. This is the overhead
associated with the firmware reading accelerator memory to determine if there
are available compute resources to issue an additional instruction.

Nevertheless, we do obtain a significant performance improvement. If we pre-load
our data onto the accelerator, we observe a $4\times$ cycle count improvement
with 4 functional units when multiplying large matrices. Upon further
optimisation and the use of \texttt{PMPA} units and a more optimised
communication protocol, we will most certainty obtain superior results. 

\subsection{Synthesis}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./img/accel_dg.png}
  \caption{The matrix accelerator attached to the SoC created in Chapter 1 for
    the purpose of synthesis within Xilinx Vivado.}
  \label{fig:accel_dg}
\end{figure}

Care was taken to ensure that the resources of the FPGA would not be exceeded,
by limiting the total number of functional units to 4, and therefore the memory
channels to 5. With $64$~KB of memory per functional unit, we would use $320$~KB of FPGA
BlockRAM. This is almost the entire majority of remaining cells once the CPU ROM
and RAM has been flashed. Crucially, we reduce the size of our memory regfiles
to $16$~KB, making our peripheral BlockRAM consumption fall to $80$~KB.

In the process of synthesis we discover one of the disadvantages of
parameterised hardware design. A very substantial amount of hardware may end up
being generated when only a single number is changed. Crucially, the abstraction
introduced results in a decrease of intuition for the 'finite' resource
allocation of the physical FPGA chip.

We perform synthesis with our peripheral and package it as an IP block using
Xilinx Vivado. Vivado detects that it is an AXI4 slave interface, and it is
simple and easy to connect the peripheral to the SoC, as can be seen in Figure
\ref{fig:accel_dg}.

However, we encounter a crucial disconnect between our open source development
methodology and the proprietary block design tool. The AXI4 bus interconnect within
Vivado has a 32-bit data width, whilst the AXI fabric from our simulations has a
64-bit width. Furthermore, there are discrepancies in the \texttt{ID\_WIDTH}
property. A degree of additional work and analysis of the specifics of
generating Xilinx-compatible AXI4 peripherals from Bluespec must be conducted.

\chapter{Commmunication \& Memory Bottleneck: Further Work}
From our earlier results, we observe that at some threshold, a greater number of
functional units does not necessarily translate into a greater accelerator
throughput. 

\section{Command Buffering}
\subsection{Firmware/Stack}
To address the issue of not being able to issue commands fast enough, we
pre-generate all of our commands in the stack such that we don't waste cycles
generating commands in-between issuing them. For firmware buffering, $20$ bytes of
memory must be reserved for each instruction issued. It is easy to see that this
is impractical. If our final matrix is $64\times 64$, we would consume a very
substantial $80$~KB on the stack for buffering these instructions. We are
limited both in terms of stack size, and BlockRAM. 

% TODO: include firmware
\begin{table}
  \centering
  \begin{tabular}{r|rr}
    \toprule
    Units & Peripheral & Processor\\
    \midrule
    0&?&21383 \\
    1&14658&15155 \\
    2&7458& 7957 \\
    4&4050& 4651 \\
    8&3855& 4351 \\
    12&3989& 4520 \\
    16&3955& 3329 \\
    \bottomrule
  \end{tabular}
  \caption{Cycle count recorded within our hardware and firmware when buffering
    instructions within firmware. Buffering: Overhead: Buffer cycles: 696. Copy cycles: 4982}
\end{table}

We observe very minor performance improvements, but nothing too substantial.
Whatever performance increase that is incurred may equally be attributed to
compiler optimisations, as we instruct GCC to optimise our produced machine
code with the \texttt{-O2} parameter. 

One of the problems with firmware buffering is that the memory wall of issuing
commands to the accelerator still exists, we still rely on the AXI4 bus latency
to write to accelerator memory.

\subsection{Hardware}
Within hardware buffering, we pre-emptively issue commands into an 'instruction'
region of hardware and set an associated 'program counter' on execution.
Instructions are then issued each cycle, removing the AXI4 memory bus latency.

We may have seperate regions of preloaded commands, and store only the start
addresses. As a consequence we need only set the program counter and mark
execution to perform an entire matrix multiplication.


% Very high savings --- best for small matrices, --- 32~ elements
% TODO: Fix scheduling conflicts.




% TODO: UNKNOWN TIMINGS?:
% TODO: Cycle count / byte read in IO memory (when bypassing the cache!?)
% TODO: CACHE R/W speed!?
We attempt more extreme measures
% TODO: Accelerator buffer/program counter. (Internal stack)

\section{Firmware Stalling/Interrupts}
One of the potential criticisms of the technique of constantly polling a region
of memory is that

% TODO: Cite this
On an ARM system, the hardware interrupt latency is $\approx 12$ cycles. On a
Risc-V system, interrupt behavior is exceedingly complicated. A Risc-V processor
has, by default, a memory-mapped peripheral known as a Platform Level Interrupt
Controller (PLIC). The PLIC is responsible for multiplexing multiple interrupt
requests of different priorities. While the PLIC registers interrupts and stores
the necessary information for prioritising them, it is the role of the firmware
to actually perform an associated task when an interrupt occurs.


% TODO: Cite: https://sifive.cdn.prismic.io/sifive/0d163928-2128-42be-a75a-464df65e04e0_sifive-interrupt-cookbook.pdf 
From the SiFive interrupt cookbook, we see:
\begin{lstlisting}[language=C,style=customc]
void machine_external_interrupt()
{
   //get the highest priority pending PLIC interrupt
   uint32_t int_num = plic.claim_complete;
   //branch to handler
   plic_handler[int_num]();
   //complete interrupt by writing interrupt number back to PLIC
   plic.claim_complete = int_num;
}
\end{lstlisting}

Crucially, reading the highest priority interrupt from the PLIC is a memory
read operation via the AXI4 bus and has a latency equal to that of polling our
accelerator memory. There is then the additional overhead of marking the
interrupt claim to be complete, and running the interrupt handler. Within our
interrupt handler we would need to then read from our accelerator memory to
determine which of our functional units are busy before issuing a new command.
All of this associated overhead would significantly reduce the ability of our
processor to rapidly issue instructions. There is also the additional overhead
of moving the variables in all of our registers into the stack when the
interrupt function is called. Crucially, we observe that the associated latency
of processing interrupts greatly supercedes that of looping and polling a region
of memory to determine whether an operation within the accelerator has completed.


\subsection{Other ideas}
% TODO: Why did we ignore interrupts. PLIC IS SLOW
% TODO: 64-bit bus


\section{Instruction jumping}
One of the more novel ideas for getting a marginal speed-up in issuing commands
is to reduce the processor branching overhead by incoorporating the stalling
logic into the processor itself.

% TODO: Rephrase BUBBLING
When making use of a for loop for branching, processor cycles are wasted
continuously loading the branch instruction, and bubbling the pipeline.

We observe that this overhead can be removed by moving the program counter into
a region of memory within our peipheral that jumps back to itself. This region
can then be overwritten to jump back to the address after the stalled section in
C when appropriate.

For testing purposes, we attempt to execute some example code within our
accelerator memory. This is a non-trivial process...
% TODO: Execute this code
% Highlight the issue
\begin{verbatim}
// MMIO PC
void __attribute__((noinline, section(".dummy_section"))) func_test() {
  print("Meme!\n");
}

// Linker script modified
extern unsigned char dummysec_start[];
extern unsigned char dummysec_end[];
void main() {
  /* uint32_t label_addr = &&label_test; */

  uint32_t func_size = dummysec_end-dummysec_start;
  /* printf("Func Size: 0x%X\n", func_size); */
  /* for(int i=0; i<func_size+4>>2; i++){ */
  /*   uint32_t* ft = (uint32_t*)&func_test + i; */
  /*   printf("\t%X - %X\n", ft, *ft); */
  /* } */
  memcpy((void*)(ACCEL_STAT_ADDR+8), &func_test, func_size);
  /* printf("Func Size: 0x%X\n", func_size); */
  /* for(int i=0; i<func_size+4>>2; i++){ */
  /*   uint32_t* ft = (uint32_t*)(ACCEL_STAT_ADDR+8) + i; */
  /*   printf("\t%X - %X\n", ft, *ft); */
  /* } */
  /* // Move func_test TO stupid SoC chip! */
  //asm volatile ("call % " :: "r"(&func_test));

  ((void (*)(void))0x80010CB8)();
  ((void (*)(void))0xC0002008)(); // Processor stall indefinitely.
  print("Test");
}

func_test address: 80010C98
Meme!
\end{verbatim}

\section{Further Work}
\subsection{Protocol efficiency/optimisation}
One of the most obvious ways of improving the communication rate is to compress
or improve the communication procotol. One of the ways in which we improve the
protocol is by packing our struct. As a consequence, our instruction that is
issued to the accelerator has fewer bytes, and fewer cycles are required to copy it.

To a degree, it is possible to compress our instructions. The addresses passed
to the accelerator are 32-bits. It is possible to instead directly provide the
16-bit offset instead. If we align our accelerator in our memory map
appropriately, the offset may be inexpensively obtained through a typecast
(\texttt{0xC0002900 (4B) -> 0x2900 (2B)}). This would reduce the size of our
instructions by 6 bytes.

% TODO: explain more

% Analysis of struct packing
% Attribute ((packed))
% uint8_t offset <-- redundant (-3 bytes)
% uint8_t unit <-- push to accelerator

% uintptr_t <-- Make ADDR 16 bits!?, subtract base address (64KB accel memory...) ()
% (-3*2=-6 bytes)
% 0xC0002900 --> 32 bits
% 2900 --> 16 bits
% 1000 (16-bit base address)
% 1900
% 6 bytes ~=? 6 cycles saved
% 2-4 cycles/base address subtraction
% ALIGN: ACCEL @ 16-bit base address boundary
% Therefore 0-cost LE typecast <---> address calculation! (SMART)
\subsection{Memory Striping}
One of the big problems with our accelerator is the design of the multi-ported
memory. By duplicating our memory writes across $n$ channels for $n-1$
functional units, we occupy a very significant amount of BlockRAM on our
accelerator.

% TODO: Synthesis
% The process of synthesis becomes a very delicate balance between...
Given more time, we could instead stripe our memory into separate banks, with a
low probability that more than two indexes are read from the same bank in the
same cycle. While this is unlikely due to the nature of memory accesses for matrix
multiplication, this can be made easier by also duplicating the striped memory
banks as shown previously.

% TODO: Analysis: Math to show memory consumption.
One of the most important considerations when implementing this form of striped
memory is how to apply the associated back-pressure and arbitration when two functional units
are reading from the same memory bank in the same channel. One of the units
would need to be stalled. It is likely that the processing and scheduling logic
for this would lead to wasted cycles, and therefore a reduction in compute
speed. It would be an interesting exercise to see how this reduction in compute
speed compares to the savings in BlockRAM.

\chapter{Summary and Conclusions} 
\section{Summary}
We have successfully made use of open source resources to synthesise a minimal
functioning Risc-V SoC. This system acts as a baseline and allows us to rapidly
prototype and simulate a linear algebra co-processor and develop the associated
firmware to communicate between the co-processor and a Risc-V core. We achieve
a significant performance improvement without sacrificing floating point
accuracy through exploiting the pipelined nature of the floating point Bluespec
Verilog libraries and designing our own highly pipelined multiplication pairwise
addition (PMPA) functional unit, which is almost three fold faster than a
conventional fused-multiply accumulate unit that is used within the floating
point units of conventional microprocessors. Through designing our own highly
parametised multi-channel memory module we take advantage of the inherent
parallelism offered by hardware and demonstrate a further four-fold computation
speed increase through the use of multiple functional units. We observe that the
limiting factor preventing further speed-up is the rate at which we can issue
instructions to our co-processor and we contemplate ways of increasing this.

\section{Conclusions}
We have verified the feasibility of our design through synthesis and
demonstrated the advantages through simulations.


% BlockRAM limitations
The design structure of the peripheral makes it simple to add additional
functionality. The structure may be replaced by a tagged union and different
operations may be decoded. For instance: if we want to accelerate neural network
inference, we may pass commands that apply activation functions to large vectors
of data. By sharing the same region of memory within the peripheral, there is no
data movement overhead. 



% TODO: FPGA resources occupancy --- blockRAM saturates quickly (In particular)
One of the shortcomings associated with the use of Bluespec Verilog observed
through the project, was that it is more difficult to keep track of the amount
of FPGA resources used upon synthesis. Due to the duplication of accelerator
memory across multiple channels and the parameterisation of many modules, small
changes in configuration or constants may have a impact on generated hardware
that is not intuitive to the designer despite providing more flexibility.

While we succeeded in performing synthesis with our minimal SoC system, one of
the principal shortcomings were the problems encountered in performing synthesis
with our peripheral.

While open source tools were used in the development process and for
simulations, we must still use the proprietary tools for synthesis.

The proprietary tools 



% NOTE: Clearly present argument and demonstrate that success criteria were
% met
% NOTE: Critical thought and interpretation of results
% NOTE: substantiate claims of success/novelty

% NOTE: Reflect on lessons learnt
% 'V' not ratified --- comparison would be nice

% Future Work
% Further work towards Synthesis --- make use of external DDR3 memory
% Implement Risc-V 'V' extensions

\appendix
\singlespacing

\printbibliography
% Bibliography:
% RISC-V GCC, ISA, Spike
% BSV Reference Guide

\end{document}
