%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%% 
%% 
%% SMH, May 2010. 

\documentclass[a4paper,8pt]{report}
% \documentclass[a4paper,12pt,twoside,openright]{report}


%% 
%% EDIT THE BELOW TO CUSTOMIZE
%% 

\def\authorname{Khalid Aleem\xspace}
\def\authorcollege{Trinity College\xspace}
\def\authoremail{ka476@cam.ac.uk}
\def\dissertationtitle{A Risc-V based co-processor peripheral to accelerate
  linear algebra operations.}
%\def\dissertationtitle{A Risc-V based co-processor to accelerate linear algebra operations.}
% TODO: LOOK INT LACore...
% Maximising throughput for a Risc-V linear algebra co-processor.
% XXX: Project title changes: no later than 21 May 2021 16:00 
\def\wordcount{3,860}


% \usepackage[dvips]{epsfig,graphics} 
\usepackage{epsfig,graphicx,verbatim,parskip,tabularx,setspace,xspace}

\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{fancyvrb,listings}

\usepackage[a4paper, margin=1in]{geometry}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily\color{black},
  keywordstyle=\bfseries\color{blue!40!black},
  commentstyle=\itshape\color{purple!40!black},
  stringstyle=\color{green!40!black},
  morekeywords={uint32_t,uint8_t,uintptr_t,intptr_t}
}

\lstdefinestyle{customcpp}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  xleftmargin=\parindent,
  language=C++,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily\color{black},
  keywordstyle=\bfseries\color{blue!40!black},
  commentstyle=\itshape\color{purple!40!black},
  stringstyle=\color{green!40!black},
  morekeywords={uint64_t,uint32_t,uint8_t}
}

\input{lst-riscv.tex}

%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing


\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 
\section{Motivations}
% NOTE: Clear motivation - justify the benefits of success
% NOTE: Contextualise what i'm building
% NOTE: Analysis of requirements and justified & documented use of tools!



% Work on Story
% Embedded Devices
% Common for embedded devices to have their own acclerators
% Reconfigurable hardware, software updates, self-driving cars.
% RiscV - ^ - 

% Seek to optimise openness & interoperability



% What are our main contributions:
% Improved upon/got working (FPU) testbench (verilator) for rapid prototyping and simulations
% Wrote functioning bare-metal firmware for a RiscV CPU
% Minimise dependence on proprietary and closed-source software + toolchains

% Accelerator: VLIW/Superscalar execution (?)

% Bluespec: Customisable number of functional units, varying silicon area.
% In the best case, obtained an approx 10x speed-up through hardware buffered
% commands. 


% TODO: Talk about modern processors structure - Internal FPGA - motivations
% Relate to Intel Acquisition of Altera --- (Recent) AMD Acquisition of Xilinx
% Looks inc
% TODO: Silicon surface? Embedded systems. - Which do I pick!?
% Mainly ^^ 
\section{Contributions}
This thesis is split into three main sections. In the first chapter, we fix
latent problems within the Bluespec Inc. Piccolo processor, attach our own
proof-of-concept memory mapped peripheral to the AXI4 bus, and design Risc-V
bare metal firmware to interface with the peripheral. We make use of the
modified Verilator testbench to verify the correctness of our design. This is
vital prerequisite work to base our later work off.

In our second chapter, we elaborate on our design principles and methodology for
designing our linear algebra accelerator. We implement and benchmark the behaviour
of differently designed functional units of increasing complexity. We also delve
into the memory modifications required to support multiple scheduling units. We
benchmark our different functional units, and benchmark our accelerator with a
varying number of functional units.
% TODO: We evaluate our design through Synthesis on an FPGA (Separate section)

During our benchmarks of our accelerator, we run into the problem of the memory
wall. In our third chapter, we address methods of improving this
bottleneck.


\chapter{Background}
% TODO: Hardware design in general(?)
\section{Risc-V Instruction Set}
% TODO: What is an ISA?
The set of operations a CPU may perform is known as the instruction set
architecture (ISA). An ISA provides the means for a
programmer to interface with a processor, allowing software to communicate with
hardware. The ISA also specifies the method of memory addressing, whether
addresses need to be aligned, and the structure of processor instructions. Most
desktop computers make use of the x86\_64 ISA, with embedded devices and mobile
phones commonly using the ARM ISA.


Risc-V is an open standard instruction set architecture (ISA). It is provided
under open source licenses, where anybody may use the ISA with no restrictions
over the use of intellectual property (IP). In contrast ARM ISA, which is widely
used in embedded devices, is proprietary. If the ARM ISA is used
for commercial purposes, the company expects a licencing deal to be made and
royalties to be paid. Licencing deals typically cost millions of dollars and can
take months to negotiate. These IP restrictions hinder the process of academic
research, and significantly raise the barrier of entry for processor design,
where only large companies like Qualcomm can compete.

As such, there is no incentive for processor designs to be made open source.
Obtaining the source code of most ARM processors is both difficult and
impractical due to these strict and costly licencing requirements. Modern ARM
instruction sets are bloated, needing to maintain necessary backwards
compatibility --- in many cases a modern ARM CPU will contain redundant
instructions, and therefore redundant logic and complexity that is unnecessary.

In contrast, the Risc-V ISA is modular, and can be partitioned into various extensions. Of
notable interest are the 49 (\textbf{I}) 32-bit integer instructions, 11
(\textbf{A}) atomic instructions, 36 (\textbf{C}) 16-bit compressed instructions, 8
(\textbf{M}) instructions for integer multiplication and division, and 25
(\textbf{F}) 32-bit precision floating point instructions. Consequently, a
functional 32-bit processor for an embedded system with hardware floating point
support requires only \textbf{129} instructions, with the \textbf{IMAFC}
extensions. This increases the ease of processor design, and avoids the creation
of unnecessary and superfluous hardware. 

% A - 11 (atomic instructions)
% C - 36 (compressed 16-bit instructions)
% RV32I - 49 (32-bit integers)
% M - 8 (integer x and /)
% F - 25 (32-bit floating point instructions)

% TODO: Risc-V cite: https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-146.html 
Since the publication of the Risc-V ISA, a variety of different open source
processors have been designed. The source code of these processors is publically
available, with a range of different designs available on GitHub. Collaboration
and open source processor design has a range of additional benefits in hardware
security. This is of particular relevance in the light of severe hardware-level
vulnerabilities found in the majority of ARM and x86\_64 processors in
production, namely Spectre, and Meltdown. With processor designers and
manufacturers paying less attention to verifying the security of their devices
whilst competing with each other to maximise processor throughput, the design of
processors is not subject to enough scrutiny within the development process.
Open source processor development provides the opportunity for processor designs
to be verified by a community of different organisations, reducing the
likelihood that architectural level attacks can be performed. By basing our work
on an open source processor in the development of our peripheral, we benefit
substantially from these security considerations.

Moreover, when designing a peripheral, it is vital to possess an understanding
of the processor that it is connected to. Using an open source processor gives
us greater freedom, allowing us to pry into the internals, and analyse the
specifics of processor behaviour if things behave in an unexpected manner.

\section{Simulating Hardware} 
The fastest way to test and verify the design of hardware is through simulations.
Creating a silicon application-specific integrated circuit (ASIC) remains an
incredibly costly and time consuming process.
There is no room for rapid prototyping, and hundreds of thousands of units need
to be created and sold at a profit for the process to be economically
feasible. Fabricating a microprocessor at each stage in its development process
for testing purposes is too expensive. As a consequence, there is very
limited room for fabricating an integrated circuit for testing purposes in
industry, and almost no room for fabrication in academic circles.

\subsection{Verilator}
Verilator is a piece of open source software that compiles Verilog into C++. A
testbench can then be created, where the generated C++ class representing the
top-level module in hardware, can be stepped forwards in time with the clock
toggled from low to high accordingly, and signals propagated through simulated
hardware accordingly.

In order to run our verilator simulation, we include our verilator-compiled C++
headers and libraries that were generated from our hardware into our C++ test
bench. We then proceed to initialise the verilator class representing the
top-level module of our hardware design. It is then possible to toggle the clock
signal and propagate our simulated hardware forwards in time. This is done as
follows:
\begin{lstlisting}[language=C++,style=customcpp]
#include <verilated.h>
#include "VTop.h"
uint64_t main_time = 0;
double sc_time_stamp() { // Called by the $time function in Verilog
    return main_time;
}
int main (int argc, char **argv, char **env) {
    Verilated::commandArgs(argc, argv);
    VTop* top = new VTop; // Create top-level module
    while(!Verilated::gotFinish()) {
        if ((main_time % 10) == 5) { // 10 loops per cycle
            top->CLK = 1; // Raise clock at 5, 15, 25, ...
        }
        else if ((main_time % 10) == 0) {
            top->CLK = 0; // Lower clock at 0, 10, 20, ...
        }
        top->eval(); // Propagate changes
        main_time++;
    }
    top->final(); // end simulation
    delete top;
    return 0;
}

\end{lstlisting}

While verilator suceeds in simulating hardware logic, it does not take physical
considerations into account. For instance, every hardware design has a route through
combinatorial logic known as the critical path, this is the circuitry where
speed of propagation leads to a bound on the maximum clock rate that is
achieveable.

% https://www.veripool.org/papers/Verilator_Fast_Free_Me_DVClub10_pres.pdf

\subsection{FPGAs}
% TODO: image
The most effective way to simulate hardware is by using a Field Programmable
Gate Array (FPGA). An FPGA consists of a large lattice of programmable logic
blocks interwoven by a grid of configurable wires and contact points. The
programmable logic blocks range from boolean logic lookup tables (LUTs), clocks,
registers, DSP units and block RAM. The lattice of programmable logic cells can
be flashed, where the individual hardware units are programmed, and connected
together by configurable wires such that the circuit described in hardware is
physically created. A processor simulated on an FPGA can typically be clocked at
a few hundred MHz. An FPGA can therefore offer over a thousand-fold performance
increase compared to conventional software simulations of hardware circuits.

Unlike software simulations using verilator, an FPGA can suffer from physical
limitations of hardware layout. A processor that isn't pipelined sufficiently
may have a shorter critical path, preventing the generated hardware on an FPGA
from reaching a sufficiently high clock rate. Furthermore, additional care must
be taken to prevent signal timing violations, and clock domain crossing within
the FPGA.

% TODO: Clock domain crossing?
% TODO: Types of FPGA(?)

\subsubsection{Synthesis}
% TODO: FPGA picture
The process by which an FPGA is programmed with some hardware design is known
as synthesis. The goal of the synthesis process is to produce a bitstream from a
hardware design. The bitstream contains the necessary information required to
program the logic blocks of an FPGA and connect them appropriately.

The first stage of synthesis involves compiling Verilog HDL into a netlist.
A netlist is a complete description of a digital circuit at the logic gate
level, containing all of the information a schematic would, but in a structured
format that is easier for a computer to process.

The second stage involves a process known as place and route. Different FPGAs
have different architectures and designs, with varying numbers of logic cells
and logic cell types. Place and route involved running an algorithm, targetting
a specific FPGA, which figures out a sufficiently optimal placement of logic
cells and connections. Lastly, the design is verified for timing consistency,
and the bitstream is generated.

\subsubsection{Symbiflow}
% Symbiflow - the future. (Find paper) 
% TODO: Cite: https://ieeexplore.ieee.org/document/9103284
The tools required for synthesis are closed source and proprietary, requiring
licenses for use in commercial purposes. Both Xilinx and Altera, the major FPGA
vendors, employ a greater number of software engineers than hardware engineers
within their FPGA divisions to develop their complex closed-source proprietary
software for synthesis.

The Symbiflow project is an open-source computer aided design (CAD) flow used to
program commercial FPGAs. The proprietary bitstream format for the Xilinx
Arty-A7 has been fully reverse engineered.
The Symbiflow project is capable of performing synthesis, placement, routing
and bitstream generation for this device, where no proprietary licence is required. 


\section{Hardware Design Languages}
Designing hardware in traditional Verilog is a highly inefficient process. There
are no data structures to group together inputs and outputs of hardware modules
that represent different communication channels. Therefore wiring a submodule
appropriately is an arduous process, prone to errors. Code that is highly
modularised, counterintuitively, becomes harder to read and understand.
Moreover, if a single wire needs to be modified, it needs to be explicitly
stated in every instance of a module, a time consuming process.

This can be seen as follows:
\tiny
\begin{verbatim}
  mkUART uart0(.CLK(CLK),
         .RST_N(RST_N),
         .put_from_console_put(uart0$put_from_console_put),
         .set_addr_map_addr_base(uart0$set_addr_map_addr_base),
         .set_addr_map_addr_lim(uart0$set_addr_map_addr_lim),
         .slave_araddr(uart0$slave_araddr),
         .slave_arburst(uart0$slave_arburst),
         .slave_arcache(uart0$slave_arcache),
         .slave_arid(uart0$slave_arid),
         .slave_arlen(uart0$slave_arlen),
         .slave_arlock(uart0$slave_arlock),
         .slave_arprot(uart0$slave_arprot),
         .slave_arqos(uart0$slave_arqos),
         .slave_arregion(uart0$slave_arregion),
         .slave_arsize(uart0$slave_arsize),
         .slave_arvalid(uart0$slave_arvalid),
         .slave_awaddr(uart0$slave_awaddr),
         .slave_awburst(uart0$slave_awburst),
         .slave_awcache(uart0$slave_awcache),
         .slave_awid(uart0$slave_awid),
         .slave_awlen(uart0$slave_awlen),
         .slave_awlock(uart0$slave_awlock),
         .slave_awprot(uart0$slave_awprot),
         .slave_awqos(uart0$slave_awqos),
         .slave_awregion(uart0$slave_awregion),
         .slave_awsize(uart0$slave_awsize),
         .slave_awvalid(uart0$slave_awvalid),
         .slave_bready(uart0$slave_bready),
         .slave_rready(uart0$slave_rready),
         .slave_wdata(uart0$slave_wdata),
         .slave_wlast(uart0$slave_wlast),
         .slave_wstrb(uart0$slave_wstrb),
         .slave_wvalid(uart0$slave_wvalid),
         .EN_server_reset_request_put(uart0$EN_server_reset_request_put),
         .EN_server_reset_response_get(uart0$EN_server_reset_response_get),
         .EN_set_addr_map(uart0$EN_set_addr_map),
         .EN_get_to_console_get(uart0$EN_get_to_console_get),
         .EN_put_from_console_put(uart0$EN_put_from_console_put),
         .RDY_server_reset_request_put(uart0$RDY_server_reset_request_put),
         .RDY_server_reset_response_get(uart0$RDY_server_reset_response_get),
         .RDY_set_addr_map(),
         .slave_awready(uart0$slave_awready),
         .slave_wready(uart0$slave_wready),
         .slave_bvalid(uart0$slave_bvalid),
         .slave_bid(uart0$slave_bid),
         .slave_bresp(uart0$slave_bresp),
         .slave_arready(uart0$slave_arready),
         .slave_rvalid(uart0$slave_rvalid),
         .slave_rid(uart0$slave_rid),
         .slave_rdata(uart0$slave_rdata),
         .slave_rresp(uart0$slave_rresp),
         .slave_rlast(uart0$slave_rlast),
         .get_to_console_get(uart0$get_to_console_get),
         .RDY_get_to_console_get(uart0$RDY_get_to_console_get),
         .RDY_put_from_console_put(uart0$RDY_put_from_console_put),
         .intr(uart0$intr));
\end{verbatim}
\normalsize

The traditional HDLs were built to be hardware simulation languages and were not
intended for the purpose of designing very complex hardware, such as processors.
Modern software development programming languages have abstractions, such as
structures and functions, improving productivity, encouraging the reuse of
code, and eliminating unnecessary 'cruft'.

To figure out how a module behaves, one must constantly move around the file,
observing how the state changes when registers flip in an \texttt{always\_ff}
block, usually used to control a finite state machine. Simultaneously one must
also keep track of how the combinatorial state of the hardware changes, by
looking at an \texttt{always\_comb} block.

Due to the difficulties concerning development time, and for purposes of
efficiency, more expressive hardware design languages have been created, namely,
ChiselHDL and Bluespec Verilog. The Risc-V Rocket processor is designed in
ChiselHDL, whilst the Risc-V Piccolo, Flute and TOOOBA processors are written in
BSV. The creation of a soft-core processor is a demonstration of the ability of
these languages to be applied in practice and make the process of processor
design more efficient and structured.
% TODO: Downsides of traditional Verilog
% TODO: \subsection{High-level synthesis}


% TODO: Cite https://people.eecs.berkeley.edu/~krste/papers/chisel-dac2012.pdf 
\subsection{ChiselHDL \& Bluespec Verilog}
Chisel (Constructing Hardware In a Scala Embedded Language), is a new
programming language for hardware design that is based on Scala, that is
executed on a JVM.

Chisel is capable of generating low-level Verilog from high-level hardware
design patterns that are written in Scala.

Chisel allows inputs and outputs of modules to be bundled together, and
components to be

% Chisel generator / example
% Chisel I/O / Module + Bundle of Bundles

This is best demonstrated with the AXI4 protocol:
\tiny
\begin{verbatim}
class AXIPeripheral extends Module {
  val io = new Bundle {
    val slave = new AXISlaveIF(8, 32)
  }
  [...]
}
\end{verbatim}
\normalsize

Code may be written in Scala to generate hardware.


% TODO: Chisel Bundle, Component (explain)

% Features
% Atomic rules, structs,
% Correct concurrency - strict avoidance of race conditions

% Superior to Chisel: rules + methods
% All types are first-class --> functions can be used as arguments/parameters of
% other functions and modules.

% Static & dynamic elaboration

% BSV --> Bluesim (small simulations)
% BSV --> Verilog RTL --> Verilator / Synthesis

% Example code / Comparisons
% Benefits

% TODO: Cite https://dl.acm.org/doi/abs/10.1109/MEMCOD.2004.1459818 


% TODO: Talk about SystemC \& HLS...

\chapter{Related Work} 
\section{BlueVec}
% https://www.cl.cam.ac.uk/~swm11/papers/FPL2013-BlueVec.pdf
The BlueVec co-processor is a vector processor aimed at accelerating neural
network inference designed to address the computation botleneck of the problem
of the \emph{memory wall}.
The memory wall refers to the growing discrepancy between memory bandwidth and
compute power. It is increasingly common for an accelerator system to become
memory-bound, where the latency/clock cycle cost of moving data to and from
memory may even exceed the reduction in compute cycles from having a custom
accelerator pipeline.

% TODO: Explain Memory wall further?
The BlueVec co-processor relies on the observation that in many situations where
a custom accelerator pipeline is implemented to accelerate some computation
through performing it in hardware, a significant improvement may still be obtained
by vectorizing the computation, and implementing the computation within
software making use of SIMD instructions. The paper demonstrates the example of
neural network inference, where the BlueVec co-processor is used in the context
of a digit recognition problem.

The BlueVec co-processor is interacted with through the use of a custom
instruction set extensin for the Nios II. The Altera Nios II is a soft-core processor
that is designed to be synthesised on an Altera FPGA. It has a 32-bit RISC-based
architecture, and while the internals are closed source, it is possible to add
custom instruction set extensions within hardware. Altera provides their
proprietary embedded design suite (EDS), where it is possible to compile C/C++
software targetting the Nios RISC architecture, and making use of the custom
instructions that are implemented in hardware.

The ability to add custom instructions to the Nios processor provides one with
sufficient versatility to fine tune the designed hardware to perform specific
tasks very efficiently, making use of the parallellisation-based strengths of
designing things in hardware.

One of the key ways in which the BlueVec processor tackles the problem of the
memory wall is by ensuring that memory is written to and read from in a
streamlined fashion. Vectorised data is stored in contiguous arrays within
memory, and the BlueVec co-processor highlights that it makes efficient use of
external memory bandwidth through the use of burst memory access to stream data
from external memory at a very high rate.

% TODO: Lane-local memories / Pipelining
% Each thread has its own LANE. --- Similar to multi-channel memory.

In order to maximise the clock frequency of the BlueVec processor, and therefore
maximise throughput, the BlueVec processor pipelines operand fetching,
instruction execution and result writeback. This reduces the length of the
critical path.

% TODO : Link to BSV
The BlueVec co-processor consists of approximately 1000 lines of Bluespec
Verilog and was developed over the course of only a few months. The quick, and
minimalist development practices used during its construction is a testament to
the benefits of the use of BSV over standard Verilog HDL.

One of the principal disadvantages of the BlueVec co-processor is its reliance
on proprietary synthesis tools (Altera Quartus), and the closed source soft-core
processor. The development practices, and soft-core Nios processor involved in
the construction of BlueVec lock us into the Altera software ecosystem. An
Altera Quartus licence in particular is required to synthesise the hardware on
an Altera FPGA and run the associated benchmarks in the paper. We do not have
the ability to easily move the design to a Xilinx, or iCE FPGA, as not only is
the soft-core Nios processor closed source, we must also rely on the
automatically generated Avalon bus interconnect from the Altera Qsys SoC
designer. As such, a substantial amount of work is required for the BlueVec
processor to be moved onto a different platform.

% TODO: Reliability considerations - vendor black boxes. Bugs (if found) -
% impossible to correct. (Another section!?)

Furthermore, we highlight the loss of interopability. We are constrained to the
Nios instruction set. There is no way for an ARM, MIPS, Risc-V, or x86\_64 based
system to interface with, and make use of the vector co-processor. With the
growth of ARM processors used in embedded devices, and the recent emergence of
Risc-V, this lack of interopability is a costly downside to the performance
improvements demonstrated by BlueVec for vector processing.

% TODO: Comment on Benchmarks
% TODO: Bluevec - fails to hit capacity - CPU can't issue one instruction per
% cycle

% NOTE: BlueVec - Literature Review
% NOTE: General argument --- elaborate upon motivations
% NOTE: BlueVec --- Nios. Custom instructions ---> Loss of interopability
% NOTE: Are there truly multiple functional units? Instructions STALL the cpu.
% NOTE: Unadjustable silicon consumption --- less control!

% TODO: ZynqNet - Literature Review
% NOTE: ZynqNet --- too SPECIFIC, NN architecture needed beforehand.
%\section{ZynqNet}

% TODO: Read ZynqNet Paper
% How to contextualise!?
% NO general-purpose linalg unit

% Google TPU? & Nvidia GPUs? --- Look into

% TODO VLIW?

% Lots of GPU acceleration of linear algebra acceleration 
% Big takeaway - unless super specific problem --- communication bottleneck
% existed
% Whole computation must be performed in GPU memory --- constrained

% Tightly coupled Accelerator+CPU. General programmability + fixed hardware. Low
% latency between each.

% TODO: Look at Linpack library
% https://link.springer.com/article/10.1007/s11390-011-0184-1

\section{SIMD}
The SIMD acronym refers to processor instructions that operate on multiple
registers of data (Single Instruction Multiple Data). We therefore save
processor cycles through a technique known as DLP (Data level parallelism),
where processor instructions, and therefore fewer cycles, are required in order
to perform the same mathematical operation.

\subsection{Risc-V Vector ISA extension}
The Risc-V ISA has the (\textbf{V}) extension to support vector instructions.
That is, instructions that operate on a contiguous region of memory. The ISA
extension specifies the existence of vector registers which may be some power of
2 larger than a traditional 4-byte (32-bit) register. Mathematical operations
may then be simultaneously applied to groups of numbers loaded in these vector
register.

Crucially, the Risc-V vector extension has not yet been ratified. As such, its
features may change, and it is unwise to implement the instructions as the ISA
may undergo breaking changes. Ratification is a time consuming process. The
Risc-V foundation needs to be certain that the definition of the ISA extension
is infallible, where considerations of backwards compatability and the necessity of
the proposed instructions are verified by a group of experts.

Moreover, The vector ISA extension consists of 184
additional instructions, exceeding the preexisting 129 instructions in the
RV32IMAFC combination. A very substantial modification to the decode and
execution sections of the processor will need to be made at the cost of a great
deal of silicon space. Even if the ISA is implemented, compilers do not yet
support producing vector instructions in machine code.


\subsection{ARM Neon/x86 SSE,AVX}
% TODO: elaborate - what are the key disadvantages
% TODO: Small subsection at top of SIMD!?
The AVX-512 instruction set extension consists of 16 x 4 byte, or 512-bit,
registers. 

Perhaps the principal disadvantage of vector ISAs is that a small subset of the
ISA is required to accelerate a particular task at hand. While it is necessary
for the entire ISA to be implemented at the cost of both silicon surface and
power efficiency.



% TODO: ARM Neon / Risc-V 'V' extension
% Why is ratification slow --- needs to be done CORRECTLY!
% Think about future support, backwards compatability.
% Some applications only need one part of it.
% Neon SSE/AVX
% Neon is defined within ARM ISA
% Very heavyweight --- complex decoding logic, lots of instructions
% ARM isn't truly RISC. --- SIMD extensions

% speciality --- linear algebra - dot products. Lots of writing/reading. Gain
% order of magnitude efficiency for SPECIFIC matrix multiplication.
% NN, 

\section{Open Source Cores}
Since we seek to minimise the use of proprietary, and closed-source libraries,
we avoid using proprietary soft-cores. Notably, Xilinx has its Microblaze
processor, while Altera has its Nios II cores. We may base our accelerator on
one of these, but we are immediately locked into either the Xilinx or Altera
proprietary hardware development (CAD) ecosystems. It is not even possible to
simulate an SoC of one of these processor attached to our peripheral
Moreover, the proprietary simulation tools offer similar or worse performance
compared to Verilator. Verilator provides us with less-overhead and greater
freedom within our simulations, giving us direct control to our testbench with
C++. In contrast, the proprietary xsim, and modelsim have more required overhead
to generate a clock, initialise our memory and simulate an SoC.

\subsection{Rocket}
% TODO: talk about Rocket
% TODO: Cite Rocket Chip Generator:https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.pdf 
The Rocket processor is written in ChiselHDL. It is packed with a utility known
as the Rocket Chip Generator, which generates an SoC consisting of the Rocket
CPU and the necessary peripherals required for a functioning system. 

% TODO: Elaborate: Freedom means physical + commercial use has been demonstrated
% The Freedom E310 is a Risc-V SoC designed by the company SiFive, the Risc-V ASIC
% SoC designed by SiFive is 

The Rocket processor has the Rocket Custom Co-processor (RoCC) interface. This
allows an accelerator to be implemented through the use of custom instructions.
By having the hardware of the processor itself fetch and execute instructions as
opposed to a firmware and memory-mapped approach, the RoCC system saves
processor cycles. This is accomplished by eliminating the associated overhead of
reading/writing from memory via an AXI4 bus.

% TODO: verify this claim
One of the principal disadvantages of the RoCC is its tight coupling to the CPU.
The RoCC constrains us to using the Rocket processor in particular, and alters
the processor's decode and execution units. Because of instruction set
dependency, we are constrained to Risc-V processors, and therefore making
sacrifices in terms of interoperability. 

Further arguments may be made in terms of silicon complexity. Communicating with
an accelerator through an AXI4 bus allows the silicon of the accelerator to be
physically separated from the CPU. This provides us with greater benefits in
terms of heat dissipation in the circumstance that an ASIC is created.

One of the key disadvantages of ChiselHDL is that it does not offer the same
degree of scheduling abstraction that exists with BSV. While Scala gives us the
ability to generate arbitrary and parameterised hardware, it does not provide
the benefits of designing hardware which executes atomic rules and the
processing/execution of complex dependency tree.

\subsection{Bluespec Risc-V CPUs}
% TODO: MIT TOOOBA project
% TODO: Cite paper
The Bluespec designed processors take full advantage of the modular nature of
processor design that is made possible through the use of Bluespec Verilog.
Seperate processor ISA extensions, the processor register width, and the AXI4
bus size may be set appropriately on compilation of the BSV code into Verilog
RTL.

Bluespec Inc. has created three example processors, the Piccolo processor
consists of a 3-stage in-order pipeline, and is designed primarily for small
embedded systems. The Flute processor contains a 5-stage in-order pipeline, and
contains an MMU, allowing it to obtain a higher clock rate and boot a linux OS.
The Toooba processor, based on the MIT Riscy-OOO, has a superscalar architecture
and supports out-of-order execution.

All of these designed processors have the advantage of an example testbench
created in BSV. The HDL may be quickly compiled, and .

%\chapter{Design and Implementation}
% NOTE: Main contribution to (the field?)
% NOTE: Original implementation
% NOTE: Somehow demonstrate - extra-curricular reading?!
% NOTE: Compare the design to BlueVec!?

% NOTE: Challenging goals - substantial deliverables
% NOTE: Design decisions

% RV32imafc - bare metal ELF
% Spike, proxy kernel?
% Cost of movement... memory wall
% MMAP - ARM & RiscV

\chapter{SoC and Testbench Setup: Design \& Implementation}
% TODO: Summarise contribution

\section{SoC Design}
% TODO: SoC Image/Picture
\subsection{Piccolo SoC}
Bluespec Verilog offers superior scheduling capabilities compared to ChiselHDL,
and standard Verilog. The creation of hardware that performs more complex tasks
is therefore made significantly easier. As such, we decide to use the Piccolo
processor from the Bluespec Inc. family within our simulation and testbench.

We choose the Piccolo processor over the Flute processor since we have no need
for virtual memory, therefore an MMU is superfluous. While the Toooba processor
would offer superior performance, due to its inherent complexity it would
consume far too much silicon space for the purpose of synthesis. We seek to
minimise our silicon area to open up the opportunity for the later task of
performing synthesis on an FPGA. This will lead to a lower LUT count, and give
us greater flexibility when designing and synthesising our own accelerator as we
have greater FPGA resources to make use of.

% TODO: Disadvantages of Piccolo(?)
% AXI4 bus configurability
% CPU 64/32-bit
% ISA extension customisability
% IMAFC --- ALSO D, M ... etc
% Will not boot a Linux Kernel, but FreeRTOS. Simple bare metal.
% Ignore context switching

\section{Risc-V Firmware}
\subsection{Risc-V Assembly \& Firmware Structure}
The firmware which runs on an embedded Risc-V core is significantly different to
software that runs on top of an operating system. In order to compile C and
assembly language into machine code, we make use of the GNU compiler collection
(GCC).

The naming convention for compilers is \texttt{arch-vendor-os-libc-gcc}. The
standard, desktop version of gcc \texttt{x86\_64-pc-linux-gnu-gcc} is not
capable of producing Risc-V machine code. Moreover, it is targetted at the linux
operating system, and uses the linux GNU LibC and supports linux system calls.
In order to compile our Risc-V assembly and C into Risc-V machine code, we must
use a cross-compiler. We build a compiler on our host x86\_64 system to run on
an x86\_64 machine, that is capable of producing a Risc-V ELF binary.

The \texttt{riscv64-unknown-elf-gcc} compiler is designed to produce a
bare-metal Risc-V binary. Instead of using GNU LibC, the C library used is
Newlib. Newlib is C library that is designed for embedded systems. With Newlib,
we now have access to the standard C functions for IO \texttt{printf}, and math
functions, such as \texttt{pow}, and \texttt{sqrt}. The relevant machine code
for these functions are statically linked, and the code within our final binary
ELF is entirely self-contained. This can be seen in the section layout of our
produced ELF file.

Due to the 32-bit configuration of the Piccolo processor that we are using, we
must restrict GCC to using 32-bit instructions, and 

Relevant sections of our final ELF file: We have a $64$~KB stack, and statically
linked functions from newlib.  
\tiny
\begin{verbatim}
Section .stack         : addr         800000f8 to addr         80010100; size 0x   10008 (= 65544) bytes
Section .text.pow       : addr         80013444 to addr         8001352e; size 0x      ea (= 234) bytes
Section .text.__ieee754_pow: addr         8001352e to addr         80013fee; size 0x     ac0 (= 2752) bytes
Section .text.__ieee754_sqrt: addr         80013fee to addr         800141b4; size 0x     1c6 (= 454) bytes
Section .text.memcpy    : addr         8001707c to addr         80017164; size 0x      e8 (= 232) bytes
\end{verbatim}
\normalsize

\subsubsection{UART/Serial Output}
% TODO: UART cite
The most important firmware debugging feature to get working is the ability to
print to console. The Piccolo techbench SoC kindly contains a NS16550 UART
peripheral that is connected to the AXI4 bus interconnect as a slave device.  

This UART peripheral is memory mapped, and we write to its internal registers
from within our firmware to obtain a console output. This is vital for debugging
purposes, as it allows us to extract and verify the computed results of our
accelerator.

Crucially, the UART peripheral provided by Bluespec Inc. is only suitable for
simulation purposes. Data written into its transmit register are printed
directly to console. 

% TODO: Cite Linker File guide
\subsubsection{Linker File}
Since we are compiling our C into a bare-metal ELF, we must specify the layout
of the machine code within the binary. This includes specifying the start
address of RAM, the region of memory where the stack is located, and where
static/initialised variables and sections of machine code are located within our
binary. This is so it is loaded into memory in an appropriate manner.

The main sections of the linker file are as follows: The \texttt{.vector}
section contains the instructions that are run after the processor is booted.
The \texttt{.stack} section is an empty region at the beginning of memory
where the \texttt{\_stack\_start} symbol is defined. The \texttt{.data} section
contains initialised data, that is typically accessed through global pointer
(\texttt{gp}) relative addressing. The \texttt{.bss} section contains
uninitialised data. Lastly, the \texttt{.text} section contains our machine
code.

We intentionally place the stack below our \texttt{.text} code section for security
purposes. The stack grows downwards, and as a result it cannot write into our
code section in the situation that a stack overflow occurs.

\scriptsize
\begin{verbatim}
OUTPUT_FORMAT("elf32-littleriscv")
OUTPUT_ARCH(riscv)
ENTRY(crtStart) /* Symbol for entry point */

MEMORY {
  RAM      (rwx): ORIGIN = 0x80000000, LENGTH = 0x10000000
}

_stack_size = 0x10000; /* 64KB stack = 0x10000 */

SECTIONS {
  .vector ORIGIN(RAM) : {
    *crt.o(.start_jump); /* ASM to run at boot */
  } > RAM

  .stack (NOLOAD) : {
    . = ALIGN(16);
    PROVIDE (_stack_end = .);
    . = . + _stack_size;
    . = ALIGN(16);
    PROVIDE (_stack_start = .);
  } > RAM

  .data : {
    *(.rdata) /* global static data */
    *(.rodata .rodata.*)
    *(.data .data.*) /* initialised data */
    . = ALIGN(8);
    PROVIDE( __global_pointer$ = . + 0x800 );
    *(.sdata .sdata.*) /* small initialised data */
    . = ALIGN(8);
    *(.srodata .srodata.*) /* read-only initialised data */
  } > RAM

  .bss (NOLOAD) : {
		. = ALIGN(4);
		_bss_start = .;
    *(.sbss*) /* small uninitialised data */
    *(.bss .bss.*) /* uninitialised data */
    *(COMMON) /* common symbols */
		. = ALIGN(4);
		_bss_end = .;
  } > RAM

  .text : {
    *(.text);
    /* *(.text.*); */ /* seperate sections for static linked NewLib functions */
  } > RAM
}
\end{verbatim}
\normalsize

One of the first problems encountered was a CPU trap due to a stack overflow.
The size of the stack may be modified in the linker file. The \texttt{embeddedartistry}
printf implementation that avoids using heap memory stores a considerable amount
of data on the stack. As such, we set the stack to $64$~KB, rather than the
$4$~KB used for testing purposes. A larger stack reduces the likelihood of a
stack overflow.

\subsubsection{LibC \& printf}
The standard printf function defined within the \texttt{stdio.h} header will
include the declaration of printf compiled within the NewLib library. It will
then be statically linked to our final binary, and have the code section
corresponding to the print function included within your final ELF file.

The standard printf function makes use of heap memory. Due to memory
constraints, the location of the heap is not specified within the linker file.
We therefore seek an implementation of the printf function that relies only on
the stack. 
% TODO: Cite https://github.com/embeddedartistry/printf

\subsubsection{crt.s --- RiscV Assembly}
The initial location of our program counter after it is moved from the boot rom
is \texttt{0x80000000}. This is the start address of our RAM. For
instruction-level control over CPU behaviour on boot, we place our own section
of assembly code at the beginning of RAM.

Of notable importance is setting the stack pointer \texttt{sp}, to ensure that
stack memory is allocated in the correct region of memory. We also set the
global pointer to allow global pointer relative addressing of variables in our
data section.


\lstset{language=[RISC-V]Assembler, style=customrv}
\begin{lstlisting}
.global crtStart
.global main

.section	.start_jump,"ax",@progbits
crtStart:
  lui  x2,      %hi(crtInit)
  addi x2, x2,  %lo(crtInit)
  jalr x1, x2   // jump to label crtInit
  nop

.section .text
crtInit:
  .option push
  .option norelax
  la gp, __global_pointer$ // Set global pointer for GP-relative addressing
  .option pop
  la sp, _stack_start // Set stack pointer to beginning of stack
  call main
infinitLoop:
  j infinitLoop

\end{lstlisting}
\lstset{}

\section{Testbench Setup}
\subsection{Debugging Piccolo: FPU}
\subsubsection{Fixing Piccolo FPU typeerrors}
% TODO: hyperlink: https://github.com/bluespec/Piccolo/tree/a4e34ef2f2ba4e82a95faa2bbe3dd832ca3c51a0 
The latest commit of the Piccolo processor, at the time of development, (a4e34ef), had
a type error in the floating point unit (FPU). 

The compiler experiences ambiguity between the RoundMode enumerations declared
within the BSV FloatingPoint library (\texttt{FloatingPoint.bsv}), and the
enumeration within \texttt{ISA\_Decls}. We hypothesise that a previous version
of the Bluespec Verilog compiler was capable of either resolving this ambiguity
by falling back to the library definition of the datatype, or prioritised the
library one to allow the processor to compile. Due to a lack of continuous
integration, or testing, this issue has not yet been fixed.
% TODO: Submit a pull request

% NOTE: Compile log
\begin{verbatim}
compiling Piccolo/src_Core/CPU/FPU.bsv
Warning: Unknown position: (S0080)
   1 warnings were suppressed.
Error: "Piccolo/src_Core/CPU/FPU.bsv", line 52, column 30: (T0080)
Type error at the use of the following function:
   mkFloatingPointDivider

The expected return type of the function:
   g__#(ClientServer::Server#(Tuple3#(FPU::FSingle, FPU::FSingle, ISA_Decls::RoundMode), FPU::FpuR))

The return type according to the use:
   c__#(ClientServer::Server#(Tuple3#(FloatingPoint::FloatingPoint#(d__, e__),
      FloatingPoint::FloatingPoint#(d__, e__),
      FloatingPoint::RoundMode),
      Tuple2#(FloatingPoint::FloatingPoint#(d__, e__), FloatingPoint::Exception)))
\end{verbatim}

In order to successfully compile the Piccolo processor with floating point
support, we must eliminate the ambiguity manually by specifying which package,
or library the datatype is declared within. We enter, \texttt{FPU.bsv} and prefix the
RoundMode type with \texttt{FloatingPoint::RoundMode} when it is used within the
arguments of a floating point library module. Additionally, we must enter
\texttt{FPU\_Core.bsv} and modify the function that converts between both
enumerations. The \texttt{fv\_getRoundMode} function must be modified to return
\texttt{FloatingPoint::RoundMode}.

After these changes, we observe that the Piccolo processor with the
floating point Risc-V ISA extension successfully compiles into Verilog RTL from
Bluespec Verilog. We may then simulate the processor performing floating point
precision mathematics using our verilator test bench.

\subsubsection{mstatus CSR}
Once the FPU is successfully compiled and the processor is compiled with
floating point support, the processor will trap and throw an exception when any
floating point instruction is executed. When a processor exception takes place,
the processor moves the program counter back to the start of the boot ROM, this
is observed in the simulation output as an infinite loop, where all instructions
after the first floating point instruction will never execute, as the processor
will constantly trap and reset itself, perpetually restarting the program.

With an open source processor, we have the ability to inspect and modify the
source code, and recompile the processor. We can instruct the CPU to print its
internal state when a trap occurs, notably: information about the trap, and the
type of exception that has been thrown. With this information, we can look
through the codebase of the Piccolo processor, and identify what is causing the
processor to fail to execute floating point instructions, despite the fact that
the FPU module is included within the processor.

We enter \texttt{CPU.bsv}, and modify the rule responsible for modifying the
behaviour of the CPU when the trap condition is met. We display additional
information about the trap, and instruct the simulation to end when a trap
occurs. This is where BluespecVerilog is incredibly useful. The
\texttt{Trap\_Info} structure within BSV contains the \texttt{FShow} typeclass.
This contains information for printing the structure in a clean and easy-to-read
way, specifically for debugging purposes. Additionally, the
\texttt{fshow\_trap\_Exc\_Code} function will print the human-readable name of
the exception code enumeration. As such, we do not need to constantly switch
between looking at our program output, and figuring out what the output
represents in our source code files.

\begin{verbatim}
// CPU.bsv
   rule rl_trap ((rg_state == CPU_TRAP)
     && (stage1.out.ostatus != OSTATUS_BUSY));

      [...]

      $display("DEBUG: CPU TRAP: ", fshow(rg_trap_info));
      $display("Exc_Code: ", fshow_trap_Exc_Code(exc_code));
      $finish(0);
   endrule: rl_trap
\end{verbatim}

The information we obtain about our processor trap can be matched to the
specific address in assembly code that causes the error. In this particular
case, the \texttt{flw} instruction is responsible for triggering an
\texttt{ILLEGAL\_INSTRUCTION} exception.
\begin{lstlisting}[language=C,style=customc],
  float a = 1.0f;
  // Assembly:
  // 80002276: lui a5, 0x80001
  // 8000227a: flw fa5,424(a5)
  // 8000227e: fsw fa5,-28(s0)
  // Prints:
  // Trap_Info { epc: 'h8000227a, exc_code: 'h2, tval: 'h1a87a787 }
  // Exc_Code: ILLEGAL_INSTRUCTION
\end{lstlisting}

We look through our processor code, and identify the locations where this
particular exception code is set. We observe that it is set in the
\texttt{alu\_outputs\_base} struct within \texttt{EX\_ALU\_functions.bsv}. This
struct is copied within the ALU function that corresponds to loading data into
registers, \texttt{fv\_LD}. If the \texttt{flw} instruction is called, a
particular bit on the \texttt{mstatus} control-status register (CSR) is read. In
our case, the fs bit is found to be disabled.
\begin{verbatim}
// EX_ALU_functions.bsv
function ALU_Outputs fv_LD (ALU_Inputs inputs);
   [...]
   // FP loads are not legal unless the MSTATUS.FS bit is set
   Bool legal_FP_LD = True;
`ifdef ISA_F
   if (opcode == op_LOAD_FP)
      legal_FP_LD = (fv_mstatus_fs (inputs.mstatus) != fs_xs_off);
`endif
   let alu_outputs = alu_outputs_base;

   alu_outputs.control   = ((legal_LD && legal_FP_LD) ? CONTROL_STRAIGHT
                                                      : CONTROL_TRAP);
   [...]
   return alu_outputs;
endfunction
\end{verbatim}

Compared to debugging standard Verilog, the structure of Bluespec Verilog
provides us with a greater ability to debug, and identify relevant codepaths.

We further observe that all FP ALU functions check the fs bit of the CSR register.
This can be seen from the ALU fv\_LD function that runs when the flw instruction
is executed. We later learn, that this is an intentional design descision as per
the Risc-V specification, Chapter 3, Section 1.11. The fs bit on the mstatus CSR must be
set in order for floating-point instructions to be enabled. We can see how this
is done in bare-metal C, by looking at the source code for the Spike Risc-V
proxy kernel, we implement the relevant parts of this code within our firmware
initialisation sequence. Once implemented, we observe that floating point
instructions operate correctly.
% https://github.com/riscv/riscv-pk/blob/master/machine/minit.c
% https://github.com/riscv/riscv-pk/blob/master/machine/encoding.h 
\begin{lstlisting}[language=C,style=customc]
// encoding.h
#define MSTATUS_FS          0x00006000
#define write_csr(reg, val) ({ \
  asm volatile ("csrw " #reg ", %0" :: "rK"(val)); })

// minit.c
static void mstatus_init()
{
  uintptr_t mstatus = 0;
  // Enable FPU
  if (supports_extension('F'))
    mstatus |= MSTATUS_FS;
  [...]
  write_csr(mstatus, mstatus);
  [...]
}
\end{lstlisting}

% TODO: Citation
% “The RISC-V Instruction Set Manual, Volume II: Privileged Architecture,
% Document Version 20190608-Priv-MSU-Ratified”, Editors Andrew Waterman and
% Krste Asanovi´c, RISC-V Foundation, June 2019
% https://riscv.org/wp-content/uploads/2017/05/riscv-privileged-v1.10.pdf
% TODO: Citation (Spike) + RiscV PK

\subsection{AXI4 Test Peripheral}
When building a peripheral, it is important to first work constructively from a
minimum functioning unit. In order for our embedded processor to communicate
with with our peripheral, we make use of memory-mapped I/O. We map a small
unoccupied memory address range to our peripheral, and give it a small internal
region of memory where 4-byte integers can be read/written from.

\subsubsection{What is AXI4?}
A microprocessor communicates with 

% TODO: Explain AXI4
The AXI4 protocol is designed by ARM, and is royalty-free, where a licence is
not required for academic or commercial use.

% https://forums.xilinx.com/t5/Design-and-Debug-Techniques-Blog/AXI-Basics-1-Introduction-to-AXI/ba-p/1053914 

% AXI4 RAM: https://github.com/alexforencich/verilog-axi/blob/master/rtl/axi_ram.v 

% 10 wire write address channel
% 5 wire write data channel
% 4 wire write response channel
% 10 wire read address channel
% 6 wire read data channel
% 35 wires in total, excluding clock & reset

Due to the inherent complexity of the AXI4 protocol, large combinatorial finite
state machines are required to ensure the read and write requests of the AXI4
bus are processed correctly.
The state of over a dozen wires needs to be considered by the programmer on
every clock cycle. Developing a peripheral in this manner in standard Verilog
RTL is a challenging feat.

Moreover, when a peripheral for the module is initialised, all 35 wires
corresponding to signals in the AXI4 protocol need to be connected to the AXI4
interconnect. All of these connections need to be manually specified. This can
be observed in Verilog RTL as follows:
\tiny
\begin{verbatim}
  mkUART uart0(.CLK(CLK),
         .RST_N(RST_N),
         .put_from_console_put(uart0$put_from_console_put),
         .set_addr_map_addr_base(uart0$set_addr_map_addr_base),
         .set_addr_map_addr_lim(uart0$set_addr_map_addr_lim),
         .slave_araddr(uart0$slave_araddr),
         .slave_arburst(uart0$slave_arburst),
         .slave_arcache(uart0$slave_arcache),
         .slave_arid(uart0$slave_arid),
         .slave_arlen(uart0$slave_arlen),
         .slave_arlock(uart0$slave_arlock),
         .slave_arprot(uart0$slave_arprot),
         .slave_arqos(uart0$slave_arqos),
         .slave_arregion(uart0$slave_arregion),
         .slave_arsize(uart0$slave_arsize),
         .slave_arvalid(uart0$slave_arvalid),
         .slave_awaddr(uart0$slave_awaddr),
         .slave_awburst(uart0$slave_awburst),
         .slave_awcache(uart0$slave_awcache),
         .slave_awid(uart0$slave_awid),
         .slave_awlen(uart0$slave_awlen),
         .slave_awlock(uart0$slave_awlock),
         .slave_awprot(uart0$slave_awprot),
         .slave_awqos(uart0$slave_awqos),
         .slave_awregion(uart0$slave_awregion),
         .slave_awsize(uart0$slave_awsize),
         .slave_awvalid(uart0$slave_awvalid),
         .slave_bready(uart0$slave_bready),
         .slave_rready(uart0$slave_rready),
         .slave_wdata(uart0$slave_wdata),
         .slave_wlast(uart0$slave_wlast),
         .slave_wstrb(uart0$slave_wstrb),
         .slave_wvalid(uart0$slave_wvalid),
         .EN_server_reset_request_put(uart0$EN_server_reset_request_put),
         .EN_server_reset_response_get(uart0$EN_server_reset_response_get),
         .EN_set_addr_map(uart0$EN_set_addr_map),
         .EN_get_to_console_get(uart0$EN_get_to_console_get),
         .EN_put_from_console_put(uart0$EN_put_from_console_put),
         .RDY_server_reset_request_put(uart0$RDY_server_reset_request_put),
         .RDY_server_reset_response_get(uart0$RDY_server_reset_response_get),
         .RDY_set_addr_map(),
         .slave_awready(uart0$slave_awready),
         .slave_wready(uart0$slave_wready),
         .slave_bvalid(uart0$slave_bvalid),
         .slave_bid(uart0$slave_bid),
         .slave_bresp(uart0$slave_bresp),
         .slave_arready(uart0$slave_arready),
         .slave_rvalid(uart0$slave_rvalid),
         .slave_rid(uart0$slave_rid),
         .slave_rdata(uart0$slave_rdata),
         .slave_rresp(uart0$slave_rresp),
         .slave_rlast(uart0$slave_rlast),
         .get_to_console_get(uart0$get_to_console_get),
         .RDY_get_to_console_get(uart0$RDY_get_to_console_get),
         .RDY_put_from_console_put(uart0$RDY_put_from_console_put),
         .intr(uart0$intr));
\end{verbatim}
\normalsize

Bluespec Verilog offers a significant degree of abstraction when processing AXI4
read and write requests, and also when attaching the peripheral to the AXI4 bus interconnect.
% TODO: Include code!


% TODO: Include picture of Vivado SoC
Proprietary CAD tools, such as Xilinx Vivado, typically have a built-in block
design tool. The built in design tool allows the developer to see the SoC as a
whole, and visually connect components together. These design tools generate the
necessary Verilog HDL representing the SoC and the necessary connections between
the module components.

Our SoC can be created by creating the necessary IP blocks for our accelerator,
and processor, and making use of the proprietary AXI interconnect, UART module
and block memory controller. The block design tools created by Xilinx and Altera
are a solution to the mess of specifying wires and connections when writing
standard Verilog HDL in a modular fashion. The design may be then simulated
directly using the xsim command within Xilinx Vivado.
% TODO: talk about why we avoid this (proprietary IP bugs)

In contrast, our testbench SoC written in Bluespec Verilog has no graphical
configuration. The entire network consisting of the processor core, peripherals
and AXI4 interconnect fabric are all written in BSV, and the modules are
connected together manually. This process is significantly simpler than with
Verilog RTL, as the overloaded module/typeclass \texttt{mkConnection} can be used to
forward arbitrary structures of data between modules of particular interfaces.
An instance of \texttt{mkConnection} exists for the AXI4 master and slave
interfaces that are also implemented in BSV themselves. The AXI4 fabric module
is the interconnect, and is parametised by the number of master and slave
interfaces it should have. Each instance of \texttt{mkConnection} is a module
that forwards data between all 35 AXI4 wires. As shown below:
% TODO: Add SoCMap/SoCTop
\scriptsize
\begin{verbatim}
   Fabric_AXI4_IFC  fabric <- mkFabric_AXI4;
   Core_IFC #(N_External_Interrupt_Sources)  core <- mkCore;
   UART_IFC   uart0  <- mkUART;
   Test_IFC   test <- mkTest;
   mkConnection (core.cpu_imem_master,  fabric.v_from_masters [imem_master_num]); // instruction memory
   mkConnection (core.cpu_dmem_master,  fabric.v_from_masters [dmem_master_num]); // data memory
   mkConnection (fabric.v_to_slaves [uart0_slave_num],  uart0.slave); // UART0 serial peripheral
   mkConnection (fabric.v_to_slaves [test_slave_num],  test.slave); // Test peripheral
   // Connect to BlockRAM
\end{verbatim}
\normalsize

% TODO: Perhaps include example AXI4 r/w

\subsubsection{AXI4 Alignment/Padding}
Due to the structure of the AXI4 bus and the way a processor writes to memory,
data stored in memory must be aligned depending on its size.
A 32-bit AXI4 bus may only access at most a single aligned
4-byte block per read/write request. As a consequence, it is possible to fail
writing/reading from memory when crossing the block boundary.

% TODO: Include image
For example, writing to an unaligned 4-byte region at the address $0x2-0x6$ crosses
the $0x4$ boundary:
\begin{verbatim}
*((uint32_t*)0xC0001002) = 0x1337;
Trap_Info { epc: 'h800022d6, exc_code: 'h6, tval: 'hc0001002 }
TRAP EXC: STORE_AMO_ADDR_MISALIGNED
*((uint32_t*)0xC0001000) = 0x1337;
\end{verbatim}

This is vital to keep in mind throughout writing to peripheral memory. An
integer must be aligned to 4 bytes, a short to 2 bytes, and a char to the
nearest byte. 

\subsubsection{Struct Padding}
% TODO: Introduce the need for struct padding/packing
This is of particular importance to consider later when we issue commands to the
accelerator. By default, a C struct is padded and aligned to conform to the
alignment of memory access. When a struct is written to memory, it always begins
at the nearest 4 byte boundary. All datatypes within a struct ar aligned to the
nearest 4, For instance:

\begin{verbatim}
struct paddedstuff_t {
    uint16_t A; // short (2B) --> (4B) PADDED to ALIGN to nearest 4 bytes
    uint32_t B; // int (4B)
}; // 8 BYTES
struct packedstuff_t {
    uint16_t A; // short (2B)
    uint32_t B; // int (4B) ---> Will write at base+0x2
} __attribute((packed)); // 6 BYTES
\end{verbatim}


\chapter{SoC \& Testbench Setup: Evaluation}
\section{Clock rate within simulation}
\subsection{Methodology}
% TODO: Why was Piccolo chosen(?) --- backtrack
The Piccolo processor was chosen due to its relative simplicity. With a simple
3-stage in-order pipeline, the generated C++ should be relatively simple
compared to the 5-stage pipeline in the Flute processor, and the out-of-order
superscalar architecture of the TOOOBA processor. As a consequence, we would
therefore maximise the likelihood that we obtain a reasonable clock rate.

% TODO: Include modified testbench code. std::chrono
% TODO: How to measure clock rate!?
We modify the testbench program to output the clock rate when the testbench is
receives an interrupt signal, or when the simulation ends. 

\subsection{Results}
On interrupting our program we measure a clock rate of $22406$~Hz. This is impressive for a simulation.
% TODO: Evaluate properly

\section{Synthesis}
% TODO: Include Vivado Studio
% TODO: SoC Map
% TODO: MML file --- flash BlockRAM
% TODO: P&R Diagram - highlighted!

\section{Simulating an AXI4 Peripheral}
\subsection{Methodology}
We make use of a verilator testbench to simulate our SoC. We write some simple
test firmware to write into the peripheral, and trigger its execution.

When a special execution bit is set, we set a special accelerator busy bit and
unset the execution bit. In a single cycle, within the accelerator, if the
busy bit is set, we double the array and unset the busy bit. This allows the
firmware to stall until the busy bit is unset, and the computation has been
completed.

% NOTE: Firmware C code for accelerator
\begin{lstlisting}[language=C,style=customc]
#define TEST_CONTROL_ADDR 0xC0001000UL
#define TEST_DATA_ADDR 0xC0001008UL
#define TEST_RANGE 4
void main() {
  for(int i=0; i<TEST_RANGE; i++) {
    *((uint32_t volatile*)TEST_DATA_ADDR+i) = i;
    uint32_t data = *((uint32_t*)TEST_DATA_ADDR+i);
    printf("%d: %d\n", i, data);
  }

  println("Set Control Bit..");
  *((uint8_t volatile*)TEST_CONTROL_ADDR) |= 1; // Set execute!

  while(*((uint8_t volatile*)TEST_CONTROL_ADDR) & 2 == 0) {}
  println("Stalling.."); // Printing Stalling... stalls anyway...

  println("Stall complete!");
  for(int i=0; i<TEST_RANGE; i++) {
    uint32_t data = *((uint32_t volatile*)TEST_DATA_ADDR+i);
    printf("%d: %d\n", i, data);
  }
}
\end{lstlisting}


\subsection{Results}
% NOTE: Console log
\begin{verbatim}
0: 0
1: 1
2: 2
3: 3
Set Control Bit..
Stalling..
Stall complete!
0: 0
1: 2
2: 4
3: 6
\end{verbatim}

Now that we have demonstrated a functioning testbench, and 
% TODO: Compare to proprietary SoC?!



\chapter{Matrix Multiplication Peripheral: Design \& Implementation}
\section{Overview}
\subsection{Background}
Mathematically, the result of a matrix multiplication may be expressed
accordingly:
\begin{align*}
  C &= AB \\
  C_{ij} &= \sum_k A_{ik}B_{kj}
\end{align*}

Matrix multiplication may be expressed in a programmatic form as such:
\texttt{C[i][j] = sum zipWith (op*) (row A i) (col B j)}

When matrices are stored in memory, they can only be indexed with a single
dimension. A multi-dimensional matrix is represented as a multi-dimensional
array, this array is represented in memory in as a flattened contiguous
one-dimensional array, with the rows of the matrix next to each other in memory.

Consequently we access the element on the xth row, and yth column of a matrix as
such:
$$ \text{IndexOf} A[x][y] = x + y * width(A) $$

This methodology of accessing the x, and y elements of a stored matrix is used
throughout the accelerator.

Block RAM (BRAM) is a contiguous region of memory that exists within the FPGA. In
Bluespec Verilog, block RAM is created through the use of something known as a
Register File (\texttt{RegFile}). Crucially, this is distinct from the Bluespec
array-access methodology of \texttt{Vector\#(N, Reg\#(type))}. A vector of
registers are seperate registers, which need not necessary be contiguous in
hardware, that can be array-accessed in a contiguous fashion within Bluespec.

Our peripheral contains a RegFile of internal BRAM which is memory mapped to
the CPU via an AXI4 bus slave interface. 
% RegFile vs Vector#(Reg#()) vs BRAM

\subsection{Design}
\subsubsection{Hybrid/Monolithic}
% TODO: Citations - argue around, base ideas off.
We propose two potential ways of implementing matrix multiplication. We could
pursue a monolithic approach and implement the entire matrix multiplication
operation purely in hardware. Alternatively, we could perform a hybrid approach,
where each element of the matrix \texttt{C[i][j]} may be computed by issuing a
separate command to the accelerator. This is performed under the assumption that the
creation and scheduling of instructions for each element in the final matrix, is
more easily performed within the firmware of the embedded processor, without a
significant perfomance penalty.

\subsubsection{Memory Regions}
We split our accelerator memory into seperate memory-mapped regions. We
interface with our accelerator through the AXI4 bus by writing and reading from
memory appropriately.  
\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    Offset Address Range & Size & Purpose \\
    \midrule
    \texttt{0x0000-0x0020} & 32 Bytes & Status \\
    \texttt{0x0020-0x0040} & 32 Bytes & Control \\
    \texttt{0x0040-0x1000} & $\approx$ 4 KB & Memory \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Status}
The first bit of our status register in memory exists to monitor the execution
of the accelerator. In the situation that all functional units are occupied, a
command that is issued to the accelerator will be ignored. It is therefore
important to keep track of the internals of the acclerator and make sure that it
is not at maximum utilisation when commands are issued. Within our firmware, we
must check that there is at least one free functional unit before issuing a
command, or else our commands will drop and the computation of our final matrix
will not be correctly performed.

\subsubsection{Control}
We write our control command/instruction to the control register. This is
discussed in greater detail in the following section. Once our command is
written, it is executed by setting the execute status bit within the control
register. The accelerator should then dispatch the appropriate command to one of
its functional units if it is free.

\subsubsection{Memory}
Like the test peripheral, our data memory consists of memory mapped
\texttt{RegFile} to make use of FPGA BRAM cells. In the later section, we modify
the structure of the memory to support multiple functional units.

\section{Command Issuing \& Decoding}
\subsection{Format}
In the hybrid model of our accelerator, and individual command must be able to
compute an individual element of our final matrix, C. As discussed earlier,
matrices are stored linearly in memory. 

\scriptsize
\begin{verbatim}
typedef struct {
   LittleEndian#(Bit#(32)) addr;
   LittleEndian#(Bit#(8)) offset;
   LittleEndian#(Bit#(8)) stride;
   } MatUnitPtr deriving (Eq, Bits, FShow);

typedef struct {
   LittleEndian#(Bit#(8)) unit;
   LittleEndian#(Bit#(8)) count;
   MatUnitPtr ptr_a;
   MatUnitPtr ptr_b;
   MatUnitPtr ptr_c;
} MatUnitArgs deriving (Eq, Bits, FShow);

\end{verbatim}
\normalsize

\subsection{Decoding}
One of the principal problems encountered between issuing a command containing
a struct of multi-byte data to the accelerator from the embedded processor. By
default, the unsigned integer type in BSV is in big endian form, while the
embedded core stores data in little endian form. The motivation for embedded
processors to use little endian byte ordering is to ensure that the address of a
variable is unchanged when a typecast is performed. This saves processor cycles,
and results in improved performance. Variables are automatically
truncated/extended when an appropriate typecast is made. \\

We write a C struct into our control region of memory. In order for the data
within the instructions to be interpreted correctly in hardware, the
byte-ordering needs to be reversed. BSV provides us with a systematic and clean
way of doing this within hardware. The alternative would be to reverse the bytes
in software, and therefore lose cycles. \\

% TODO: Link to:
% https://github.com/jeffreycassidy/BlueLink/blob/master/Core/Endianness.bsv 

% TODO: C padding - citations
Naturally, C structures are padded and aligned, where N-byte variables are
aligned to the nearest N-byte block. Variables in a struct are extended in size
in accordance with this padding. A structure in BSV is unpadded, and as such, it
is vital to ensure that the structure has no padding within our C code for when
it is written to the peripheral.

\section{Hardware floating point operations}
\subsection{Fixed/Floating Point}
We have a decision to implement calculations with rational numbers using either
fixed-point, or floating-point.

% TODO: Why FP over FixedP 

GCC has built-in support for fixed-point arithmetic.

\subsection{Bluespec Verilog FloatingPoint}
% TODO: Introduce/explain floating-points in BSV
BSV contains a FloatingPoint library. Modules already exist for addition and
multiplication which are polymorphic, extending the Server interface. The most
simple is the floating point adder module. The Server interface in Bluespec
makes use of an internal Put interface for a request, and a Get interface for a
response.

The floating point modules are paremetised by the precision of our desired floating
point format. Naturally, we use the IEEE754 32-bit floating point standard, with a 1-bit sign,
23-bit significand and 8-bit exponent. We therefore make use of our own
single-precision floating-point datatype \texttt{FSingle}. For the sake of
convenience, we also define the datatype for the associated response, containing
a two element tuple of our result, and an exception.

By making use of the server interface, we can we can put a request containing a
3-element tuple of our operands and the appropriate floating point rounding mode
to our floating point adder. After 6 clock cycles, we obtain a response. A test
implementation of this is shown below: \\

\tiny
\begin{verbatim}
import GetPut :: *;
import ClientServer :: *;
import FloatingPoint :: *;

typedef FloatingPoint#(8,23) FSingle;
typedef Tuple2#( FSingle, FloatingPoint::Exception ) FpuR;

module mkFPAddTest(Empty);
   Reg#(UInt#(8)) cycle <- mkReg(0);
   rule rl_cycle(cycle < 255);
      cycle <= cycle + 1;
   endrule

   Server# (Tuple3# (FSingle, FSingle, RoundMode)
            , FpuR ) fpu_add <- mkFloatingPointAdder;
   rule rl_start(cycle == 0);
      FSingle opd1 = 1.0;
      FSingle opd2 = -2.0;
      fpu_add.request.put(tuple3(opd1, opd2, defaultValue));
      $display("%2d: Start", $time);
   endrule

   rule rl_end;
      match { .res, .exc } <- fpu_add.response.get();
      $display("%2d: Result: %h", $time, pack(res)); 
      $finish(0);
   endrule
endmodule

// OUTPUT:
// 10: Start
// 70: Result: bf800000 (-1.0)
\end{verbatim}
\normalsize

% TODO: Pipeline --- diagram
We propose that the pipelined nature of the floating point units offered by the
Bluespec FloatingPoint library should offer a significant speedup. Provided that
the pipeline of the unit is fully occupied, it is possible to retrieve the result
of a floating point operation in every cycle. This feature will later be
exploited to greatly reduce the number of compute cycles within an individual
functional unit.

% TODO: Move to evaluation (?)
In order to simulate our simple module, we make use of Bluesim. Our Bluespec
Verilog is compiled directly into an executable, with no need to write a
testbench program in C++ as is required by Verilator. 

\section{Functional Unit Design}
The base compute unit that executes an issued command is known as a functional
unit. In the context of matrix multiplication, a single functional unit is
responsible for calculating a single element in the final matrix $C = AB$, by
calculating the dot product between the ith row of A, and the jth column of B,
as follows: $C_{ij} = A_{ik}B_{kj} = \text{row}(A,i) \cdot \text{col}(B,j)$.
This involves adding the pairwise result of multiplications to an accumulator.

\subsection{Fused Multiply-Accumulate (FMA)}
We make use of a floating-point fused multiply-accumulate (FMA) module. These are used
for addition and multiplication within the FPU of a processor. This allows the
floating-point operation $(a*b)+c$ to be performed with only a single rounding
step, thus saving compute cycles.

An FMA unit saves cycles compared to an individual floating-point multiplier and
adder circuit, as it only performs the floating-point rounding procedure once
both the multiplication and addition procedures have completed.

The simplest functional unit consisting of a fused-multiply accumulate circuit
should take $11$~cycles per element for an $N$~element vector.

% Cannot exploit pipelined nature of unit.
% Faster than individual FP multiply, then add. 10 cycles vs 14 cycles.

% TODO: move to eval
% 10: FSM Start
% 20: Init
% 40: PUT: 00000000 40800000 40800000
% 40: STATE_LOCK
% 140: 0: MulAcc Result: 41800000
% 140: STATE_READY
% 150: PUT: 41800000 40800000 40800000
% 150: STATE_LOCK
% 250: 1: MulAcc Result: 42000000
% 250: STATE_READY
% 260: PUT: 42000000 40800000 40800000
% 260: STATE_LOCK
% 360: 2: MulAcc Result: 42400000
% 360: STATE_READY
% 370: PUT: 42400000 40800000 40800000
% 370: STATE_LOCK
% 470: 3: MulAcc Result: 42800000
% 470: STATE_COMPLETE
% 480: Output: 42800000


% TODO: FMA diagram
% TODO: Explain how FMA works and advantages...
% TODO: Explain design decisions

\subsection{Pipelined Multiplication with Addition (PMA)}
We observe that the pipeline is not at maximum occupancy using an FMA unit. This
is because we need to feed the accumulator result back into the multiplier when
computing the accumulated product of the next two elements. As a consequence, we
waste $10N$~cycles throughout the computation for computing each element due to
the fact that the pipelined nature of FP hardware is not fuly utilised.

We observe that by having an internal buffer of memory within our functional
unit, we can pipeline our floating point multiplications. By subsequently
accumulating our result using a floating point adder, we observe that addition
will take $6N$ cycles, while the pipelined multiplication step will take $11+N$
cycles.

\begin{align*}
  T &= (11 + N) + 6N \\
  T &= 11 + 7N
\end{align*}
\subsection{Pipelined Multiplication \& Pairwise Addition (PMPA)}
In an attempt to maximise the occupancy of the addition pipeline, we propose a
more hardware-intensive strategy that makes use of pipelining for both the
multiplication and accumulation steps.

As before, in our first stage, we perform pipelined multiplication of our
elements into a buffer. As before, this will take $11+N$ cycles

We then perform reductive pairwise addition of the elements in the buffer.
The pairwise addition procedure has the advantage that we may fully occupy the
FP Adder pipeline. We then perform pipelined addition of pairs of elements.
It should take $\log_2{N}$ reductive steps, where each step consumes
$6+\frac{N_{i}}{2}$ cycles.

\begin{align*}
  T &= 6\log_2{N} + \left(N + \frac{N}{2} + \frac{N}{4} + ... + 1\right) + \text{const.}\\
  T &= 6\log_2{N} + 2N + \text{const.}
\end{align*}

The recursive algorithm only works cleanly when the size of our buffer is a
power of two. A naive implementation would therefore waste processing cycles
performing pairwise additions of the elements for the entire buffer size, rather
than the size of our input vectors. Instead, after each step, we guarantee that
our buffer is multiple of two, and collect together an array of remainder
elements. The power of this algorithm can be understood through binary
representations.

\begin{align*}
  N_i = 255_{10} = 11111111_{2}
\end{align*}

After each pass, we extract a remainer if it exists, then bitshift $N_i$. At
most, the binary representation of the number of elements in our input vector is
the number of binary digits of the size of our buffer $B$. Therefore, there will
be a maximum of $\log_{2}{B}$ remainder values to place in a seperate buffer and
accumulate. The accumulation process will therefore take $6\log_{2}{B}$ cycles.

Therefore, the total cycle consumption is:
\begin{align*}
  T &= (N) + (6\log_2{N} + 2N) + (6\log_2{B}) + \text{const.} \\
  T &= 3N + 6(\log_2{N} + \log_2{B})
\end{align*}

In this way, we've managed to very substantially reduce the linear constant
factor through the use of pipelining. This is where Bluespec Verilog shines,
scheduling complex pipelining behaviour through the extensive use of FIFOs, and
avoiding hardware race conditions is an incredibly challenging feat. It would be
an excruciatingly difficult process to write this in standard Verilog HDL. 

\subsection{Summary of Predicted Performance}
% Unit Design: TODO: Include diagrams
We observe that through further pipelining optimisations we substantially reduce
the amount of cycles required to calculate a matrix element using an individual
functional unit. Generally speaking, by using a more complicated functional unit
design, we can obtain a lower linear coefficient with the cost of a greater
constant term.

\begin{align*}
  T_{\text{FMA}}(N) &= 11N \\
  T_{\text{PMA}}(N) &= 7N + \text{const.}  \\
  T_{\text{PMPA}}(N) &= 3N + 6(\log_2{N} + \log_2{B}) + \text{const.}
\end{align*}

\subsection{Multiple Functional Units \& Multi-ported memory}
We observe that the rate at which we issue instructions to the accelerator
exceeds the compute speed of the functional unit. Crucially, our firmware stalls
its execution and waits for the accelerator to complete its task before issuing
a new instruction. Thus, we are compute-bound.

We seek to exploit the parallelism offered in hardware design by duplicating our
functional units, sequentially issuing commands to different functional units
without stalling our processor's execution. By keeping our processor occupied,
we maximise throughput. With multiple functional units executing their
instructions simultaneously, we should be able to obtain a compute speed
improvement of a multiple of the number of functional units we have.

A standard BRAM RegFile in BSV has 2 read ports and 1 write port. As such, in a
single clock cycle it is only possible to read from two random access locations,
and write to one location of a RegFile. Thus, we must modify the architecture of
our memory to support multiple functional units.

\scriptsize
\begin{verbatim}
 Error: "MultiPortBRAM.bsv", line 18, column 37: (G0002)
 `pmem_mem_0.sub' needs more than 5 ports for the following uses:
 `pmem_mem_0.sub a__h30554' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h31995' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32028' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32061' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32094' at "MultiPortBRAM.bsv", line 18, column 37
 `pmem_mem_0.sub a__h32127' at "MultiPortBRAM.bsv", line 18, column 37
\end{verbatim}
\normalsize
We observe that the standard BSV RegFile, has a maximum of 5 read ports, and 1
write port. We exploit the ability to parameterise our modules in BSV, and
create our own custom MultiPortedMemory module. Our vector of RegFiles will
allocate seperate RegFiles at different locations on the FPGA fabric. They may
be then accessed in parallel.

The BSV interface for our MultiPortBRAM module is similar to that of the RegFile
module. However, the type has an extra parameter, $n$, representing the number of memory
channels. The multi-ported memory has only one write port, but has $2n$ read
ports. This is because it contains $n$ channels of RegFile BRAM and can
distribute its reads to $n-1$ functional units and the AXI bus within a single
clock cycle appropriately.

\scriptsize
\begin{verbatim}
interface MultiPortBRAM#(type addr, type data, numeric type n);
   method Action upd(addr a, data x);
   method ActionValue#(data) sub(Bit#(8) c, addr a);
endinterface
module mkMultiPortBRAM(MultiPortBRAM#(addr, data, n))
provisos(
   Bits#(addr, addr_sz),
   Bits#(data, data_sz),
   Bounded#(addr)
   );
   // Create N channels of memory
   Vector#(n, RegFile#(addr, data)) mem <- replicateM(mkRegFileWCF(minBound, maxBound));
   method Action upd(addr a, data x);
      for(Integer i=0; i<valueOf(n); i=i+1)
         mem[i].upd(a, x); // Update ALL internal memory channels
   endmethod
   // Read from the channel with index 'c'.
   method ActionValue#(data) sub(Bit#(8) c, addr a);
      return mem[c].sub(a);
   endmethod
endmodule
endpackage
\end{verbatim}
\normalsize

When we write to our accelerator memory through the AXI4 bus or from one of our
functional units, we write to all memory channels. A write operation can only be
performed at one random access memory address per cycle.

Our new multi-ported memory is read at a different channel by each different
functional unit. As such, we are capable of reading memory at the maximum rate
possible. Our individual functional units do not use too many RegFile read ports.

% TODO: Discuss somewhere else?
% This leads to one of the key development problems experienced, rule conflicts.
% By default, all rules with a true predicate execute on each cycle unless there
% is a conflict between them.


\chapter{Matrix Multiplication Peripheral: Evaluation}
\section{Methodology}
\subsection{Benchmarking \& Counting cycles}
From the Risc-V specification, we take a look at the mcycles and minstret
registers. We observe that it is possible to read from CSRs by running the
appropriate instructions within Risc-V assembly. We therefore make use of inline
assembly. The Risc-V specification highlights the \texttt{mcycles} CSR, that is
responsible for storing the number of cycles that the processor has performed.
This can be accessed from our firmware, and used to provide an estimate of the
runtime of one of our algorithms. Crucially, it is an overestimate and there is
some inherent overhead of cycles taken to read from the CSR itself, this
cycle overhead itself can differ depending on whether the stack is pre-allocated
or not when the instruction is ran or other factors dependent on how the
compiler decides to optimise the C firmware when it is compiled into Risc-V
instructions within machine code.

\begin{lstlisting}[language=C,style=customc]
#define read_csr_safe(reg) ({ register long __tmp asm("a0");  \
      asm volatile ("csrr %0, " #reg : "=r"(__tmp));          \
      __tmp; })
void dummy_function() {
  uint32_t time0, time1;
  time0 = read_csr_safe(cycle);
  // [...]
  time1 = read_csr_safe(cycle);
  printf("Run cycles: %d\n", time1-time0);
}

\end{lstlisting}


\section{Results}
\subsection{Functional Unit Type}
We seek to benchmark the execution time of the individual functional unit types.
Clearly, for a large $N$ with large matrices, our pipelined multiplication \&
pairwise addition unit is the most cycle efficient. Yet it is vital to consider
the cases where the constant offset plays a role. This is particularly relevant
for 'flatter' matrices, where the size of our input vectors entering our
functional units are smaller.

% TODO: Evaluation: for different FU types...


\subsection{Functional Unit Parallelism}
We make use of the fused multiply-accumulate functional unit for benchmarking
purposes. This ensures that our individual functional units are saturated, and
that our computation bottleneck is instead the number of functional units that
are enabled.

% TODO: Evaluation: put in unbuffered results
\begin{table}
  \centering
  \begin{tabular}{r|rr}
    \toprule
    Units & Peripheral & Processor \\
    \midrule
    0  &     ? & 21500 \\
    1  & 13366 & 13596 \\
    2  &  6817 &  7085 \\
    4  &  3670 &  3954 \\
    8  &  2621 &  2889 \\
    12 &  2666 &  2929 \\
    16 &  2633 &  2916 \\
    \bottomrule
  \end{tabular}
  \caption{Overhead: Buffer cycles: 723. Copy cycles: 4984}
\end{table}


\subsection{Synthesis}
\subsubsection{Memory Striping}
One of the big problems with our accelerator is the design of the multi-ported
memory. By duplicating our memory writes across $n$ channels for $n-1$
functional units, we occupy a very significant amount of BlockRAM on our
accelerator.

% TODO: Synthesis
% The process of synthesis becomes a very delicate balance between...


Given more time, we could instead stripe our memory into separate banks, with a
low probability that more than two indexes are read from the same bank in the
same cycle. While this is unlikely due to the nature of memory accesses for matrix
multiplication, this can be made easier by also duplicating the striped memory
banks as shown previously.

% TODO: Analysis: Math to show memory consumption.

One of the most important considerations when implementing this form of striped
memory is how to apply the associated back-pressure when two functional units
are reading from the same memory bank in the same channel. One of the units
would need to be stalled. It is likely that the processing and scheduling logic
for this would lead to wasted cycles, and therefore a reduction in compute
speed. It would be an interesting exercise to see how this reduction in compute
speed compares to the savings in BlockRAM.


% TODO: Alternative proposition --- bank/striped memory
% TODO: Put in further work
% Difficult to apply appropriate backpressure, --- perhaps look into further (Conclusion)


\chapter{Commmunication \& Memory Bottleneck}
From our earlier results, we observe that at some threshold, a greater number of
functional units does not necessarily translate into a greater accelerator
throughput. 

\section{Design \& Implementation}
\subsection{Protocol efficiency/optimisation}
One of the most obvious ways of improving the communication rate is to.

% Analysis of struct packing
% Attribute ((packed))
% uint8_t offset <-- redundant (-3 bytes)
% uint8_t unit <-- push to accelerator

% uintptr_t <-- Make ADDR 16 bits!?, subtract base address (64KB accel memory...) ()
% (-3*2=-6 bytes)
% 0xC0002900 --> 32 bits
% 2900 --> 16 bits
% 1000 (16-bit base address)
% 1900
% 6 bytes ~=? 6 cycles saved
% 2-4 cycles/base address subtraction
% ALIGN: ACCEL @ 16-bit base address boundary
% Therefore 0-cost LE typecast <---> address calculation! (SMART)


\subsection{Firmware Command Buffering}
To address the issue of not being able to issue commands fast enough, we
pre-generate all of our commands in the stack such that we don't waste cycles
generating commands between issuing them.

% TODO: Include firmware
% Stack Instruction Buffering

% TODO: conditions
\begin{table}
  \centering
  \begin{tabular}{r|rr}
    \toprule
    Units & Peripheral & Processor\\
    \midrule
    0  &     ? & 21500 \\
    1  & 13366 & 13596 \\
    2  &  6817 &  7085 \\
    4  &  3670 &  3954 \\
    8  &  2621 &  2889 \\
    12 &  2666 &  2929 \\
    16 &  2633 &  2916 \\
    \bottomrule
  \end{tabular}
  \caption{Software Buffering: Overhead: Buffer cycles: 723. Copy cycles: 4984}
\end{table}
% TODO: Evaluation of results: ~10x faster
% TODO: Explanation of results

% TODO: Jesus...

% TODO: Accelerator buffer/program counter. (Internal stack)

\subsection{Firmware Stalling/Interrupts}
One of the potential criticisms of the technique of constantly polling a region
of memory is that

% TODO: Cite this
On an ARM system, the hardware interrupt latency is $\approx 12$ cycles. On a
Risc-V system, interrupt behavior is exceedingly complicated. A Risc-V processor
has, by default, a memory-mapped peripheral known as a Platform Level Interrupt
Controller (PLIC). The PLIC is responsible for multiplexing multiple interrupt
requests of different priorities. While the PLIC registers interrupts and stores
the necessary information for prioritising them, it is the role of the firmware
to actually perform an associated task when an interrupt occurs.


% TODO: Cite: https://sifive.cdn.prismic.io/sifive/0d163928-2128-42be-a75a-464df65e04e0_sifive-interrupt-cookbook.pdf 
From the SiFive interrupt cookbook, we see:
\begin{lstlisting}[language=C,style=customc]
void machine_external_interrupt()
{
   //get the highest priority pending PLIC interrupt
   uint32_t int_num = plic.claim_comlete;
   //branch to handler
   plic_handler[int_num]();
   //complete interrupt by writing interrupt number back to PLIC
   plic.claim_complete = int_num;
}
\end{lstlisting}

Crucially, reading the highest priority interrupt from the PLIC is a memory
read operation via the AXI4 bus and has a latency equal to that of polling our
accelerator memory. There is then the additional overhead of marking the
interrupt claim to be complete, and running the interrupt handler. Within our
interrupt handler we would need to then read from our accelerator memory to
determine which of our functional units are busy before issuing a new command.
All of this associated overhead would significantly reduce the ability of our
processor to rapidly issue instructions.

\subsection{Hardware Command Buffering}
% Very high savings --- best for small matrices, --- 32~ elements
% TODO: Fix scheduling conflicts. 

\subsection{Rule conflicts, rapid execution}


% TODO: UNKNOWN TIMINGS?:
% TODO: Cycle count / byte read in IO memory (when bypassing the cache!?)
% TODO: CACHE R/W speed!?

\subsection{Other ideas}
% TODO: Why did we ignore interrupts. PLIC IS SLOW



\subsection{Instruction jumping}
One of the more novel ideas for getting a marginal speed-up in issuing commands
is to reduce the processor branching overhead by incoorporating the stalling
logic into the processor itself.

% TODO: Rephrase BUBBLING
When making use of a for loop for branching, processor cycles are wasted
continuously loading the branch instruction, and bubbling the pipeline.

We observe that this overhead can be removed by moving the program counter into
a region of memory within our peipheral that jumps back to itself. This region
can then be overwritten to jump back to the address after the stalled section in
C when appropriate.

For testing purposes, we attempt to execute some example code within our
accelerator memory. This is a non-trivial process...
% TODO: Execute this code
% Highlight the issue
\begin{verbatim}
// MMIO PC
void __attribute__((noinline, section(".dummy_section"))) func_test() {
  print("Meme!\n");
}

// Linker script modified
extern unsigned char dummysec_start[];
extern unsigned char dummysec_end[];
void main() {
  //print("Hello, world!\n");
  //mstatus_init();
  //init_stack();

  //accel_buffered_test();

  // GOTO TEST!

  /* asm ("addi sp,sp,-4"); */
  /* uint32_t label_addr = &&label_test; */
  /* printf("Label Address: %X\n", label_addr); */

  uint32_t func_size = dummysec_end-dummysec_start;
  /* printf("Func Size: 0x%X\n", func_size); */
  /* for(int i=0; i<func_size+4>>2; i++){ */
  /*   uint32_t* ft = (uint32_t*)&func_test + i; */
  /*   printf("\t%X - %X\n", ft, *ft); */
  /* } */
  memcpy((void*)(ACCEL_STAT_ADDR+8), &func_test, func_size);
  /* printf("Func Size: 0x%X\n", func_size); */
  /* for(int i=0; i<func_size+4>>2; i++){ */
  /*   uint32_t* ft = (uint32_t*)(ACCEL_STAT_ADDR+8) + i; */
  /*   printf("\t%X - %X\n", ft, *ft); */
  /* } */
  /* // Move func_test TO stupid SoC chip! */
  //asm volatile ("call % " :: "r"(&func_test));

  ((void (*)(void))0x80010CB8)();
  ((void (*)(void))0xC0002008)(); // Processor stall indefinitely.
  print("Test");
}

func_test address: 80010C98
Meme!
\end{verbatim}





% TODO: Example + study (Evaluation)




\section{Synthesis}
% TODO: Synthesis
\subsection{Firmware modification}


\subsection{Vivado simulation}                                     

\subsection{Implementation}

\subsection{Xilinx Arty A7-100T}

\chapter{Summary and Conclusions} 

% NOTE: Clearly present argument and demonstrate that success criteria were
% metal
% NOTE: Critical thought and interpretation of results
% NOTE: substantiate claims of success/novelty

% NOTE: Reflect on lessons learnt - 

% Future Work
% Further work towards Synthesis --- make use of external DDR3 memory
% Implement Risc-V 'V' extensions

\appendix
\singlespacing

\bibliographystyle{unsrt} 
% \bibliography{dissertation} 

% Bibliography:
% RISC-V GCC, ISA, Spike
% BSV Reference Guide

\end{document}
